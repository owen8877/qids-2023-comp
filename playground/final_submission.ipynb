{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cea8f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:12:04.764840Z",
     "iopub.status.busy": "2023-03-15T19:12:04.763685Z",
     "iopub.status.idle": "2023-03-15T19:12:20.806484Z",
     "shell.execute_reply": "2023-03-15T19:12:20.805268Z"
    },
    "papermill": {
     "duration": 16.060043,
     "end_time": "2023-03-15T19:12:20.809286",
     "exception": false,
     "start_time": "2023-03-15T19:12:04.749243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "ModelLike = Any\n",
    "SupportsPredict = Any\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    \"\"\"\n",
    "    Make sure that the given path is a valid directory by creating one if missing.\n",
    "\n",
    "    :param path: the absolute/relative path to the desired directory\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "class Performance:\n",
    "    def __init__(self):\n",
    "        self._df = DataFrame()\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        try:\n",
    "            index, metric_name = key\n",
    "        except:\n",
    "            raise ValueError('Expecting the key to be iterable. \\n'\n",
    "                             'Hint: this object shall be called as `performance[index, metric_name]=metric_value`.')\n",
    "\n",
    "        self._df.loc[index, metric_name] = value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._df.sort_index(inplace=True)\n",
    "        return self._df.columns\n",
    "\n",
    "    def items(self):\n",
    "        return ((column, self._df[column]) for column in self._df.columns)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        self._df.sort_index(inplace=True)\n",
    "        if isinstance(item, tuple):\n",
    "            raise NotImplementedError('`__getitem__` on performance object is not supported yet.')\n",
    "        return self._df[item]\n",
    "\n",
    "\n",
    "def plot_performance(performance: Performance, metrics_selected: Iterable[str] = None, *,\n",
    "                     ax: Axes = None):\n",
    "    \"\"\"\n",
    "    A generic plotting function for list of metric evaluations.\n",
    "\n",
    "    :param performance:\n",
    "    :param metrics_selected: a subset of metrics to be plotted, `None` means using all metrics existing.\n",
    "    :param ax:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if metrics_selected is not None:\n",
    "        for name, values in performance.items():\n",
    "            if name in metrics_selected:\n",
    "                i = 1\n",
    "                while np.isnan(values.iloc[-i]):\n",
    "                    i += 1\n",
    "                print(f'The ending score for metric {name} is: {values.iloc[-i]:.4e}')\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    for name, values in performance.items():\n",
    "        if metrics_selected is None or name in metrics_selected:\n",
    "            ax.plot(values.index, values, label=name)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set(xlabel='index of fold', ylabel='metric', ylim=[-0.05, 0.1])\n",
    "from pathlib import Path\n",
    "from unittest import TestCase\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "N_asset = 54\n",
    "N_timeslot = 50\n",
    "N_train_days = 1000\n",
    "N_test_days = 700\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, market: DataFrame, fundamental: DataFrame, ref_return: DataFrame):\n",
    "        print('DeprecationWarning: load `@/data/nc/base.nc` instead!')\n",
    "        self.is_train = ref_return is not None\n",
    "\n",
    "        self.market = market\n",
    "        self.fundamental = fundamental\n",
    "        self.ref_return = ref_return\n",
    "\n",
    "    def dump(self, path: str):\n",
    "        ensure_dir(path)\n",
    "        # Notice that the convention is to make multiindex ['day', 'asset', ?'timeslot'], but feather does not support\n",
    "        # multiindex, thus we need to reset index.\n",
    "        self.market.reset_index().to_feather(f'{path}/market.feather')\n",
    "        self.fundamental.reset_index().to_feather(f'{path}/fundamental.feather')\n",
    "        self.ref_return.reset_index().to_feather(f'{path}/ref_return.feather')\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str = 'data/parsed'):\n",
    "        try:\n",
    "            market = pd.read_feather(f'{path}/market.feather').set_index(['day', 'asset', 'timeslot']).sort_index()\n",
    "            fundamental = pd.read_feather(f'{path}/fundamental.feather').set_index(['day', 'asset']).sort_index()\n",
    "            ref_return = pd.read_feather(f'{path}/ref_return.feather').set_index(['day', 'asset']).sort_index()\n",
    "            return Dataset(market, fundamental, ref_return)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f'Data load failed! Check if the pre-processed data is at {path}.')\n",
    "            print('Hint: run the `Test.test_parse_raw_df_and_dump_full` method in `@/pipeline/parse_raw_df.py` if this'\n",
    "                  'is first time to you.')\n",
    "            raise e\n",
    "\n",
    "\n",
    "def generate_mini_csv(n_days: int = 10, path_prefix: str = '..'):\n",
    "    \"\"\"\n",
    "    Generate a mini dataset (usually consisting 10 days) that is useful for unittesting.\n",
    "\n",
    "    :param n_days:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mini_path = f'{path_prefix}/data/raw_mini/{n_days}'\n",
    "    full_path = f'{path_prefix}/data/raw'\n",
    "    ensure_dir(mini_path)\n",
    "    template = '{}/first_round_train_{}_data.csv'\n",
    "    try:\n",
    "        pd.read_csv(template.format(full_path, 'fundamental')).iloc[:N_asset * n_days, :].to_csv(\n",
    "            template.format(mini_path, 'fundamental'), index=False)\n",
    "        pd.read_csv(template.format(full_path, 'market')).iloc[:N_asset * N_timeslot * n_days, :].to_csv(\n",
    "            template.format(mini_path, 'market'), index=False)\n",
    "        pd.read_csv(template.format(full_path, 'return')).iloc[:N_asset * n_days, :].to_csv(\n",
    "            template.format(mini_path, 'return'), index=False)\n",
    "    except FileNotFoundError as e:\n",
    "        print('csv raw data not found! make sure that the training fundamental/market/return data '\n",
    "              'is under the @/data/raw folder')\n",
    "        raise e\n",
    "\n",
    "\n",
    "def load_mini_dataset(parent_path: str = 'data/parsed_mini', n_days: int = 10, path_prefix='.'):\n",
    "    market_path = f'{path_prefix}/{parent_path}/{n_days}/market.feather'\n",
    "    fundamental_path = f'{path_prefix}/{parent_path}/{n_days}/fundamental.feather'\n",
    "    ref_return_path = f'{path_prefix}/{parent_path}/{n_days}/ref_return.feather'\n",
    "    if not Path(market_path).exists():\n",
    "        print(f'Warning: could not find mini market data at `{market_path}`, trying to run the pipleline.')\n",
    "        generate_mini_csv(n_days, path_prefix)\n",
    "        parse_raw_df._dump(is_mini=True, n_days=n_days, path_prefix=path_prefix)\n",
    "\n",
    "    market = pd.read_feather(market_path).set_index(['day', 'asset', 'timeslot']).sort_index()\n",
    "    fundamental = pd.read_feather(fundamental_path).set_index(['day', 'asset']).sort_index()\n",
    "    ref_return = pd.read_feather(ref_return_path).set_index(['day', 'asset']).sort_index()\n",
    "    return Dataset(market, fundamental, ref_return)\n",
    "\n",
    "\n",
    "class Test(TestCase):\n",
    "    def test_generate_mini_dataset(self):\n",
    "        generate_mini_csv(n_days=10)\n",
    "from abc import abstractmethod\n",
    "from typing import Union, Tuple, List, Optional\n",
    "from unittest import TestCase\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, MultiIndex\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.auto import trange\n",
    "from xarray import Dataset, DataArray\n",
    "import xarray as xr\n",
    "\n",
    "from qids_package.qids import make_env\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Strings = Union[List[str], Tuple[str]]\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "def cross_validation(model: ModelLike, feature_columns: Strings, ds: Dataset = None, return_column: str = 'return',\n",
    "                     train_lookback: Optional[int] = None, per_eval_lookback: int = 1) -> Tuple[Performance, Dataset]:\n",
    "    \"\"\"\n",
    "    Perform cross validation backtest on the given set of features.\n",
    "\n",
    "    :param model: a model-like that supports fitting and predicting.\n",
    "    :param feature_columns:\n",
    "    :param ds: the full dataframe containing all necessary data\n",
    "    :param return_column:\n",
    "    :param train_lookback: max number of days of the training feature dataframe, `None` means no truncation.\n",
    "    :param per_eval_lookback: specifies how many days are need for evaluating the prediction on one validation day.\n",
    "    :return: performance: a dictionary containing the metric evaluation for each fold.\n",
    "    :return: cum_y_val_df: a dataframe containing the progressive prediction of y on the validation set and the true values.\n",
    "    \"\"\"\n",
    "    if ds is None:\n",
    "        # TODO: refactor loading code\n",
    "        dataset = pipeline.Dataset.load(f'{__file__}/../data/parsed')\n",
    "        ds = Dataset.from_dataframe(pd.concat([dataset.fundamental, dataset.ref_return], axis=1).dropna())\n",
    "\n",
    "    assert not ds[return_column].isnull().any().item(), 'Input return column contains NaN; maybe truncate to day 998?'\n",
    "\n",
    "    # TODO: refactor check dataset code\n",
    "    # check_dataframe(ds, expect_index=['day', 'asset'], expect_feature=feature_columns + [return_column])\n",
    "    assert {'day', 'asset'}.issubset(set(ds.dims.keys()))\n",
    "    assert set(feature_columns + [return_column]).issubset(set(ds.variables))\n",
    "\n",
    "    start_day = ds.day.min().item()\n",
    "    end_day = ds.day.max().item()\n",
    "    val_start_day = (2 + start_day) if train_lookback is None else (per_eval_lookback + train_lookback + start_day)\n",
    "    pbar = trange(val_start_day, end_day + 1)\n",
    "    # pbar = trange(val_start_day, end_day - 1) # the last two days have no proper return\n",
    "\n",
    "    performance = Performance()\n",
    "    coords = ds.sel(day=slice(val_start_day, end_day)).coords\n",
    "    sub_coords = {k: coords.get(k) for k in ('day', 'asset')}\n",
    "    cum_y_val_prediction = DataArray(np.nan, dims=['day', 'asset'], coords=sub_coords)\n",
    "    cum_y_val_true = DataArray(np.nan, dims=['day', 'asset'], coords=sub_coords)\n",
    "\n",
    "    for val_index in pbar:\n",
    "        days_train = np.arange(start_day if train_lookback is None else\n",
    "                               (val_index - per_eval_lookback - train_lookback), val_index - 1)\n",
    "        days_val = np.arange(val_index + 1 - per_eval_lookback, val_index + 1)\n",
    "\n",
    "        X_train = ds[feature_columns].sel(day=days_train)\n",
    "        # X_train.reindex(day=X_train.day[::-1]) #for LSTM??\n",
    "        y_train_true = ds[return_column].sel(day=days_train[per_eval_lookback - 1:])\n",
    "        # y_train_true.reindex(day=y_train_true.day[::-1])  # for LSTM??\n",
    "        y_train_pred = model.fit_predict(X_train, y_train_true)\n",
    "\n",
    "        y_train_prediction = DataArray(data=y_train_pred, coords=y_train_true.coords)  # TODO: check shape\n",
    "        # y_train_true has shape [days_slice, assets]\n",
    "\n",
    "        X_val = ds[feature_columns].sel(day=days_val)\n",
    "        # X_val.reindex(day=X_val.day[::-1]) ## for LSTM??\n",
    "        y_val_true = ds[return_column].sel(day=days_val[per_eval_lookback - 1:])\n",
    "        # y_val_true.reindex(day=y_val_true.day[::-1]) ## for LSTM??\n",
    "        y_val_prediction = DataArray(data=model.predict(X_val), coords=y_val_true.coords)\n",
    "\n",
    "        cum_y_val_true.loc[dict(day=val_index)] = y_val_true.sel(day=val_index)\n",
    "        cum_y_val_prediction.loc[dict(day=val_index)] = y_val_prediction.sel(day=val_index)\n",
    "\n",
    "        train_r2 = r2_score(y_train_true.to_series(), y_train_prediction.to_series())\n",
    "        performance[val_index, 'train_r2'] = train_r2\n",
    "        val_r2 = r2_score(y_val_true.to_series(), y_val_prediction.to_series()) if len(\n",
    "            y_val_true.to_series()) > 1 else np.nan\n",
    "        performance[val_index, 'val_r2'] = val_r2\n",
    "        val_pearson = y_val_true.to_series().corr(y_val_prediction.to_series()) if len(\n",
    "            y_val_true.to_series()) > 1 else np.nan\n",
    "        performance[val_index, 'val_pearson'] = val_pearson\n",
    "\n",
    "        y_val_true_so_far = cum_y_val_true.sel(day=slice(start_day, val_index)).to_series()\n",
    "        y_val_pred_so_far = cum_y_val_prediction.sel(day=slice(start_day, val_index)).to_series()\n",
    "        val_cum_r2 = r2_score(y_val_true_so_far, y_val_pred_so_far)\n",
    "        performance[val_index, 'val_cum_r2'] = val_cum_r2\n",
    "        val_cum_pearson = y_val_true_so_far.corr(y_val_pred_so_far)\n",
    "        performance[val_index, 'val_cum_pearson'] = val_cum_pearson\n",
    "\n",
    "        # raise\n",
    "\n",
    "        pbar.set_description(\n",
    "            f'Validation on day {val_index}, train_r2={train_r2:.4f}, val_r2={val_r2:.4f}, val_cum_r2={val_cum_r2:.4f}, val_cum_pearson={val_cum_pearson:.4f}')\n",
    "\n",
    "    cum_y_val_df = Dataset({k.name: k for k in (cum_y_val_true, cum_y_val_prediction)})\n",
    "\n",
    "    return performance, cum_y_val_df\n",
    "\n",
    "\n",
    "def nan_series_factory(index, name) -> Series:\n",
    "    data = np.empty(len(index))\n",
    "    data.fill(np.nan)\n",
    "    return Series(data, index, dtype=float, name=name)\n",
    "\n",
    "\n",
    "def nan_dataframe_factory(index, columns) -> DataFrame:\n",
    "    data = np.empty((len(index), len(columns)))\n",
    "    data.fill(np.nan)\n",
    "    return DataFrame(data, index, columns, dtype=float)\n",
    "\n",
    "\n",
    "class AugmentationOption:\n",
    "    def __init__(\n",
    "            self,\n",
    "            return_lookback: int = 2,\n",
    "            market_return: bool = False, market_return_lookback: int = 5,\n",
    "    ):\n",
    "        self.return_lookback = return_lookback\n",
    "        self.market_return = market_return\n",
    "        self.market_return_lookback = market_return_lookback\n",
    "\n",
    "\n",
    "def evaluation_for_submission(model: ModelLike, given_ds: Dataset, qids, lookback_window: Union[int, None] = 200,\n",
    "                              per_eval_lookback: int = 1, option: AugmentationOption = None) -> Tuple[Performance, Series] :\n",
    "    \"\"\"\n",
    "    Evaluate the given model on the test dataset for submission.\n",
    "    Assuming no additional features except the fundamental data and extracted market data.\n",
    "    \"\"\"\n",
    "    N_total = N_train_days + N_test_days\n",
    "    more_ds = Dataset(data_vars={k: np.nan for k in given_ds.data_vars if k != 'market_share'},\n",
    "                      coords=dict(day=range(N_train_days + 1, N_total + 1), asset=range(N_asset),\n",
    "                                  timeslot=range(1, N_timeslot + 1)))\n",
    "    ds: Dataset = xr.combine_by_coords([given_ds, more_ds])\n",
    "    ds['market_share'] = given_ds['market_share']\n",
    "\n",
    "    if option is None:\n",
    "        option = AugmentationOption()\n",
    "\n",
    "    # Add return data from past few days\n",
    "    return_0 = ds['return_0']\n",
    "    for i in range(1, option.return_lookback + 1):\n",
    "        return_i = return_0.shift(day=i, fill_value=0)\n",
    "        ds[f'return_{i}'] = return_i\n",
    "\n",
    "    # Add market return\n",
    "    if option.market_return:\n",
    "        market_return_0 = calculate_market_return(ds['return_0'])\n",
    "        ds['market_return_0'] = market_return_0\n",
    "        for i in range(1, option.market_return_lookback + 1):\n",
    "            ds[f'market_return_{i}'] = market_return_0.shift(day=i, fill_value=0)\n",
    "\n",
    "    # Assuming that the days start from 1; needs to be checked\n",
    "    ds['return_pred'] = DataArray(data=np.nan, coords=dict(day=range(1, N_total + 1), asset=range(N_asset)),\n",
    "                                  dims=['day', 'asset'])\n",
    "    all_features_but_return = list(set(ds.data_vars.keys()) - {'return', 'return_pred'})\n",
    "\n",
    "    performance = Performance()\n",
    "    env = qids.make_env()\n",
    "    pbar = trange(N_train_days + 1, N_train_days + N_test_days + 1)\n",
    "    pbar_iter = iter(pbar)\n",
    "\n",
    "    while not env.is_end():\n",
    "        current_day = next(pbar_iter)\n",
    "        before1_day = current_day - 1\n",
    "        before2_day = current_day - 2\n",
    "\n",
    "        # Obtain a slice of today's data and update to the full dataset\n",
    "        new_fundamental_raw, new_market_raw = env.get_current_market()\n",
    "        new_ds = pre_process_df(new_fundamental_raw, new_market_raw)\n",
    "        for col in new_ds.data_vars:\n",
    "            ds[col].loc[dict(day=current_day)] = new_ds[col].sel(day=current_day)\n",
    "\n",
    "        new_market_brief = extract_market_data(\n",
    "            ds[['open', 'close', 'low', 'high', 'volume', 'money']].sel(day=[before1_day, current_day])\n",
    "        ).sel(day=[current_day])\n",
    "        for col in new_market_brief:\n",
    "            ds[col].loc[dict(day=current_day)] = new_market_brief[col].sel(day=current_day)\n",
    "\n",
    "        return_forecast_true = ds['close_0'].sel(day=current_day) / ds['close_0'].sel(day=before2_day) - 1\n",
    "        ds['return'].loc[dict(day=before2_day)] = return_forecast_true\n",
    "\n",
    "        # Data augmentation\n",
    "        if option.market_return:\n",
    "            current_market_return = calculate_market_return(new_market_brief['return_0'])\n",
    "            ds[current_market_return.name].loc[dict(day=current_day)] = current_market_return.sel(day=current_day)\n",
    "            for i in range(1, option.market_return_lookback + 1):\n",
    "                ds[f'market_return_{i}'].loc[dict(day=current_day)] = ds['market_return_0'].sel(day=current_day - i)\n",
    "        for i in range(1, option.return_lookback + 1):\n",
    "            ds[f'return_{i}'].loc[dict(day=current_day)] = ds[f'return_{i - 1}'].sel(day=before1_day)\n",
    "\n",
    "        _d_ = dict(day=current_day)\n",
    "        for p_f, f in ('b', 'book'), ('e_ttm', 'earnings_ttm'), ('e', 'earnings'), ('s', 'sales'), ('cf', 'cashflow'):\n",
    "            ds[f].loc[_d_] = ds['close_0'].loc[_d_] / ds[f'p{p_f}'].loc[_d_]\n",
    "        ds['market_cap'].loc[_d_] = ds['close_0'].sel(day=current_day) * ds['market_share']\n",
    "\n",
    "        # Train the model on what we already have\n",
    "        days_train = range(1 if lookback_window is None else (current_day - per_eval_lookback - lookback_window),\n",
    "                           before1_day)\n",
    "        X_train = ds[all_features_but_return].sel(day=days_train)\n",
    "        assert not X_train.isnull().any().to_array().any().item()\n",
    "        y_train_true = ds['return'].sel(day=days_train[per_eval_lookback - 1:])\n",
    "        y_train_pred = model.fit_predict(X_train, y_train_true)\n",
    "        train_r2 = r2_score(y_train_true.to_series(), y_train_pred.to_series())\n",
    "\n",
    "        X_eval = ds[all_features_but_return].sel(day=slice(current_day + 1 - per_eval_lookback, current_day))\n",
    "        assert not X_eval.isnull().any().to_array().any().item()\n",
    "        y_eval_prediction = DataArray(data=model.predict(X_eval), dims=['day','asset']).rename('return_pred')\n",
    "        assert set(y_eval_prediction.dims) == {'day', 'asset'}\n",
    "        assert y_eval_prediction.asset.to_series().is_monotonic_increasing\n",
    "        env.input_prediction(y_eval_prediction.transpose('day', 'asset').to_series())\n",
    "        ds['return_pred'].loc[dict(day=current_day)] = y_eval_prediction.isel(\n",
    "            day=0)  # TODO: why day information is lost in rolling_exp?\n",
    "\n",
    "        performance[current_day, 'train_r2'] = train_r2\n",
    "        if before2_day > N_train_days:\n",
    "            return_forecast_pred = ds['return_pred'].sel(day=before2_day)\n",
    "            test_r2 = r2_score(return_forecast_true.to_series(), return_forecast_pred.to_series())\n",
    "            performance[before2_day, 'test_r2'] = test_r2\n",
    "            test_pearson = return_forecast_true.to_series().corr(return_forecast_pred.to_series())\n",
    "            performance[before2_day, 'test_pearson'] = test_pearson\n",
    "\n",
    "            cum_return_true = ds['return'].sel(day=slice(N_train_days + 1, before2_day))\n",
    "            cum_return_pred = ds['return_pred'].sel(day=slice(N_train_days + 1, before2_day))\n",
    "            test_cum_r2 = r2_score(cum_return_true.to_series(), cum_return_pred.to_series())\n",
    "            performance[before2_day, 'test_cum_r2'] = test_cum_r2\n",
    "            test_cum_pearson = cum_return_true.to_series().corr(cum_return_pred.to_series())\n",
    "            performance[before2_day, 'test_cum_pearson'] = test_cum_pearson\n",
    "\n",
    "            pbar.set_description(f'Day {current_day}, train r2={train_r2:.4f}, test cum r2={test_cum_r2:.4f}, '\n",
    "                                 f'test cum pearson {test_cum_pearson:.4f}')\n",
    "\n",
    "    return performance, ds['return_pred'].to_series().iloc[1000*54:].reset_index()\n",
    "\n",
    "\n",
    "class Test(TestCase):\n",
    "    def test_cross_validation(self):\n",
    "        \"\"\"\n",
    "        A minimalist set-up to show how to use the backtest cross-validation suite.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from visualization.metric import plot_performance\n",
    "        from matplotlib import pyplot as plt\n",
    "        from datatools import data_quantization\n",
    "        from pipeline import load_mini_dataset\n",
    "        import matplotlib as mpl\n",
    "\n",
    "        dataset = load_mini_dataset('data/parsed_mini', 10, path_prefix='..')\n",
    "        quantized_fundamental, _ = data_quantization(dataset.fundamental)\n",
    "        df = pd.concat([quantized_fundamental, dataset.fundamental, dataset.ref_return], axis=1).dropna()\n",
    "        quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                            'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE']\n",
    "        original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf']\n",
    "\n",
    "        class SimpleLinearModel(ModelLike):\n",
    "            def __init__(self):\n",
    "                self.reg = LinearRegression()\n",
    "\n",
    "            def fit_predict(self, X: DataFrame, y: Series):\n",
    "                print('fit:')\n",
    "                print(X.index.get_level_values(0).unique())\n",
    "                self.reg.fit(X, y)\n",
    "                return self.reg.predict(X)\n",
    "\n",
    "            def predict(self, X: DataFrame):\n",
    "                print('predict:')\n",
    "                print(X.index.get_level_values(0).unique())\n",
    "                return self.reg.predict(X)\n",
    "\n",
    "        simple_linear_model = SimpleLinearModel()\n",
    "\n",
    "        performance, cum_y_val_df = cross_validation(simple_linear_model, quantile_feature, ds=df, train_lookback=5,\n",
    "                                                     per_eval_lookback=1)\n",
    "        performance, cum_y_val_df = cross_validation(simple_linear_model, quantile_feature, ds=df, train_lookback=5,\n",
    "                                                     per_eval_lookback=3)\n",
    "\n",
    "        mpl.use('TkAgg')\n",
    "        plt.figure()\n",
    "        plot_performance(performance, metrics_selected=['train_r2', 'val_cum_pearson'])\n",
    "        plt.show()\n",
    "\n",
    "    def test_evaluation_for_submission(self):\n",
    "        \"\"\"\n",
    "        A minimalist set-up to show how to use the evalution and submission suite.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from visualization.metric import plot_performance\n",
    "        from matplotlib import pyplot as plt\n",
    "        from pipeline import Dataset\n",
    "\n",
    "        dataset = Dataset.load('../data/parsed')\n",
    "        # dataset = load_mini_dataset(path_prefix='..')\n",
    "        df = dataset.fundamental\n",
    "        f_quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                              'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE']\n",
    "        m_quantile_feature = ['avg_price_QUANTILE', 'volatility_QUANTILE', 'mean_volume_QUANTILE']\n",
    "        feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf', 'avg_price', 'volatility',\n",
    "                   'mean_volume']\n",
    "\n",
    "        class SimpleLinearModel:\n",
    "            def __init__(self, features):\n",
    "                self.reg = LinearRegression(fit_intercept=True)\n",
    "                self.features = features\n",
    "\n",
    "            def fit_predict(self, X, y):\n",
    "                self.reg.fit(X, y)\n",
    "                y_pred = self.reg.predict(X)\n",
    "                return y_pred\n",
    "\n",
    "            def predict(self, X):\n",
    "                return self.reg.predict(X)\n",
    "\n",
    "        qids = QIDS(path_prefix='../')\n",
    "        # qids = None\n",
    "        model = SimpleLinearModel(feature)\n",
    "        performance,_ = evaluation_for_submission(model, dataset=dataset, qids=qids, lookback_window=200,\n",
    "                                                option=AugmentationOption(market_return=True))\n",
    "\n",
    "        plt.figure()\n",
    "        plot_performance(performance, metrics_selected=['train_r2', 'test_cum_r2', 'test_cum_pearson'])\n",
    "        plt.show()\n",
    "from unittest import TestCase\n",
    "\n",
    "from xarray import Dataset\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def calculate_fundamental_v0(ds: Dataset):\n",
    "    earnings_ttm = ds['close_0'] / ds['pe_ttm']\n",
    "    earnings = ds['close_0'] / ds['pe']\n",
    "    book = ds['close_0'] / ds['pb']\n",
    "    sales = ds['close_0'] / ds['ps']\n",
    "    cashflow = ds['close_0'] / ds['pcf']\n",
    "\n",
    "    market_share_history = ds['volume'].sum('timeslot') / ds['turnoverRatio']\n",
    "    market_share = market_share_history.sel(day=slice(990, 1000)).median(dim='day') / 1e8\n",
    "    market_cap = market_share * ds['close_0']\n",
    "\n",
    "    return Dataset(data_vars={\n",
    "        'earnings_ttm': earnings_ttm,\n",
    "        'earnings': earnings,\n",
    "        'book': book,\n",
    "        'sales': sales,\n",
    "        'cashflow': cashflow,\n",
    "        'market_share': market_share,\n",
    "        'market_cap': market_cap,\n",
    "    })\n",
    "\n",
    "\n",
    "class Test(TestCase):\n",
    "    def test_export_fundamental_v0(self):\n",
    "        path = '../data/nc'\n",
    "        base_ds = xr.open_dataset(f'{path}/base.nc')\n",
    "        market_brief_ds = xr.open_dataset(f'{path}/market_brief.nc')\n",
    "        ds = base_ds.merge(market_brief_ds)\n",
    "\n",
    "        fundamental_ds = calculate_fundamental_v0(ds)\n",
    "        fundamental_ds.to_netcdf(f'{path}/fundamental_v0.nc')\n",
    "from unittest import TestCase\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from xarray import Dataset\n",
    "\n",
    "\n",
    "\n",
    "def parse_date_time_column(s: Series):\n",
    "    \"\"\"\n",
    "    Parse the date_time field and scatter into asset/day/timeslot.\n",
    "\n",
    "    :param s: a `Series` object where the entries follow `s[asset]d[day]p[timeslot]`\n",
    "    :return: the parsed dataframe\n",
    "    \"\"\"\n",
    "    return s.str.split(pat='s|d|p', expand=True).iloc[:, 1:].astype(int).rename(\n",
    "        columns={1: 'asset', 2: 'day', 3: 'timeslot'})\n",
    "\n",
    "\n",
    "def parse_date_column(s: Series):\n",
    "    \"\"\"\n",
    "    Parse the date_time field and scatter into asset/day.\n",
    "\n",
    "    :param s: a `Series` object where the entries follow `s[asset]d[day]`\n",
    "    :return: the parsed dataframe\n",
    "    \"\"\"\n",
    "    return s.str.split(pat='s|d', expand=True).iloc[:, 1:].astype(int).rename(columns={1: 'asset', 2: 'day'})\n",
    "\n",
    "\n",
    "def pre_process_df_with_date_time_legacy(df: DataFrame):\n",
    "    date_time_series = df['date_time']\n",
    "    p_df = pd.concat((parse_date_time_column(date_time_series), df.drop(columns='date_time')), axis=1)\n",
    "    return p_df.set_index(['day', 'asset', 'timeslot']).sort_index()\n",
    "\n",
    "\n",
    "def pre_process_df_with_date_legacy(df: DataFrame):\n",
    "    date_time_series = df['date_time']\n",
    "    p_df = pd.concat((parse_date_column(date_time_series), df.drop(columns='date_time')), axis=1)\n",
    "    return p_df.set_index(['day', 'asset']).sort_index()\n",
    "\n",
    "\n",
    "def pre_process_df(f_df: DataFrame, m_df: DataFrame):\n",
    "    f_ds = Dataset.from_dataframe(pre_process_df_with_date_legacy(f_df))\n",
    "    m_ds = Dataset.from_dataframe(pre_process_df_with_date_time_legacy(m_df))\n",
    "    return f_ds.merge(m_ds)\n",
    "\n",
    "\n",
    "def _dump(is_mini: bool = False, n_days: int = 10, path_prefix='..'):\n",
    "    \"\"\"\n",
    "    The actual working file that dumps the dataset file.\n",
    "\n",
    "    :param is_mini:\n",
    "    :param n_days:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    NC_PATH = 'data/nc'\n",
    "    RAW_PATH = 'data/raw'\n",
    "\n",
    "    nc_path = f'{path_prefix}/{NC_PATH}' + (f'_mini/{n_days}' if is_mini else '')\n",
    "    raw_path = '/kaggle/input/hku-qids-2023-quantitative-investment-competition'\n",
    "    template = '{}/first_round_train_{}_data.csv'\n",
    "    ensure_dir(nc_path)\n",
    "\n",
    "    try:\n",
    "        f_df = pre_process_df_with_date_legacy(pd.read_csv(template.format(raw_path, 'fundamental')))\n",
    "        m_df = pre_process_df_with_date_time_legacy(pd.read_csv(template.format(raw_path, 'market')))\n",
    "        r_df = pre_process_df_with_date_legacy(pd.read_csv(template.format(raw_path, 'return')))\n",
    "        ds = Dataset.from_dataframe(f_df)\n",
    "        ds.update(Dataset.from_dataframe(m_df))\n",
    "        ds.update(Dataset.from_dataframe(r_df))\n",
    "        ds.to_netcdf(f'{nc_path}/base.nc')\n",
    "    except FileNotFoundError as e:\n",
    "        print('csv raw data not found! make sure that the training fundamental/market/return data '\n",
    "              f'is under the @{raw_path} folder')\n",
    "        raise e\n",
    "\n",
    "\n",
    "def _dump_legacy(is_mini: bool = False, n_days: int = 10, path_prefix='..'):\n",
    "    \"\"\"\n",
    "    The actual working file that dumps the dataset file.\n",
    "\n",
    "    :param is_mini:\n",
    "    :param n_days:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    PARSED_PATH = 'data/parsed'\n",
    "    RAW_PATH = 'data/raw'\n",
    "\n",
    "    parsed_path = '/kaggle/working/parsed'\n",
    "    raw_path = '/kaggle/input/hku-qids-2023-quantitative-investment-competition'\n",
    "    template = '{}/first_round_train_{}_data.csv'\n",
    "    try:\n",
    "        f_df = pre_process_df_with_date_legacy(pd.read_csv(template.format(raw_path, 'fundamental')))\n",
    "        m_df = pre_process_df_with_date_time_legacy(pd.read_csv(template.format(raw_path, 'market')))\n",
    "        r_df = pre_process_df_with_date_legacy(pd.read_csv(template.format(raw_path, 'return')))\n",
    "        pipeline.Dataset(m_df, f_df, r_df).dump(parsed_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print('csv raw data not found! make sure that the training fundamental/market/return data '\n",
    "              f'is under the @{raw_path} folder')\n",
    "        raise e\n",
    "\n",
    "\n",
    "class Test(TestCase):\n",
    "    def test_parse_raw_df_and_dump_full(self):\n",
    "        _dump(is_mini=False, n_days=-1, path_prefix=\"/kaggle/working\")\n",
    "\n",
    "    def test_parse_raw_df_and_dump_mini(self, n_days: int = 10):\n",
    "        _dump(is_mini=True, n_days=n_days)\n",
    "from typing import Optional\n",
    "from unittest import TestCase\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xarray import DataArray, Dataset\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "\n",
    "def standardize_pd(Xin):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    Xin : <pd.DataFrame> dataframe containing the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    <pd.DataFrame> the column-wise standardized dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return Xin - Xin.min() / (Xin.max() - Xin.min())\n",
    "\n",
    "\n",
    "def make_clean_data(pd_data, verbose=False):\n",
    "    \"\"\"\n",
    "    -Given any dataframe, removes rows with nan or none, examining\n",
    "    feature by feature\n",
    "    -This probably can be done by doing the whole dataframe simultaneously\n",
    "    \"\"\"\n",
    "    # infer the features we have kept by checking columns of the input dataframe\n",
    "\n",
    "    # Needed: check all features for missing values and remove everybody with missing.\n",
    "    # check remaining percentage\n",
    "\n",
    "    features = pd_data.columns\n",
    "    missing_dict = dict()\n",
    "    # bad_rows = list()\n",
    "\n",
    "    bad_rows_index = pd_data.isna().sum(axis=1).to_numpy().nonzero()\n",
    "    bad_feature_index = pd_data.isna().sum().to_numpy().nonzero()\n",
    "    bad_feature = pd_data.columns[bad_feature_index]\n",
    "\n",
    "    for ft in bad_feature:\n",
    "        # Calculate the percentage of that feature which was True under .isnull()\n",
    "        num_missing_bool = pd_data.isna().sum()[ft]\n",
    "        missing_dict[ft] = num_missing_bool / len(pd_data[ft])\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Issue Feature:\\n\", ft, '\\n',\n",
    "                  '\\n Num of null=', num_missing_bool, '\\n\\n')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # for ft in features:\n",
    "    #     pd_data.isna().sum()\n",
    "    #     feature_series = pd_data[ft]\n",
    "    #     missing_bool = feature_series.isnull()\n",
    "    #     bad_indices = feature_series.index[missing_bool]\n",
    "    #     #Calculate the percentage of that feature which was True under .isnull()\n",
    "    #     missing_dict[ft] = 100*float(np.sum(missing_bool)/feature_series.shape[0])\n",
    "    #\n",
    "    #\n",
    "    #     if not bad_indices.empty:\n",
    "    #         if verbose:\n",
    "    #             print(\"Issue Feature:\\n\", ft,'\\n', bad_indices, '\\n Num of null=', len(bad_indices), '\\n\\n')\n",
    "    #             bad_rows += list(bad_indices)\n",
    "    #             print('Here are Nan Indices:', bad_indices)\n",
    "    #         else:\n",
    "    #             pass\n",
    "\n",
    "    # Total percentage(s) of data removed\n",
    "    if verbose:\n",
    "        # print('Here are Nan Row Indices:', bad_rows_index[0], '\\n') #maybe we don't need but I added here\n",
    "        print('Total Number of Removed Row Instances = ',\n",
    "              len(bad_rows_index[0]), '\\n ')\n",
    "        print('Percentage of Removed Features: \\n', missing_dict)\n",
    "    # Eliminate duplicates and sort\n",
    "    # bad_rows = list(set(bad_rows))\n",
    "    # bad_rows.sort()\n",
    "\n",
    "    # Get rid of rows containing null or empty\n",
    "    # clean_data = pd_data.drop(bad_rows)\n",
    "    clean_data = pd_data.drop(bad_rows_index[0])\n",
    "\n",
    "    # Check if clean\n",
    "    assert np.size(clean_data.isna().sum(axis=1).to_numpy().nonzero()[\n",
    "                       0]) == 0, \"Clean data still contains NaN\"\n",
    "\n",
    "    # Check the number of resulting data points\n",
    "    # if verbose:\n",
    "    #     print('Here is shape of original data:',data.shape,'\\n\\n')\n",
    "    #     print('Here is shape of the clean data:', data_clean.shape,\\\n",
    "    #           '\\n Number of Removed Instances =',len(bad_rows))\n",
    "\n",
    "    return clean_data, missing_dict, bad_rows_index\n",
    "\n",
    "\n",
    "def remove_quantiles(pd_data, p=1):\n",
    "    percentile = p\n",
    "    quantile = percentile / 100\n",
    "    remove_indices = list()\n",
    "    # feature_stats = dict()\n",
    "\n",
    "    for feature in pd_data.columns:\n",
    "        feature_series = pd_data[feature]\n",
    "        quantile_filter = np.quantile(feature_series, [quantile, 1 - quantile])\n",
    "        feature_outside = feature_series[(feature_series < quantile_filter[0]) | (\n",
    "                feature_series > quantile_filter[1])]\n",
    "        # outside_indices = feature_outside.index\n",
    "        remove_indices += list(feature_outside.index)\n",
    "    remove_indices = list(set(remove_indices))\n",
    "    remove_indices.sort()\n",
    "\n",
    "    pd_data_reduced = pd_data.drop(remove_indices)\n",
    "\n",
    "    # Calculate what percent of total data is captured in these indices\n",
    "    percent_removed = 100 * (len(remove_indices) / pd_data.index.shape[0])\n",
    "    print('Percent of Data Removed Across These Quantiles Is: ', percent_removed)\n",
    "\n",
    "    return pd_data_reduced, percent_removed\n",
    "\n",
    "\n",
    "def run_svd(pd_data, percent_var=95):\n",
    "    # def run_svd(data, rank = 3):\n",
    "    \"\"\"\n",
    "    :pd_data: the dataframe containing the 'already standardized'] data\n",
    "    :percent_var: float - a value between [0,100]\n",
    "    \"\"\"\n",
    "    # add checking if percent_var between 0 and 100\n",
    "\n",
    "    # Calculate the desired number of SVD components in the decomposition\n",
    "    start_rank = (pd_data.shape[-1] - 1)\n",
    "    # Make instance of SVD object class from scikit-learn and run the decomposition\n",
    "    # Issue: scikitlearn TruncatedSVD only allows n_components < n_features (strictly)\n",
    "\n",
    "    SVD = TruncatedSVD(n_components=start_rank)\n",
    "    SVD.fit(pd_data)\n",
    "    X_SVD = SVD.transform(pd_data)\n",
    "\n",
    "    # Wrap the output as a dataframe\n",
    "    X_SVD = pd.DataFrame(\n",
    "        X_SVD, columns=['Singular Component ' + str(i + 1) for i in range(X_SVD.shape[-1])])\n",
    "\n",
    "    # Calculate the number of components needed to reach variance threshold\n",
    "    var_per_comp = SVD.explained_variance_ratio_\n",
    "\n",
    "    # Calculate the total variance explainend in the first k components\n",
    "    total_var = 100 * np.cumsum(var_per_comp)\n",
    "    print('------------- SVD Output ----------------')\n",
    "    print('Percent Variance Explained By First ' +\n",
    "          str(start_rank) + ' Components: ', total_var, '\\n\\n')\n",
    "    # rank = np.nonzero(total_var>=var_threshold)[0][0]+1\n",
    "    rank = (next(x for x, val in enumerate(total_var) if val > percent_var))\n",
    "    rank += 1\n",
    "\n",
    "    if rank == 0:\n",
    "        print('No quantity of components leq to ' + str(start_rank + 1) +\n",
    "              ' can explain ' + str(percent_var) + '% variance.')\n",
    "    else:\n",
    "        print(str(total_var[rank - 1]) + '% variance ' + 'explained by ' + str(rank) + ' components. ' +\n",
    "              'Variance Threshold Was ' + str(percent_var) + '.\\n\\n')\n",
    "\n",
    "    return X_SVD, rank, percent_var, total_var\n",
    "\n",
    "\n",
    "def data_quantization(pd_data, scale=10):\n",
    "    \"\"\"\n",
    "    Quantize a panda data frame into integer with new features according to the given scale.\n",
    "    e.g. if scale = 10: the new feature assign label 1 to the first, and 10 to the last\n",
    "    :param pd_data:\n",
    "    :param scale:\n",
    "    :return: data_quantile: the quantized data\n",
    "             percent_of_zero: at least that much percent of feature are zeros\n",
    "    \"\"\"\n",
    "    p = np.linspace(0, scale, scale + 1) * 0.1\n",
    "    data_quantile = pd_data.copy()\n",
    "    percent_of_zero = {}\n",
    "    eps = 1e-5\n",
    "\n",
    "    for feature in pd_data.columns:\n",
    "        feature_new = feature + '_QUANTILE'\n",
    "        data_quantile[feature_new] = 0\n",
    "\n",
    "        for (i, quantile) in enumerate(p[:-1]):\n",
    "            quantile_filter = np.quantile(\n",
    "                pd_data[feature], [quantile, p[i + 1]])\n",
    "            data_quantile.loc[((pd_data[feature] > quantile_filter[0]) &\n",
    "                               (pd_data[feature] <= quantile_filter[1])), feature_new] = i + 1\n",
    "\n",
    "            # deal with 0-quantile being non-zero\n",
    "            if i == 0 and quantile_filter[0] > 0:\n",
    "                data_quantile.loc[((pd_data[feature] >= quantile_filter[0]) &\n",
    "                                   (pd_data[feature] <= quantile_filter[1])), feature_new] = i + 1\n",
    "\n",
    "            if quantile_filter[0] <= eps and quantile_filter[1] >= eps:\n",
    "                percent_of_zero[feature] = quantile\n",
    "            elif quantile_filter[0] > eps and i == 0:\n",
    "                percent_of_zero[feature] = 0\n",
    "\n",
    "        data_quantile.drop(columns=feature, axis=1, inplace=True)\n",
    "    return data_quantile, percent_of_zero\n",
    "\n",
    "\n",
    "def extract_market_data_legacy(m_df: DataFrame):\n",
    "    \"\"\"\n",
    "    Input the market data and extract mean price using close\n",
    "    prices, volatility, daily return and mean volume\n",
    "    :param m_df: [pd.DataFrame] market data, set_index already\n",
    "    :return: [pd.DataFrame] extracted features from market data\n",
    "    \"\"\"\n",
    "    # sort indexing\n",
    "    if m_df.index.names == ['asset', 'day', 'timeslot']:\n",
    "        m_df = m_df.swaplevel(0, 1)\n",
    "    elif m_df.index.names == ['day', 'asset', 'timeslot']:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('Unsupported index ordering')\n",
    "\n",
    "    m_df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    m_df_day = m_df.groupby(level=[0, 1])[['volume', 'money']].sum()\n",
    "    # Compute average price\n",
    "    m_df_day['avg_price'] = m_df_day['money'] / m_df_day['volume']\n",
    "    # find index of zero volume\n",
    "    indx_day = m_df_day[m_df_day['volume'] == 0].index\n",
    "    # compute replacing value as mean of high and low\n",
    "    for (i, indx) in enumerate(indx_day):\n",
    "        replace_value = .5 * m_df.loc[indx, 'high'].max() + .5 * m_df.loc[indx, 'low'].min()\n",
    "        m_df_day.loc[indx, 'avg_price'] = replace_value\n",
    "\n",
    "    assert np.size(m_df_day.isna().sum(axis=1).to_numpy().nonzero()[\n",
    "                       0]) == 0, \"Clean data still contains NaN\"\n",
    "\n",
    "    # Compute volatility\n",
    "    T = 50.0  # number of time units\n",
    "    # note numpy use 0 dof while pd use 1 dof\n",
    "    m_df_day['volatility'] = m_df.groupby(level=[0, 1])['close'].std() * np.sqrt(T)\n",
    "\n",
    "    # Compute average volume:\n",
    "    m_df_day['mean_volume'] = m_df_day['volume'] / T\n",
    "\n",
    "    # drop unnecessary features:\n",
    "    m_df_day = m_df_day.drop(columns=['volume', 'money'])\n",
    "\n",
    "    # Daily return that compares the first open and the last close\n",
    "    price_groupby = m_df.reset_index(level=[0, 1]).groupby(by=['day', 'asset'])\n",
    "    close_price = price_groupby['close'].take([N_timeslot - 1]).reset_index('timeslot', drop=True).rename('return_0')\n",
    "    open_price = price_groupby['open'].take([0]).reset_index('timeslot', drop=True).rename('return_0')\n",
    "\n",
    "    day_1_return = close_price.iloc[:N_asset] / open_price.iloc[:N_asset] - 1\n",
    "    remaining_day_return = close_price.iloc[N_asset:] / close_price.shift(N_asset)[N_asset:] - 1\n",
    "\n",
    "    m_df_day['return_0'] = pd.concat([day_1_return, remaining_day_return])\n",
    "    m_df_day['close_0'] = m_df.groupby(level=[0, 1])['close'].take([N_timeslot - 1]).values\n",
    "    return m_df_day\n",
    "\n",
    "\n",
    "def extract_market_data(ds: Dataset):\n",
    "    \"\"\"\n",
    "    Input the market data and extract mean price using close\n",
    "    prices, volatility, daily return and mean volume\n",
    "    :param ds: [xr.Dataset] dataset that contains market data\n",
    "    :return: [xr.Dataset] extracted features from market data\n",
    "    \"\"\"\n",
    "    assert not ds.isnull().any().to_array().any().item()\n",
    "\n",
    "    day_money_sum = ds['money'].sum('timeslot')\n",
    "    day_volume_sum = ds['volume'].sum('timeslot')\n",
    "    avg_price = day_money_sum / day_volume_sum\n",
    "    day_volume_s = day_volume_sum.to_series()\n",
    "    indx_day = day_volume_s.where(day_volume_s == 0).dropna()\n",
    "\n",
    "    for day_idx, asset_idx in indx_day.index:\n",
    "        sub_ds = ds.sel(day=day_idx, asset=asset_idx)\n",
    "        replace_value = (sub_ds['high'].max() + sub_ds['low'].min()) / 2\n",
    "        avg_price.loc[dict(day=day_idx, asset=asset_idx)] = replace_value\n",
    "\n",
    "    assert not avg_price.isnull().any().item(), \"Clean data still contains NaN\"\n",
    "\n",
    "    # Compute volatility\n",
    "    T = len(ds.timeslot)  # number of time units\n",
    "    # note numpy use 0 dof while pd use 1 dof\n",
    "    volatility = ds['close'].std('timeslot', ddof=1) * np.sqrt(T)\n",
    "\n",
    "    # Compute average volume:\n",
    "    mean_volume = day_volume_sum / T\n",
    "\n",
    "    # Daily return that compares the first open and the last close\n",
    "    daily_close_price = ds['close'].sel(timeslot=T, drop=True)\n",
    "    daily_open_price = ds['open'].sel(timeslot=1, drop=True)\n",
    "    daily_high_price = ds['high'].max(dim='timeslot')\n",
    "    daily_low_price = ds['low'].min(dim='timeslot')\n",
    "\n",
    "    previous_close_price = daily_close_price.shift(day=1)\n",
    "    previous_close_price[dict(day=0)] = daily_open_price.isel(day=0)\n",
    "    daily_return = daily_close_price / previous_close_price - 1\n",
    "\n",
    "    return Dataset(dict(\n",
    "        avg_price=avg_price,\n",
    "        volatility=volatility,\n",
    "        mean_volume=mean_volume,\n",
    "        close_0=daily_close_price,\n",
    "        open_0=daily_open_price,\n",
    "        high_0=daily_high_price,\n",
    "        low_0=daily_low_price,\n",
    "        return_0=daily_return,\n",
    "    ))\n",
    "\n",
    "\n",
    "def check_dataframe(df, expect_index=None, expect_feature=None, shutup=True):\n",
    "    \"\"\"\n",
    "    Check if the input DataFrame contains NaN, and check the desired index and features if provided\n",
    "    :param df[pd.DataFrame]: input dataframe to check\n",
    "    :param expect_index[list]: input list for expected indices in df\n",
    "    :param expect_feature[list]: input list for expected features in df\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if expect_index is not None and df.index.names != expect_index:\n",
    "        raise ValueError('Expecting index as {} but got {}'.format(expect_index, list[df.index.names]))\n",
    "    else:\n",
    "        if not shutup:\n",
    "            print('Indices matched')\n",
    "\n",
    "    if expect_feature is not None and not set(expect_feature).issubset(set(df.columns)):\n",
    "        raise ValueError(f'Expecting feature as {expect_feature} but got {df.columns}')\n",
    "    else:\n",
    "        if not shutup:\n",
    "            print('Features matched')\n",
    "\n",
    "    if df.isnull().values.any():\n",
    "        raise ValueError('DataFrame still contains NaN')\n",
    "    if not shutup:\n",
    "        print('DataFame is all good for the tests')\n",
    "\n",
    "\n",
    "def calculate_market_return_legacy(\n",
    "        df: DataFrame, return_0_column: str = 'return_0', weight: Optional[np.ndarray] = None,\n",
    ") -> Series:\n",
    "    \"\"\"\n",
    "    Calculate the market daily return by combining the return for each asset according to the given weight.\n",
    "\n",
    "    :param df: Dataframe containing the daily return data.\n",
    "    :param return_0_column:\n",
    "    :param weight:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    check_dataframe(df, expect_index=['day', 'asset'], expect_feature=[return_0_column])\n",
    "    if weight is not None:\n",
    "        raise ValueError('Currently, only `weight=None` (i.e. simple average) is supported.')\n",
    "    else:\n",
    "        # Simple return\n",
    "        market_return = df.groupby(level=0)[return_0_column].mean()\n",
    "\n",
    "    return market_return.rename(f'market_{return_0_column}')\n",
    "\n",
    "\n",
    "def calculate_market_return(return_0: DataArray, weight: Optional[np.ndarray] = None, ) -> DataArray:\n",
    "    \"\"\"\n",
    "    Calculate the market daily return by combining the return for each asset according to the given weight.\n",
    "    \"\"\"\n",
    "    # assert {'day', 'asset'}.issubset(set(return_0.dims))\n",
    "    if weight is not None:\n",
    "        raise ValueError('Currently, only `weight=None` (i.e. simple average) is supported.')\n",
    "    else:\n",
    "        # Simple return\n",
    "        market_return = return_0.mean(dim='asset')\n",
    "\n",
    "    return market_return.rename(f'market_return_0')\n",
    "\n",
    "def generate_sbumission(pred: Series, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    generate csv for Kaggle submission\n",
    "    :param pred: input prediction from eval_to_submission; contain columns 'day' 'asset' and 'return_pred'\n",
    "    :param filename: path to the saved file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred['date_time'] = 's' + pred['asset'].astype(str) + 'd' + pred['day'].astype(str)\n",
    "    pred = pred.drop(['day', 'asset'], axis=1).rename(columns={'return_pred': 'return'})\n",
    "    pred = pred[['date_time', 'return']]\n",
    "    pred.to_csv(filename, index=False)\n",
    "\n",
    "class Test(TestCase):\n",
    "    def test_export_extract_market(self):\n",
    "        ds = xr.open_dataset('data/nc/base.nc')\n",
    "        market_brief = extract_market_data(ds[['money', 'volume', 'close', 'open', 'high', 'low']])\n",
    "        market_brief.to_netcdf('data/nc/market_brief.nc')\n",
    "\n",
    "    def test_compare_extract_market(self):\n",
    "        market_brief = xr.open_dataset('data/nc/market_brief.nc')\n",
    "        dataset = pipeline.Dataset.load('data/parsed')\n",
    "        market_legacy = extract_market_data_legacy(dataset.market)\n",
    "        for col in market_legacy:\n",
    "            assert np.isclose(market_brief.to_dataframe()[col], market_legacy[col]).all()\n",
    "\n",
    "    def test_checkdata(self):\n",
    "        from pipeline import load_mini_dataset\n",
    "\n",
    "        dataset = load_mini_dataset('./data/parsed_mini', 10)\n",
    "        quantized_fundamental, _ = data_quantization(dataset.fundamental)\n",
    "        df = pd.concat([quantized_fundamental, dataset.fundamental, dataset.ref_return], axis=1).dropna()\n",
    "        quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                            'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE']\n",
    "        original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf']\n",
    "        quantile_feature.extend(original_feature)\n",
    "        quantile_feature.append('return')\n",
    "        check_dataframe(df, expect_index=['day', 'asset'], expect_feature=df.columns)\n",
    "        check_dataframe(df, expect_index=['day', 'asset'], expect_feature=quantile_feature)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "from datetime import date\n",
    "from typing import Optional, Callable, Tuple\n",
    "from unittest import TestCase\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import xarray as xr\n",
    "from xarray import Dataset, DataArray\n",
    "\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "def _num_flat_features(x):\n",
    "    size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "    num_features = 1\n",
    "    for s in size:\n",
    "        num_features *= s\n",
    "    return num_features\n",
    "\n",
    "\n",
    "def _device_helper(is_cuda):\n",
    "    return 'cuda' if is_cuda else 'cpu'\n",
    "\n",
    "\n",
    "class CUDAModule(nn.Module):\n",
    "    def __init__(self, is_cuda: bool):\n",
    "        super().__init__()\n",
    "        self.is_cuda = is_cuda\n",
    "        self.device = _device_helper(self.is_cuda)\n",
    "        self._has_moved = False\n",
    "\n",
    "    def move(self):\n",
    "        self._has_moved = True\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward_device_independent(self, x: torch.Tensor):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not self._has_moved:\n",
    "            print('This module might have not been moved to the correct device!')\n",
    "\n",
    "        return self.forward_device_independent(x.to(self.device)).to('cpu')\n",
    "\n",
    "\n",
    "class MLP(CUDAModule):\n",
    "    \"\"\"\n",
    "    Fully connected NN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, D_out, input_shape, is_cuda: bool = True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param D_out: number of classes label\n",
    "        :param input_shape:\n",
    "        :param is_cuda:\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__(is_cuda)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_shape[1] * input_shape[2], 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, D_out)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # self.final_layer = nn.Softmax()\n",
    "\n",
    "        self.move()\n",
    "\n",
    "    def forward_device_independent(self, x):\n",
    "        x = x.contiguous().view(-1, _num_flat_features(x))\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.dropout(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "        # x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LSTM(CUDAModule):\n",
    "    def __init__(self, num_output, num_features, hidden_size, num_layers, is_cuda: bool = False):\n",
    "        \"\"\"\n",
    "\n",
    "        Remark:\n",
    "            1. hidden_size is like embedding feature space dimension\n",
    "            2. better > num_assets?\n",
    "                # if num_layers = 2: stack 2 LSTM of lyaer 1:\n",
    "                # nn.Sequential(OrderedDict([\n",
    "                #     ('LSTM1', nn.LSTM(input_size, hidden_size, 1),\n",
    "                #     ('LSTM2', nn.LSTM(hidden_size, hidden_size, 1)\n",
    "                #     ]))\n",
    "        :param num_output:\n",
    "        :param num_features:\n",
    "        :param hidden_size:\n",
    "        :param num_layers:\n",
    "        :param is_cuda:\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__(is_cuda)\n",
    "        self.num_output = num_output\n",
    "        self.num_layers = num_layers\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size = hidden_size\n",
    "        # LSTM input size (batch_first) (batch, seq_length, feature)\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "        self.move()\n",
    "\n",
    "    def forward_device_independent(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Remark: the second dim for h_0 and c_0 is the batch dim\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        # TODO: review: Variable if deprecated\n",
    "        # https://pytorch.org/docs/stable/autograd.html?highlight=variable#variable-deprecated\n",
    "        h_0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=self.device)  # hidden state\n",
    "        c_0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=self.device)  # cell state\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        # get only the last hidden layer, needed for multiple layer\n",
    "        h_out = h_out[-1, :, :].view(-1, self.hidden_size)\n",
    "\n",
    "        out = self.fc(self.dropout(h_out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class oneDVerConvNet(CUDAModule):\n",
    "    def __init__(self, D_in, D_out, input_shape, b_size, is_cuda: bool = True, activation: str = 'relu'):\n",
    "        # input_shape: without batch dimension\n",
    "        # b_size: batch size\n",
    "        super(oneDVerConvNet, self).__init__(is_cuda)\n",
    "        self.activation = nn.ReLU() if activation == 'relu' else nn.Tanh()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(D_in, 16, (3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(16),  # no batch\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 1024, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        n_size = self._get_conv_output(input_shape, b_size)\n",
    "        self.fc1 = nn.Linear(n_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, D_out)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.move()\n",
    "\n",
    "    def _get_conv_output(self, shape, b_size):\n",
    "        input = Variable(torch.rand(b_size, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(b_size, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # print(x.shape) # shape = (b_size, feature=1024, H=1, W = 54)\n",
    "        x = torch.max(x, 3)[0]\n",
    "        return x\n",
    "\n",
    "    def forward_device_independent(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = torch.max(x, 3)[0]  # questionable maybe do along [1]\n",
    "\n",
    "        x = x.view(-1, _num_flat_features(x))\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.drop_out(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        # x = F.relu(self.transform1(x))\n",
    "        # x = F.relu(self.drop_out(self.transform2(x)))\n",
    "        # x = self.transform3(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "class Transformer(CUDAModule):\n",
    "    def __init__(self, n_class: int, seq_length: int, feature: list, hidden_size: int, output_size: int,\n",
    "                 num_layer: int = 1, is_cuda: bool = False, offset_feature_size: Optional[int] = None):\n",
    "        super(Transformer, self).__init__(is_cuda)\n",
    "        self.n_feature = len(feature)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.class_dim = n_class\n",
    "        self.time_dim = seq_length\n",
    "        # self.class_embed = nn.Embedding(self.class_dim, self.) # try without embedding first\n",
    "\n",
    "        self.embed_feature_dim = self.n_feature  # no embedding\n",
    "        ## one-hot encoding and positional encoding:\n",
    "        if 'offset_feature' in feature:\n",
    "            self.is_time_embed = True\n",
    "#             self.time_embed = nn.Embedding(offset_feature_size, self.time_dim)\n",
    "#             self.embed_feature_dim += self.time_dim-1\n",
    "            self.time_embed = nn.Embedding(offset_feature_size, 8)\n",
    "            self.embed_feature_dim += 7\n",
    "            self.day_feature_indx = feature.index('offset_feature')\n",
    "        else:\n",
    "            self.is_time_embed = False\n",
    "\n",
    "        if 'asset_name' in feature:\n",
    "            self.embed_feature_dim += 53\n",
    "            self.is_asset_embed = True\n",
    "            self.asset_name_indx = feature.index('asset_name')\n",
    "        else:\n",
    "            self.is_asset_embed = False\n",
    "                    \n",
    "        if 'season_day' in feature:\n",
    "            self.is_season_embed = True\n",
    "            self.season_embed = nn.Embedding(65, 8)\n",
    "            self.embed_feature_dim += 7\n",
    "            self.season_day_indx = feature.index('season_day')\n",
    "        else:\n",
    "            self.is_season_embed = False\n",
    "            \n",
    "        print('embedding feature dimension is', self.embed_feature_dim)\n",
    "\n",
    "        self.linear_1 = nn.Linear(self.embed_feature_dim, self.hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(self.time_dim)\n",
    "#         self.drop_out = nn.Dropout1d(p=0.2)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_size,\n",
    "                                                   nhead=8,\n",
    "                                                   dropout=0.0,\n",
    "                                                   batch_first=True,\n",
    "                                                   )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, self.num_layer)\n",
    "        self.linear_2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.move()\n",
    "\n",
    "    def forward_device_independent(self, x: torch.Tensor):\n",
    "        columns_indx = torch.arange(x.shape[-1]).long()\n",
    "        # print('original indx', columns_indx)\n",
    "        indx_remain_bool = torch.ones(columns_indx.shape).long()\n",
    "#         print(x.shape)\n",
    "\n",
    "        if self.is_time_embed:\n",
    "#             print(\"offset index\", self.day_feature_indx)\n",
    "            day_feature = x[:, :, self.day_feature_indx].long()\n",
    "            day_feature = self.time_embed(day_feature)\n",
    "            indx_remain_bool *= (columns_indx != self.day_feature_indx)\n",
    "\n",
    "        if self.is_asset_embed:\n",
    "#             print(\"asset index:\", self.asset_name_indx)\n",
    "            asset_name = x[:, :, self.asset_name_indx]\n",
    "            # print(x)\n",
    "            # print(asset_name)\n",
    "            # print(asset_name.max())\n",
    "            assert asset_name.max().item() <= 54\n",
    "            asset_name = F.one_hot(asset_name.long(), 54)\n",
    "            indx_remain_bool *= (columns_indx != self.asset_name_indx)\n",
    "            \n",
    "        if self.is_season_embed:\n",
    "            season_day = x[:, :, self.season_day_indx].long()\n",
    "            season_day = self.season_embed(season_day)\n",
    "            indx_remain_bool *= (columns_indx != self.season_day_indx)\n",
    "        \n",
    "\n",
    "        # print('remain indices:', columns_indx[indx_remain_bool.long()==1])\n",
    "\n",
    "        # get data out except for embedded ones\n",
    "        x = x[:, :, indx_remain_bool.long() == 1]\n",
    "        # print(x.shape)\n",
    "        if self.is_time_embed:\n",
    "            x = torch.cat((x, day_feature), dim=-1)\n",
    "        if self.is_asset_embed:\n",
    "            x = torch.cat((x, asset_name), dim=-1)\n",
    "        if self.is_season_embed:\n",
    "            x = torch.cat((x, season_day), dim=-1)\n",
    "            \n",
    "        # print(x.shape) # (432, 4, 71)\n",
    "#         x = self.drop_out(x)\n",
    "        x = self.linear_1(x)\n",
    "#         x = self.drop_out(x)\n",
    "#         x = self.bn1(x)\n",
    "        h = self.encoder(x)\n",
    "        h = h.mean(dim=1)\n",
    "        outputs = self.linear_2(h)\n",
    "        return outputs.squeeze()\n",
    "\n",
    "\n",
    "class SimpleLinearNetwork(nn.Module):\n",
    "    def __init__(self, D_in):\n",
    "        super(SimpleLinearNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(D_in, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x[:, :, 0, :], 1, 2)\n",
    "        return self.fc(x).squeeze()\n",
    "\n",
    "\n",
    "def easy_winsorize(s: Series, trim_ratio: float = 0.05):\n",
    "    return np.clip(s, s.quantile(trim_ratio), s.quantile(1 - trim_ratio))\n",
    "\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def unskew(self, X: DataFrame):\n",
    "        return DataFrame({\n",
    "            'log_turnoverRatio': np.log(X['turnoverRatio']),\n",
    "            'log_transactionAmount_pos': np.log(X['transactionAmount'] + 1),\n",
    "            'log_pb': np.log(X['pb']),\n",
    "            'log_ps': np.log(X['ps']),\n",
    "            # 'winsor_pe_ttm': easy_winsorize(X['pe_ttm']),\n",
    "            'winsor_pe': easy_winsorize(X['pe']),\n",
    "            'winsor_pcf': easy_winsorize(X['pcf']),\n",
    "        })\n",
    "\n",
    "    def working(self, X: DataFrame, t):\n",
    "        # return DataFrame(t(self.unskew(X)), index=X.index, columns=list(set(X.columns) - {'return_0+1'})\n",
    "        #                  ).merge(X['return_0+1'], on=['day', 'asset'])\n",
    "        tX = self.unskew(X)\n",
    "        aa = (X['return_0'] + 1) * (X['return_1'] + 1) - 1\n",
    "        df = DataFrame(t(tX), index=tX.index, columns=tX.columns)\n",
    "        df['return_0+1'] = aa\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X: DataFrame):\n",
    "        return self.working(X, lambda Y: self.scaler.fit_transform(Y))\n",
    "\n",
    "    def transform(self, X: DataFrame):\n",
    "        return self.working(X, lambda Y: self.scaler.transform(Y))\n",
    "\n",
    "\n",
    "    \n",
    "class TransformerOpt:\n",
    "    \"Optim wrapper that implements rate for Transformer.\"\n",
    "    def __init__(self, optimizer, hidden_size, warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup_steps\n",
    "        self.model_size = hidden_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the warmup scheduler as a :class:`dict`.\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer.\n",
    "        \"\"\"\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the warmup scheduler's state.\n",
    "        Arguments:\n",
    "            state_dict (dict): warmup scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state_dict) \n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5))) \n",
    "    \n",
    "def _get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class Neg_Pearson_Loss(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Neg_Pearson_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # loss_without_reduction = max(0, target * (input1  input2) + margin)\n",
    "        pearson = PearsonCorrCoef()\n",
    "        return -pearson(pred.squeeze(), target.squeeze())\n",
    "\n",
    "\n",
    "class MonStEr_Loss(nn.Module):\n",
    "    def __int__(self) -> None:\n",
    "        super(MonStEr_Loss, self).__int__()\n",
    "\n",
    "    def forward(self, pred_, target_):\n",
    "        pred = pred_/0.2\n",
    "        target = target_/0.2\n",
    "        loss = 0\n",
    "        # is_tar_pos = target >= 0\n",
    "        # is_pred_ge_tar = pred >= target\n",
    "\n",
    "        is_dir_correct= torch.sign(pred-target) == torch.sign(target)\n",
    "        # print(target.max())\n",
    "        loss += torch.sum(is_dir_correct*(1/(1+0.1*torch.abs(target)))*torch.abs(pred-target)\\\n",
    "             + (~is_dir_correct)*torch.pow(torch.abs(target - pred), 1+4*torch.abs(target)))\n",
    "\n",
    "\n",
    "        # loss += torch.sum(is_tar_pos * is_pred_ge_tar * (1 / (1 + 4 * target) * (pred - target)) \\\n",
    "        #         + is_tar_pos * (~is_pred_ge_tar) * torch.pow(target - pred, 1 + target) \\\n",
    "        #         + (~is_tar_pos) * is_pred_ge_tar * torch.pow(target - pred, 1 - target) \\\n",
    "        #         + (~is_tar_pos) * (~is_pred_ge_tar) * (1 / (1 + 4 * target) * (target - pred)))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class NN_wrapper:\n",
    "    def __init__(\n",
    "            self, preprocess, lr=0.001, criterion=nn.MSELoss(), n_epoch=5, train_lookback=32, per_eval_lookback=16,\n",
    "            n_asset=54, hidden_size=64, lr_scheduler_constructor: Optional[Callable] = None, network='LSTM',\n",
    "            is_eval: bool = False, feature_name=None, load_model_path=None, is_cuda=True, embed_season: bool = False,\n",
    "            embed_offset: bool = False, embed_asset: bool = False, l2_weight=1e-5, l1_weight=0, var_weight=0\n",
    "    ):\n",
    "        if feature_name is None:\n",
    "            raise ValueError('Please provide a list of feature')\n",
    "        feature_to_nn = feature_name.copy()\n",
    "        if embed_offset:\n",
    "            feature_to_nn += ['offset_feature']\n",
    "        if embed_asset:\n",
    "            feature_to_nn += ['asset_name']\n",
    "#         if embed_season:\n",
    "#             feature_to_nn += ['season_day']\n",
    "        n_feature = len(feature_name)\n",
    "\n",
    "        if network == 'LSTM':\n",
    "            self.net = LSTM(num_output=1, num_features=n_feature, hidden_size=hidden_size, num_layers=1,\n",
    "                            is_cuda=is_cuda)\n",
    "        elif network == 'MLP':\n",
    "            self.net = MLP(D_out=1, input_shape=[n_asset * train_lookback, per_eval_lookback, n_feature],\n",
    "                           is_cuda=is_cuda)\n",
    "        elif network == 'CONV':\n",
    "            self.net = oneDVerConvNet(D_in=1, D_out=1, input_shape=[1, per_eval_lookback, n_feature],\n",
    "                                      b_size=train_lookback * n_asset, is_cuda=is_cuda)\n",
    "        elif network == 'Transformer':\n",
    "            self.net = Transformer(n_class=n_asset, seq_length=per_eval_lookback, feature=feature_to_nn,\n",
    "                                   hidden_size=hidden_size, output_size=1, is_cuda=is_cuda,\n",
    "                                   offset_feature_size=per_eval_lookback)\n",
    "        else:\n",
    "            raise ValueError('Network architecture not supported')\n",
    "\n",
    "        if is_eval:\n",
    "            if load_model_path is None:\n",
    "                raise ValueError('Please provide model path to load')\n",
    "            else:\n",
    "                self.net.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "        if load_model_path is not None:\n",
    "            self.net.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "        self.is_eval = is_eval\n",
    "        self.feature_name = feature_name\n",
    "        self.feature_to_nn = feature_to_nn\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "        if criterion == 'pearson':\n",
    "            self.criterion = Neg_Pearson_Loss()\n",
    "        elif criterion == 'monster':\n",
    "            self.criterion = MonStEr_Loss()\n",
    "        elif criterion == 'combined':\n",
    "            self.criterion = 'combined'\n",
    "        else:\n",
    "            self.criterion = criterion\n",
    "\n",
    "        self.train_lookback = train_lookback\n",
    "        self.per_eval_lookback = per_eval_lookback\n",
    "        self.n_asset = n_asset\n",
    "        self.net_name = network\n",
    "        self.embed_asset = embed_asset\n",
    "        self.embed_offset = embed_offset\n",
    "        self.embed_season = embed_season\n",
    "\n",
    "        # regularization parameter\n",
    "        self.l1_weight = l1_weight\n",
    "        self.var_weight = var_weight\n",
    "\n",
    "        # Define the optimizier\n",
    "        ## optimizer with L2-regularization\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=l2_weight)\n",
    "#         self.optimizer = optim.Adagrad(self.net.parameters(), lr=lr, weight_decay=l2_weight, eps=1e-9)\n",
    "#         self.optimizer = optim.SGD(self.net.parameters(), lr=lr, momentum=0.9, weight_decay=l2_weight)\n",
    "    \n",
    "        ## original optimizer\n",
    "        # self.optimizer = optim.Adam(self.net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "        \n",
    "        # self.scheduler = optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=lr / 10, max_lr=lr,\n",
    "        #                                              step_size_up=n_epoch // 2, cycle_momentum=False)\n",
    "        if lr_scheduler_constructor is None:\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100000, gamma=0.5)\n",
    "        elif lr_scheduler_constructor == \"warmup\":\n",
    "            self.scheduler = TransformerOpt(self.optimizer, hidden_size, 3000)\n",
    "        else:\n",
    "            self.scheduler = lr_scheduler_constructor(self.optimizer)\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.is_learning = True\n",
    "        # self.early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "\n",
    "    def prepare_X(self, X: Dataset, fit: bool, lookback: int) -> Tuple[torch.Tensor, DataArray]:\n",
    "        start_day = X.day.min().to_numpy().item()\n",
    "\n",
    "        X_pd = X[self.feature_name].to_dataframe(dim_order=['day', 'asset'])\n",
    "\n",
    "        ## Minmax scale except for the asset name (category)\n",
    "        X_pd[X_pd.columns] = self.preprocess.fit_transform(X_pd.values) if fit else self.preprocess.transform(\n",
    "            X_pd.values)\n",
    "        X_transformed = xr.Dataset.from_dataframe(X_pd)\n",
    "\n",
    "        # New data preprocessing with Xarray - Hooray!\n",
    "        X_list = []\n",
    "        for i in range(lookback):\n",
    "            X_slice = X_transformed.sel(\n",
    "                day=slice(start_day + i, start_day + i + self.per_eval_lookback - 1)).expand_dims(\n",
    "                batch=[start_day + i + self.per_eval_lookback - 1])\n",
    "            X_slice.coords['offset'] = X_slice.day - start_day - i  # calculate offset coordinate/index for each slice\n",
    "            X_slice_o = X_slice.swap_dims(\n",
    "                {'day': 'offset'})  # swap to make offset the dimension instead of the previous 'day'\n",
    "            X_list.append(X_slice_o.reset_coords(drop=True))  # (reset and) drop all non-index coordinates\n",
    "        X_concat = xr.concat(X_list, dim='batch')  # concat along the batch dimension\n",
    "        if self.embed_offset:\n",
    "            X_concat = X_concat.assign(offset_feature=X_concat.offset)\n",
    "        if self.embed_asset:\n",
    "            X_concat = X_concat.assign(asset_name=X_concat.asset)\n",
    "#         if self.embed_season:\n",
    "#             X_concat = X_concat.assign(season_day=X_concat.day)\n",
    "#             X_concat['season_day'] = X_concat['season_day'] % 65\n",
    "            \n",
    "        # create a new batch dimension cartesian product all batch and asset\n",
    "        X_arr = (X_concat.stack({'batch_asset': ['batch', 'asset']}).to_array('feature')\n",
    "                 .transpose('batch_asset', 'offset', 'feature'))\n",
    "        X_arr_np = X_arr.to_numpy()\n",
    "\n",
    "        if self.net_name == 'CONV':\n",
    "            X_arr_np = X_arr_np[:, np.newaxis, :, :]\n",
    "\n",
    "        # then reshape into the desire form for NN (batch, seq_length, feature)\n",
    "        # batch_asset \"is\" a multi-index array, with the ordering of train_lookback, asset\n",
    "        X_tensor = torch.from_numpy(X_arr_np).to(torch.float)\n",
    "        return X_tensor, X_arr.batch_asset\n",
    "\n",
    "    def fit_predict(self, X: Dataset, y: DataArray):\n",
    "        self.net.train()\n",
    "        X_tensor, batch_asset = self.prepare_X(X, fit=True, lookback=self.train_lookback)\n",
    "        y_tensor = torch.from_numpy(y.stack({'batch_asset': ['day', 'asset']}).values).to(torch.float)\n",
    "\n",
    "\n",
    "        # LSTM or NN shape (batch, seq_length, feature)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.net(X_tensor).squeeze()\n",
    "\n",
    "            if self.criterion == 'combined':\n",
    "                mse_loss = nn.MSELoss()\n",
    "#                 mse_loss = nn.L1Loss()\n",
    "                pearson_loss = Neg_Pearson_Loss()\n",
    "                loss = mse_loss(outputs, y_tensor) + pearson_loss(outputs, y_tensor)\n",
    "            else:\n",
    "                loss = self.criterion(outputs, y_tensor)\n",
    "\n",
    "            if epoch == 1:\n",
    "                print('training loss:', loss.item())\n",
    "\n",
    "            ## add L1 regularization\n",
    "            if self.l1_weight > 0:\n",
    "                l1_norm = sum(torch.linalg.norm(p, 1) for p in self.net.parameters())\n",
    "                loss += self.l1_weight * l1_norm\n",
    "\n",
    "            if self.var_weight > 0 and isinstance(self.criterion, nn.MSELoss):\n",
    "                # print('variance is computed')\n",
    "                loss += self.var_weight * torch.abs(torch.var(outputs) - torch.var(y_tensor))\n",
    "\n",
    "            ## check parameter values\n",
    "            # print(list(self.net.parameters()))\n",
    "\n",
    "            ## check parameter name and require_grad\n",
    "            # for name, param in self.net.named_parameters():\n",
    "            #     print(name, param.requires_grad)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(self.net.parameters(), 2, norm_type='inf')\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "        if X.day.max().to_numpy().item() >= 994 and not self.is_eval:  # save final model\n",
    "            dump_folder = 'model/dump/' + str(date.today())\n",
    "            self.model_path = dump_folder + '/' + str(date.today()) + '_' + self.net_name\n",
    "            ensure_dir(dump_folder)\n",
    "            torch.save(self.net.state_dict(), self.model_path)\n",
    "            print('Final learning rate:', _get_lr(self.optimizer))\n",
    "\n",
    "        return xr.DataArray(data=outputs.detach().numpy(), coords=dict(batch_asset=batch_asset)).unstack(\n",
    "            'batch_asset').rename({'batch': 'day'})\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.net.eval()\n",
    "        X_tensor, _ = self.prepare_X(X, fit=False, lookback=1)\n",
    "        y = self.net(X_tensor).squeeze()\n",
    "\n",
    "        # return np.clip(y.detach().numpy(), -0.2, 0.2)[np.newaxis, :]  # return a numpy array\n",
    "        return y.detach().numpy()[np.newaxis, :]  # for predicting correlation\n",
    "\n",
    "\n",
    "class Test(TestCase):\n",
    "    def test_nn(self):\n",
    "        \"\"\"\n",
    "        Adapted from `playground/disaspeare/Feb_17_NN_MLP.ipynb`\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        from pipeline import Dataset\n",
    "        import pandas as pd\n",
    "        from torch import nn\n",
    "        from datatools import check_dataframe\n",
    "        from pipeline.backtest import cross_validation\n",
    "        from matplotlib import pyplot as plt\n",
    "        from visualization.metric import plot_performance\n",
    "\n",
    "        dataset = Dataset.load('../data/parsed')\n",
    "        df = pd.concat([dataset.fundamental, dataset.ref_return], axis=1).dropna()\n",
    "\n",
    "        check_dataframe(df, expect_index=['day', 'asset'])\n",
    "        df['return_0+1'] = df['return'].shift(2 * 54).fillna(0)\n",
    "\n",
    "        train_lookback = 64\n",
    "        eval_lookback = 16\n",
    "        n_epoch = 100\n",
    "        lr = 0.001\n",
    "        criterion = nn.MSELoss()\n",
    "        # optimizer = 'LBFGS'\n",
    "        optimizer = 'ADAM'\n",
    "        original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe', 'pcf', 'return_0+1']\n",
    "        n_asset = 54\n",
    "\n",
    "        pp = DataPreprocessing()\n",
    "        my_nn = NN_wrapper(pp, None, eval_lookback, train_lookback, optimizer=optimizer, learning_rate=lr,\n",
    "                           criterion=criterion, n_epoch=n_epoch, n_asset=n_asset, n_feature=len(original_feature))\n",
    "        performance, pred_df = cross_validation(my_nn, feature_columns=original_feature,\n",
    "                                                df=df.query(f'asset < {n_asset}'),\n",
    "                                                per_eval_lookback=eval_lookback, train_lookback=train_lookback)\n",
    "\n",
    "        plt.figure()\n",
    "        plot_performance(performance, metrics_selected=['train_r2', 'val_cum_r2', 'val_cum_pearson'])\n",
    "        plt.ylim([-0.15, 0.1])\n",
    "        plt.show()\n",
    "\n",
    "    def test_monster(self):\n",
    "        import torch\n",
    "\n",
    "        a=torch.rand(10)\n",
    "        b=torch.rand(10)\n",
    "        criterion = MonStEr_Loss()\n",
    "        loss = criterion(a,b)\n",
    "        print(a,'\\n')\n",
    "        print(b, '\\n')\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634fcc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:12:20.821194Z",
     "iopub.status.busy": "2023-03-15T19:12:20.820541Z",
     "iopub.status.idle": "2023-03-15T19:12:20.826649Z",
     "shell.execute_reply": "2023-03-15T19:12:20.825631Z"
    },
    "papermill": {
     "duration": 0.014503,
     "end_time": "2023-03-15T19:12:20.828880",
     "exception": false,
     "start_time": "2023-03-15T19:12:20.814377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch import optim\n",
    "\n",
    "from qids_package.qids import make_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb0d76e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:12:20.840022Z",
     "iopub.status.busy": "2023-03-15T19:12:20.839331Z",
     "iopub.status.idle": "2023-03-15T19:12:40.022164Z",
     "shell.execute_reply": "2023-03-15T19:12:40.021145Z"
    },
    "papermill": {
     "duration": 19.191534,
     "end_time": "2023-03-15T19:12:40.025061",
     "exception": false,
     "start_time": "2023-03-15T19:12:20.833527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Load necessary dataset and create them if not present\n",
    "path = '/kaggle/working/data/nc'\n",
    "base_nc_path = f'{path}/base.nc'\n",
    "if not Path(base_nc_path).exists():\n",
    "    _dump(is_mini=False, n_days=-1, path_prefix=\"/kaggle/working\")\n",
    "base_ds = xr.open_dataset(base_nc_path)\n",
    "\n",
    "market_brief_path = f'{path}/market_brief.nc'\n",
    "if not Path(market_brief_path).exists():\n",
    "    market_brief = extract_market_data(base_ds[['money', 'volume', 'close', 'open', 'high', 'low']])\n",
    "    market_brief.to_netcdf(market_brief_path)\n",
    "market_brief_ds = xr.open_dataset(market_brief_path)\n",
    "ds = base_ds.merge(market_brief_ds)\n",
    "\n",
    "fundamental_path = f'{path}/fundamental_v0.nc'\n",
    "if not Path(fundamental_path).exists():\n",
    "    fundamental_ds = calculate_fundamental_v0(ds)\n",
    "    fundamental_ds.to_netcdf(fundamental_path)\n",
    "else:\n",
    "    fundamental_ds = xr.open_dataset(fundamental_path)\n",
    "ds = ds.merge(fundamental_ds)\n",
    "# ds = ds.assign(season_day=ds.day)\n",
    "# ds['season_day'] = ds['season_day']%65\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d8f3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:12:40.036163Z",
     "iopub.status.busy": "2023-03-15T19:12:40.035832Z",
     "iopub.status.idle": "2023-03-15T19:12:40.042478Z",
     "shell.execute_reply": "2023-03-15T19:12:40.041365Z"
    },
    "papermill": {
     "duration": 0.015,
     "end_time": "2023-03-15T19:12:40.045057",
     "exception": false,
     "start_time": "2023-03-15T19:12:40.030057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Datatools extension\n",
    "def winsorize(X, q=0.1):\n",
    "    lower = X.quantile(q)\n",
    "    upper = X.quantile(1 - q)\n",
    "    return X.clip(lower, upper, axis=1)\n",
    "\n",
    "\n",
    "class CombinedScaler:\n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = winsorize(DataFrame(X))\n",
    "        return self.scaler.fit_transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = winsorize(DataFrame(X))\n",
    "        return self.scaler.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7ca1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:12:40.055471Z",
     "iopub.status.busy": "2023-03-15T19:12:40.055200Z",
     "iopub.status.idle": "2023-03-15T19:12:40.126271Z",
     "shell.execute_reply": "2023-03-15T19:12:40.125131Z"
    },
    "papermill": {
     "duration": 0.080284,
     "end_time": "2023-03-15T19:12:40.130060",
     "exception": false,
     "start_time": "2023-03-15T19:12:40.049776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> IS_CUDA=True <<<<<\n"
     ]
    }
   ],
   "source": [
    "##### CUDA\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "print(f'>>>> IS_CUDA={IS_CUDA} <<<<<')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582a7dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:12:40.143832Z",
     "iopub.status.busy": "2023-03-15T19:12:40.143181Z",
     "iopub.status.idle": "2023-03-15T19:12:40.151708Z",
     "shell.execute_reply": "2023-03-15T19:12:40.150556Z"
    },
    "papermill": {
     "duration": 0.017148,
     "end_time": "2023-03-15T19:12:40.154095",
     "exception": false,
     "start_time": "2023-03-15T19:12:40.136947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['turnoverRatio',\n",
       " 'transactionAmount',\n",
       " 'pe_ttm',\n",
       " 'pe',\n",
       " 'pb',\n",
       " 'ps',\n",
       " 'pcf',\n",
       " 'open',\n",
       " 'close',\n",
       " 'high',\n",
       " 'low',\n",
       " 'volume',\n",
       " 'money',\n",
       " 'return',\n",
       " 'avg_price',\n",
       " 'volatility',\n",
       " 'mean_volume',\n",
       " 'close_0',\n",
       " 'open_0',\n",
       " 'high_0',\n",
       " 'low_0',\n",
       " 'return_0',\n",
       " 'earnings_ttm',\n",
       " 'earnings',\n",
       " 'book',\n",
       " 'sales',\n",
       " 'cashflow',\n",
       " 'market_share',\n",
       " 'market_cap']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3349e98d",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-15T19:12:40.165170Z",
     "iopub.status.busy": "2023-03-15T19:12:40.164892Z",
     "iopub.status.idle": "2023-03-15T19:24:44.309730Z",
     "shell.execute_reply": "2023-03-15T19:24:44.308797Z"
    },
    "papermill": {
     "duration": 724.152859,
     "end_time": "2023-03-15T19:24:44.311891",
     "exception": false,
     "start_time": "2023-03-15T19:12:40.159032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding feature dimension is 68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f4398a3a764c6692e9e42ca4e05548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/779 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: -0.17869335412979126\n",
      "training loss: -0.30174222588539124\n",
      "training loss: -0.34048211574554443\n",
      "training loss: -0.31195497512817383\n",
      "training loss: -0.30805519223213196\n",
      "training loss: -0.3255724012851715\n",
      "training loss: -0.3039170205593109\n",
      "training loss: -0.25568804144859314\n",
      "training loss: -0.2633281350135803\n",
      "training loss: -0.3088696002960205\n",
      "training loss: -0.18904311954975128\n",
      "training loss: -0.29769131541252136\n",
      "training loss: -0.35028716921806335\n",
      "training loss: -0.34908345341682434\n",
      "training loss: -0.3472144603729248\n",
      "training loss: -0.3157746493816376\n",
      "training loss: -0.32196253538131714\n",
      "training loss: -0.3164793848991394\n",
      "training loss: -0.21273422241210938\n",
      "training loss: -0.2127295881509781\n",
      "training loss: -0.28493064641952515\n",
      "training loss: -0.3168867826461792\n",
      "training loss: -0.34707680344581604\n",
      "training loss: -0.3536568582057953\n",
      "training loss: -0.2918056547641754\n",
      "training loss: -0.2641312777996063\n",
      "training loss: -0.25688600540161133\n",
      "training loss: -0.3444454073905945\n",
      "training loss: -0.31387999653816223\n",
      "training loss: -0.3129439055919647\n",
      "training loss: -0.2693197727203369\n",
      "training loss: -0.21557970345020294\n",
      "training loss: -0.24533650279045105\n",
      "training loss: -0.20051780343055725\n",
      "training loss: -0.2733036279678345\n",
      "training loss: -0.32900163531303406\n",
      "training loss: -0.1725458800792694\n",
      "training loss: -0.2399028092622757\n",
      "training loss: -0.23624570667743683\n",
      "training loss: -0.2655647099018097\n",
      "training loss: -0.36681461334228516\n",
      "training loss: -0.43601879477500916\n",
      "training loss: -0.5157251954078674\n",
      "training loss: -0.5623048543930054\n",
      "training loss: -0.5278882384300232\n",
      "training loss: -0.48656097054481506\n",
      "training loss: -0.4672454297542572\n",
      "training loss: -0.45271456241607666\n",
      "training loss: -0.4327158033847809\n",
      "training loss: -0.43539679050445557\n",
      "training loss: -0.41283610463142395\n",
      "training loss: -0.41154399514198303\n",
      "training loss: -0.4491632282733917\n",
      "training loss: -0.4376102685928345\n",
      "training loss: -0.4209316670894623\n",
      "training loss: -0.4513338506221771\n",
      "training loss: -0.4537526071071625\n",
      "training loss: -0.48820921778678894\n",
      "training loss: -0.49107831716537476\n",
      "training loss: -0.5098056197166443\n",
      "training loss: -0.4204297363758087\n",
      "training loss: -0.3898993134498596\n",
      "training loss: -0.4832116663455963\n",
      "training loss: -0.5075482726097107\n",
      "training loss: -0.49117162823677063\n",
      "training loss: -0.46260732412338257\n",
      "training loss: -0.46966105699539185\n",
      "training loss: -0.4766010344028473\n",
      "training loss: -0.47940194606781006\n",
      "training loss: -0.47867515683174133\n",
      "training loss: -0.48620471358299255\n",
      "training loss: -0.4764028787612915\n",
      "training loss: -0.5028074383735657\n",
      "training loss: -0.47669801115989685\n",
      "training loss: -0.5190066695213318\n",
      "training loss: -0.5649085640907288\n",
      "training loss: -0.6118050217628479\n",
      "training loss: -0.47953125834465027\n",
      "training loss: -0.4575392007827759\n",
      "training loss: -0.4740873873233795\n",
      "training loss: -0.4554041624069214\n",
      "training loss: -0.39571329951286316\n",
      "training loss: -0.3983214497566223\n",
      "training loss: -0.39546939730644226\n",
      "training loss: -0.4109951853752136\n",
      "training loss: -0.3817426264286041\n",
      "training loss: -0.37155401706695557\n",
      "training loss: -0.38766276836395264\n",
      "training loss: -0.4087766110897064\n",
      "training loss: -0.4121786952018738\n",
      "training loss: -0.4687865078449249\n",
      "training loss: -0.43060439825057983\n",
      "training loss: -0.49549832940101624\n",
      "training loss: -0.45248550176620483\n",
      "training loss: -0.38765227794647217\n",
      "training loss: -0.4247487783432007\n",
      "training loss: -0.439107209444046\n",
      "training loss: -0.4771314263343811\n",
      "training loss: -0.5099997520446777\n",
      "training loss: -0.4511041045188904\n",
      "training loss: -0.4924013316631317\n",
      "training loss: -0.5013668537139893\n",
      "training loss: -0.4849889874458313\n",
      "training loss: -0.4855104088783264\n",
      "training loss: -0.4640774130821228\n",
      "training loss: -0.4248294234275818\n",
      "training loss: -0.4436130225658417\n",
      "training loss: -0.4705643653869629\n",
      "training loss: -0.5030650496482849\n",
      "training loss: -0.5080947875976562\n",
      "training loss: -0.5020200610160828\n",
      "training loss: -0.5075085163116455\n",
      "training loss: -0.45201563835144043\n",
      "training loss: -0.47787290811538696\n",
      "training loss: -0.5136093497276306\n",
      "training loss: -0.518992006778717\n",
      "training loss: -0.5152532458305359\n",
      "training loss: -0.5408098101615906\n",
      "training loss: -0.5151200890541077\n",
      "training loss: -0.5332809686660767\n",
      "training loss: -0.5402858257293701\n",
      "training loss: -0.5592775940895081\n",
      "training loss: -0.5784028172492981\n",
      "training loss: -0.5881202816963196\n",
      "training loss: -0.5663838386535645\n",
      "training loss: -0.6221966743469238\n",
      "training loss: -0.640221893787384\n",
      "training loss: -0.5240538716316223\n",
      "training loss: -0.411347359418869\n",
      "training loss: -0.4392695128917694\n",
      "training loss: -0.44133782386779785\n",
      "training loss: -0.4714573621749878\n",
      "training loss: -0.467379629611969\n",
      "training loss: -0.40137067437171936\n",
      "training loss: -0.46175169944763184\n",
      "training loss: -0.525932252407074\n",
      "training loss: -0.5098372101783752\n",
      "training loss: -0.5200396776199341\n",
      "training loss: -0.5215907692909241\n",
      "training loss: -0.5315357446670532\n",
      "training loss: -0.5576135516166687\n",
      "training loss: -0.5775571465492249\n",
      "training loss: -0.6123878359794617\n",
      "training loss: -0.6287853717803955\n",
      "training loss: -0.5640038251876831\n",
      "training loss: -0.5500823259353638\n",
      "training loss: -0.55478435754776\n",
      "training loss: -0.569926917552948\n",
      "training loss: -0.6218627691268921\n",
      "training loss: -0.5683220028877258\n",
      "training loss: -0.5457090735435486\n",
      "training loss: -0.5642256736755371\n",
      "training loss: -0.5837851762771606\n",
      "training loss: -0.5721302032470703\n",
      "training loss: -0.5459300875663757\n",
      "training loss: -0.5232159495353699\n",
      "training loss: -0.4872574210166931\n",
      "training loss: -0.5133101940155029\n",
      "training loss: -0.5261510610580444\n",
      "training loss: -0.5863564014434814\n",
      "training loss: -0.6496700644493103\n",
      "training loss: -0.6001545190811157\n",
      "training loss: -0.6018518209457397\n",
      "training loss: -0.6106869578361511\n",
      "training loss: -0.5962603092193604\n",
      "training loss: -0.6046100854873657\n",
      "training loss: -0.5633199214935303\n",
      "training loss: -0.588668704032898\n",
      "training loss: -0.5310938954353333\n",
      "training loss: -0.5685524344444275\n",
      "training loss: -0.5872567892074585\n",
      "training loss: -0.5863277912139893\n",
      "training loss: -0.5771445631980896\n",
      "training loss: -0.5948720574378967\n",
      "training loss: -0.5664270520210266\n",
      "training loss: -0.5629263520240784\n",
      "training loss: -0.5781052708625793\n",
      "training loss: -0.563971221446991\n",
      "training loss: -0.580515444278717\n",
      "training loss: -0.6180977821350098\n",
      "training loss: -0.607271134853363\n",
      "training loss: -0.5860542058944702\n",
      "training loss: -0.5712150931358337\n",
      "training loss: -0.5624744296073914\n",
      "training loss: -0.5708338022232056\n",
      "training loss: -0.4632284343242645\n",
      "training loss: -0.5151281356811523\n",
      "training loss: -0.5251917839050293\n",
      "training loss: -0.5437448620796204\n",
      "training loss: -0.577840268611908\n",
      "training loss: -0.5513365268707275\n",
      "training loss: -0.5097191333770752\n",
      "training loss: -0.5072967410087585\n",
      "training loss: -0.5442391037940979\n",
      "training loss: -0.5748710632324219\n",
      "training loss: -0.5688976049423218\n",
      "training loss: -0.5307328701019287\n",
      "training loss: -0.5539155602455139\n",
      "training loss: -0.5328006744384766\n",
      "training loss: -0.478889137506485\n",
      "training loss: -0.4529748857021332\n",
      "training loss: -0.47816675901412964\n",
      "training loss: -0.5254849195480347\n",
      "training loss: -0.5441220998764038\n",
      "training loss: -0.5581762790679932\n",
      "training loss: -0.511471688747406\n",
      "training loss: -0.46446144580841064\n",
      "training loss: -0.4950549304485321\n",
      "training loss: -0.5456458926200867\n",
      "training loss: -0.5418773293495178\n",
      "training loss: -0.5633518695831299\n",
      "training loss: -0.5264424085617065\n",
      "training loss: -0.5435970425605774\n",
      "training loss: -0.5427170395851135\n",
      "training loss: -0.5481674671173096\n",
      "training loss: -0.5449109077453613\n",
      "training loss: -0.5828955769538879\n",
      "training loss: -0.6172885894775391\n",
      "training loss: -0.5952091217041016\n",
      "training loss: -0.6146764159202576\n",
      "training loss: -0.6316514015197754\n",
      "training loss: -0.618127703666687\n",
      "training loss: -0.649459183216095\n",
      "training loss: -0.6398181915283203\n",
      "training loss: -0.5529419779777527\n",
      "training loss: -0.6131420731544495\n",
      "training loss: -0.6675209999084473\n",
      "training loss: -0.6819061040878296\n",
      "training loss: -0.6430169343948364\n",
      "training loss: -0.6274763941764832\n",
      "training loss: -0.5689053535461426\n",
      "training loss: -0.5077517032623291\n",
      "training loss: -0.5046162605285645\n",
      "training loss: -0.5384969115257263\n",
      "training loss: -0.5373035669326782\n",
      "training loss: -0.5221471786499023\n",
      "training loss: -0.5339667201042175\n",
      "training loss: -0.5605581998825073\n",
      "training loss: -0.6036050915718079\n",
      "training loss: -0.569305956363678\n",
      "training loss: -0.5531956553459167\n",
      "training loss: -0.561305820941925\n",
      "training loss: -0.5562297105789185\n",
      "training loss: -0.5690910816192627\n",
      "training loss: -0.529975950717926\n",
      "training loss: -0.5603432059288025\n",
      "training loss: -0.5692933201789856\n",
      "training loss: -0.5214877128601074\n",
      "training loss: -0.3463807702064514\n",
      "training loss: -0.42492222785949707\n",
      "training loss: -0.45563560724258423\n",
      "training loss: -0.5011351108551025\n",
      "training loss: -0.4845547378063202\n",
      "training loss: -0.4954146146774292\n",
      "training loss: -0.4812769889831543\n",
      "training loss: -0.5092058181762695\n",
      "training loss: -0.5264893174171448\n",
      "training loss: -0.4068167209625244\n",
      "training loss: -0.396112322807312\n",
      "training loss: -0.37707412242889404\n",
      "training loss: -0.41542112827301025\n",
      "training loss: -0.42433488368988037\n",
      "training loss: -0.40920740365982056\n",
      "training loss: -0.43368685245513916\n",
      "training loss: -0.4666192829608917\n",
      "training loss: -0.5142191052436829\n",
      "training loss: -0.5566248893737793\n",
      "training loss: -0.563317060470581\n",
      "training loss: -0.5479395985603333\n",
      "training loss: -0.580123245716095\n",
      "training loss: -0.6259530186653137\n",
      "training loss: -0.6309276223182678\n",
      "training loss: -0.6794098615646362\n",
      "training loss: -0.6271615028381348\n",
      "training loss: -0.5748037099838257\n",
      "training loss: -0.5653507709503174\n",
      "training loss: -0.5616763234138489\n",
      "training loss: -0.5679106712341309\n",
      "training loss: -0.5767890214920044\n",
      "training loss: -0.6024585366249084\n",
      "training loss: -0.6079319715499878\n",
      "training loss: -0.5815131068229675\n",
      "training loss: -0.6091112494468689\n",
      "training loss: -0.6204605102539062\n",
      "training loss: -0.6701921820640564\n",
      "training loss: -0.6627486944198608\n",
      "training loss: -0.6811604499816895\n",
      "training loss: -0.6717777848243713\n",
      "training loss: -0.6687065958976746\n",
      "training loss: -0.6491886377334595\n",
      "training loss: -0.6418192982673645\n",
      "training loss: -0.5338467359542847\n",
      "training loss: -0.5357741713523865\n",
      "training loss: -0.576401948928833\n",
      "training loss: -0.5994032621383667\n",
      "training loss: -0.5871784687042236\n",
      "training loss: -0.5813807249069214\n",
      "training loss: -0.5840648412704468\n",
      "training loss: -0.6347002387046814\n",
      "training loss: -0.6381933689117432\n",
      "training loss: -0.6101444363594055\n",
      "training loss: -0.59856116771698\n",
      "training loss: -0.5925692319869995\n",
      "training loss: -0.5471196174621582\n",
      "training loss: -0.5154557824134827\n",
      "training loss: -0.4412156939506531\n",
      "training loss: -0.4662216305732727\n",
      "training loss: -0.4820997416973114\n",
      "training loss: -0.47684210538864136\n",
      "training loss: -0.5171144008636475\n",
      "training loss: -0.5223370790481567\n",
      "training loss: -0.5292927622795105\n",
      "training loss: -0.5416325926780701\n",
      "training loss: -0.5181222558021545\n",
      "training loss: -0.5352842211723328\n",
      "training loss: -0.5552823543548584\n",
      "training loss: -0.5477450489997864\n",
      "training loss: -0.5285971760749817\n",
      "training loss: -0.49073347449302673\n",
      "training loss: -0.4834854304790497\n",
      "training loss: -0.49373093247413635\n",
      "training loss: -0.4996914565563202\n",
      "training loss: -0.48866742849349976\n",
      "training loss: -0.4823625981807709\n",
      "training loss: -0.49269452691078186\n",
      "training loss: -0.505377471446991\n",
      "training loss: -0.5305226445198059\n",
      "training loss: -0.5050930380821228\n",
      "training loss: -0.4936063885688782\n",
      "training loss: -0.49097883701324463\n",
      "training loss: -0.5063697099685669\n",
      "training loss: -0.5166698694229126\n",
      "training loss: -0.49163034558296204\n",
      "training loss: -0.49519237875938416\n",
      "training loss: -0.5405364632606506\n",
      "training loss: -0.5313633680343628\n",
      "training loss: -0.5871350169181824\n",
      "training loss: -0.6407479047775269\n",
      "training loss: -0.6054019927978516\n",
      "training loss: -0.6234964728355408\n",
      "training loss: -0.6392772793769836\n",
      "training loss: -0.6293790340423584\n",
      "training loss: -0.630510151386261\n",
      "training loss: -0.6405371427536011\n",
      "training loss: -0.6381624937057495\n",
      "training loss: -0.643402636051178\n",
      "training loss: -0.6230159401893616\n",
      "training loss: -0.6263807415962219\n",
      "training loss: -0.621065080165863\n",
      "training loss: -0.6264646649360657\n",
      "training loss: -0.6305748820304871\n",
      "training loss: -0.6471179723739624\n",
      "training loss: -0.6474117040634155\n",
      "training loss: -0.6296938061714172\n",
      "training loss: -0.6402589678764343\n",
      "training loss: -0.6564992666244507\n",
      "training loss: -0.6533861756324768\n",
      "training loss: -0.6590983867645264\n",
      "training loss: -0.6527950167655945\n",
      "training loss: -0.6545166969299316\n",
      "training loss: -0.6407224535942078\n",
      "training loss: -0.6545793414115906\n",
      "training loss: -0.6779101490974426\n",
      "training loss: -0.6595615744590759\n",
      "training loss: -0.6426562666893005\n",
      "training loss: -0.6484317779541016\n",
      "training loss: -0.6409757733345032\n",
      "training loss: -0.6329160928726196\n",
      "training loss: -0.6133517622947693\n",
      "training loss: -0.5807892084121704\n",
      "training loss: -0.5854309797286987\n",
      "training loss: -0.548767626285553\n",
      "training loss: -0.5729008913040161\n",
      "training loss: -0.5958773493766785\n",
      "training loss: -0.5965075492858887\n",
      "training loss: -0.5478608012199402\n",
      "training loss: -0.5476877093315125\n",
      "training loss: -0.546908974647522\n",
      "training loss: -0.5697566270828247\n",
      "training loss: -0.5605619549751282\n",
      "training loss: -0.5655942559242249\n",
      "training loss: -0.5747958421707153\n",
      "training loss: -0.5830539464950562\n",
      "training loss: -0.5388653874397278\n",
      "training loss: -0.5129523873329163\n",
      "training loss: -0.5416843891143799\n",
      "training loss: -0.5134814381599426\n",
      "training loss: -0.5326567888259888\n",
      "training loss: -0.588518500328064\n",
      "training loss: -0.5999279022216797\n",
      "training loss: -0.5648033022880554\n",
      "training loss: -0.5658473968505859\n",
      "training loss: -0.6046938896179199\n",
      "training loss: -0.5960894823074341\n",
      "training loss: -0.5633898377418518\n",
      "training loss: -0.4894567131996155\n",
      "training loss: -0.5053253173828125\n",
      "training loss: -0.5732926726341248\n",
      "training loss: -0.6288057565689087\n",
      "training loss: -0.6677187085151672\n",
      "training loss: -0.6961378455162048\n",
      "training loss: -0.6600841879844666\n",
      "training loss: -0.6460451483726501\n",
      "training loss: -0.6398023962974548\n",
      "training loss: -0.6412438154220581\n",
      "training loss: -0.6405436992645264\n",
      "training loss: -0.6562087535858154\n",
      "training loss: -0.6537296772003174\n",
      "training loss: -0.6580963730812073\n",
      "training loss: -0.6638363599777222\n",
      "training loss: -0.6337552666664124\n",
      "training loss: -0.5927348136901855\n",
      "training loss: -0.584041178226471\n",
      "training loss: -0.573997974395752\n",
      "training loss: -0.5610671639442444\n",
      "training loss: -0.5894775986671448\n",
      "training loss: -0.5952706933021545\n",
      "training loss: -0.6252114176750183\n",
      "training loss: -0.6473280191421509\n",
      "training loss: -0.6109604239463806\n",
      "training loss: -0.5864834785461426\n",
      "training loss: -0.5985299348831177\n",
      "training loss: -0.6133866906166077\n",
      "training loss: -0.6182259917259216\n",
      "training loss: -0.6197918057441711\n",
      "training loss: -0.6597005128860474\n",
      "training loss: -0.6849120855331421\n",
      "training loss: -0.6824043989181519\n",
      "training loss: -0.6786870360374451\n",
      "training loss: -0.6864723563194275\n",
      "training loss: -0.6671702861785889\n",
      "training loss: -0.676151692867279\n",
      "training loss: -0.6615937948226929\n",
      "training loss: -0.6725485920906067\n",
      "training loss: -0.6753948926925659\n",
      "training loss: -0.6655535101890564\n",
      "training loss: -0.6172665953636169\n",
      "training loss: -0.6330952644348145\n",
      "training loss: -0.5442545413970947\n",
      "training loss: -0.5874026417732239\n",
      "training loss: -0.5715768337249756\n",
      "training loss: -0.5653204321861267\n",
      "training loss: -0.5994555950164795\n",
      "training loss: -0.6283748745918274\n",
      "training loss: -0.6527277827262878\n",
      "training loss: -0.6347773671150208\n",
      "training loss: -0.6092021465301514\n",
      "training loss: -0.614380955696106\n",
      "training loss: -0.5893722772598267\n",
      "training loss: -0.5719711184501648\n",
      "training loss: -0.5494977235794067\n",
      "training loss: -0.5668084025382996\n",
      "training loss: -0.5916396379470825\n",
      "training loss: -0.588206946849823\n",
      "training loss: -0.5968731045722961\n",
      "training loss: -0.6377716064453125\n",
      "training loss: -0.6305277943611145\n",
      "training loss: -0.5709470510482788\n",
      "training loss: -0.5834652185440063\n",
      "training loss: -0.6265510320663452\n",
      "training loss: -0.3931308388710022\n",
      "training loss: -0.4496053457260132\n",
      "training loss: -0.5073021054267883\n",
      "training loss: -0.5328012108802795\n",
      "training loss: -0.5617270469665527\n",
      "training loss: -0.5824390053749084\n",
      "training loss: -0.6114367842674255\n",
      "training loss: -0.6554272770881653\n",
      "training loss: -0.6572960615158081\n",
      "training loss: -0.6916821002960205\n",
      "training loss: -0.723859965801239\n",
      "training loss: -0.7109647393226624\n",
      "training loss: -0.6717128753662109\n",
      "training loss: -0.6662043333053589\n",
      "training loss: -0.6479068994522095\n",
      "training loss: -0.618415892124176\n",
      "training loss: -0.6226931810379028\n",
      "training loss: -0.6098496317863464\n",
      "training loss: -0.6110168099403381\n",
      "training loss: -0.5887603759765625\n",
      "training loss: -0.6010732650756836\n",
      "training loss: -0.5912032723426819\n",
      "training loss: -0.5860457420349121\n",
      "training loss: -0.5959011912345886\n",
      "training loss: -0.6442622542381287\n",
      "training loss: -0.6128249764442444\n",
      "training loss: -0.6058264374732971\n",
      "training loss: -0.5786182880401611\n",
      "training loss: -0.6160320043563843\n",
      "training loss: -0.6291303634643555\n",
      "training loss: -0.6205580234527588\n",
      "training loss: -0.5877395868301392\n",
      "training loss: -0.5913423299789429\n",
      "training loss: -0.6027278900146484\n",
      "training loss: -0.6002234816551208\n",
      "training loss: -0.6289533376693726\n",
      "training loss: -0.6294736862182617\n",
      "training loss: -0.6339117288589478\n",
      "training loss: -0.6467767357826233\n",
      "training loss: -0.6505006551742554\n",
      "training loss: -0.6424665451049805\n",
      "training loss: -0.6362134218215942\n",
      "training loss: -0.6380272507667542\n",
      "training loss: -0.6206664443016052\n",
      "training loss: -0.6338541507720947\n",
      "training loss: -0.5902208685874939\n",
      "training loss: -0.5690194368362427\n",
      "training loss: -0.5704124569892883\n",
      "training loss: -0.5493347644805908\n",
      "training loss: -0.5996546745300293\n",
      "training loss: -0.6302109956741333\n",
      "training loss: -0.6244772672653198\n",
      "training loss: -0.6000542640686035\n",
      "training loss: -0.5963433980941772\n",
      "training loss: -0.5710741877555847\n",
      "training loss: -0.602361261844635\n",
      "training loss: -0.5929069519042969\n",
      "training loss: -0.46530160307884216\n",
      "training loss: -0.48689454793930054\n",
      "training loss: -0.5179049968719482\n",
      "training loss: -0.5246300101280212\n",
      "training loss: -0.5722312331199646\n",
      "training loss: -0.6389503479003906\n",
      "training loss: -0.675784707069397\n",
      "training loss: -0.6594210863113403\n",
      "training loss: -0.6673678755760193\n",
      "training loss: -0.678490936756134\n",
      "training loss: -0.6367291212081909\n",
      "training loss: -0.6445987820625305\n",
      "training loss: -0.6662725210189819\n",
      "training loss: -0.6960628032684326\n",
      "training loss: -0.7215672731399536\n",
      "training loss: -0.699902355670929\n",
      "training loss: -0.6727108955383301\n",
      "training loss: -0.6518309712409973\n",
      "training loss: -0.595276951789856\n",
      "training loss: -0.5858855247497559\n",
      "training loss: -0.5787790417671204\n",
      "training loss: -0.5332359671592712\n",
      "training loss: -0.5398429036140442\n",
      "training loss: -0.5554991364479065\n",
      "training loss: -0.5247705578804016\n",
      "training loss: -0.500700056552887\n",
      "training loss: -0.5047227740287781\n",
      "training loss: -0.47346481680870056\n",
      "training loss: -0.5127450227737427\n",
      "training loss: -0.5598068237304688\n",
      "training loss: -0.5908533334732056\n",
      "training loss: -0.5736572742462158\n",
      "training loss: -0.5739690065383911\n",
      "training loss: -0.6091068983078003\n",
      "training loss: -0.6565291881561279\n",
      "training loss: -0.6901793479919434\n",
      "training loss: -0.7204797267913818\n",
      "training loss: -0.7207745313644409\n",
      "training loss: -0.731472909450531\n",
      "training loss: -0.6932546496391296\n",
      "training loss: -0.7146561741828918\n",
      "training loss: -0.671271026134491\n",
      "training loss: -0.7110440731048584\n",
      "training loss: -0.6327358484268188\n",
      "training loss: -0.6315070986747742\n",
      "training loss: -0.6229007840156555\n",
      "training loss: -0.6407034993171692\n",
      "training loss: -0.650407075881958\n",
      "training loss: -0.6235870718955994\n",
      "training loss: -0.6023854613304138\n",
      "training loss: -0.5705167651176453\n",
      "training loss: -0.5777410268783569\n",
      "training loss: -0.4704514741897583\n",
      "training loss: -0.5001150369644165\n",
      "training loss: -0.5671032071113586\n",
      "training loss: -0.5848298668861389\n",
      "training loss: -0.5902177691459656\n",
      "training loss: -0.6077763438224792\n",
      "training loss: -0.6034893989562988\n",
      "training loss: -0.6057083010673523\n",
      "training loss: -0.581547737121582\n",
      "training loss: -0.6224915385246277\n",
      "training loss: -0.6528565287590027\n",
      "training loss: -0.6503459215164185\n",
      "training loss: -0.6201949119567871\n",
      "training loss: -0.6730998754501343\n",
      "training loss: -0.7101390361785889\n",
      "training loss: -0.7325859069824219\n",
      "training loss: -0.7327631115913391\n",
      "training loss: -0.7022613883018494\n",
      "training loss: -0.6744365096092224\n",
      "training loss: -0.6588079333305359\n",
      "training loss: -0.6779780983924866\n",
      "training loss: -0.672231912612915\n",
      "training loss: -0.6761332154273987\n",
      "training loss: -0.6618862748146057\n",
      "training loss: -0.6174539923667908\n",
      "training loss: -0.6145193576812744\n",
      "training loss: -0.6084240674972534\n",
      "training loss: -0.6149795651435852\n",
      "training loss: -0.6288191080093384\n",
      "training loss: -0.6481731534004211\n",
      "training loss: -0.6509935855865479\n",
      "training loss: -0.6683593988418579\n",
      "training loss: -0.6549975872039795\n",
      "training loss: -0.6670626401901245\n",
      "training loss: -0.6217251420021057\n",
      "training loss: -0.6624934673309326\n",
      "training loss: -0.6714555621147156\n",
      "training loss: -0.6313288807868958\n",
      "training loss: -0.6155997514724731\n",
      "training loss: -0.6559284925460815\n",
      "training loss: -0.6765068769454956\n",
      "training loss: -0.6693732738494873\n",
      "training loss: -0.6579166650772095\n",
      "training loss: -0.5172280669212341\n",
      "training loss: -0.39524245262145996\n",
      "training loss: -0.5033027529716492\n",
      "training loss: -0.4995060861110687\n",
      "training loss: -0.5204029679298401\n",
      "training loss: -0.5532045960426331\n",
      "training loss: -0.5717085599899292\n",
      "training loss: -0.5568000078201294\n",
      "training loss: -0.5841839909553528\n",
      "training loss: -0.5789721608161926\n",
      "training loss: -0.593531608581543\n",
      "training loss: -0.6098153591156006\n",
      "training loss: -0.6245718002319336\n",
      "training loss: -0.6239847540855408\n",
      "training loss: -0.5522743463516235\n",
      "training loss: -0.5606467127799988\n",
      "training loss: -0.5366579294204712\n",
      "training loss: -0.5212579369544983\n",
      "training loss: -0.5317907333374023\n",
      "training loss: -0.5475091338157654\n",
      "training loss: -0.5709113478660583\n",
      "training loss: -0.5748229622840881\n",
      "training loss: -0.6128115057945251\n",
      "training loss: -0.645805299282074\n",
      "training loss: -0.6545304656028748\n",
      "training loss: -0.6473453044891357\n",
      "training loss: -0.6029411554336548\n",
      "training loss: -0.6436846256256104\n",
      "training loss: -0.6716862916946411\n",
      "training loss: -0.6970371603965759\n",
      "training loss: -0.690714418888092\n",
      "training loss: -0.7039620876312256\n",
      "training loss: -0.6646629571914673\n",
      "training loss: -0.6551998853683472\n",
      "training loss: -0.645466148853302\n",
      "training loss: -0.6691148281097412\n",
      "training loss: -0.6891511678695679\n",
      "training loss: -0.7156426906585693\n",
      "training loss: -0.7306766510009766\n",
      "training loss: -0.6916689872741699\n",
      "training loss: -0.6505818367004395\n",
      "training loss: -0.67301344871521\n",
      "training loss: -0.6491823196411133\n",
      "training loss: -0.6135686635971069\n",
      "training loss: -0.6022539734840393\n",
      "training loss: -0.5854215025901794\n",
      "training loss: -0.5916772484779358\n",
      "training loss: -0.5725404620170593\n",
      "training loss: -0.5169105529785156\n",
      "training loss: -0.5507280826568604\n",
      "training loss: -0.6084801554679871\n",
      "training loss: -0.6353771686553955\n",
      "training loss: -0.6110138893127441\n",
      "training loss: -0.5775736570358276\n",
      "training loss: -0.6356443166732788\n",
      "training loss: -0.6459565758705139\n",
      "training loss: -0.6757385730743408\n",
      "training loss: -0.6677024960517883\n",
      "training loss: -0.6504763960838318\n",
      "training loss: -0.6732723712921143\n",
      "training loss: -0.6722952127456665\n",
      "training loss: -0.6290717124938965\n",
      "training loss: -0.675213098526001\n",
      "training loss: -0.6506558656692505\n",
      "training loss: -0.6788591146469116\n",
      "training loss: -0.6674870252609253\n",
      "training loss: -0.6727702617645264\n",
      "training loss: -0.696742832660675\n",
      "training loss: -0.6866511106491089\n",
      "training loss: -0.6466052532196045\n",
      "training loss: -0.6665228605270386\n",
      "training loss: -0.655665934085846\n",
      "training loss: -0.6427363753318787\n",
      "training loss: -0.5610225200653076\n",
      "training loss: -0.5712594389915466\n",
      "training loss: -0.6224538087844849\n",
      "training loss: -0.619246780872345\n",
      "training loss: -0.6370254755020142\n",
      "training loss: -0.6498590707778931\n",
      "training loss: -0.6514740586280823\n",
      "training loss: -0.6599047183990479\n",
      "training loss: -0.7029722332954407\n",
      "training loss: -0.6807147860527039\n",
      "training loss: -0.6814336180686951\n",
      "training loss: -0.7028672695159912\n",
      "training loss: -0.7316681742668152\n",
      "training loss: -0.7455247640609741\n",
      "training loss: -0.7205408215522766\n",
      "training loss: -0.723614513874054\n",
      "training loss: -0.6982060670852661\n",
      "training loss: -0.5975805521011353\n",
      "training loss: -0.5967177152633667\n",
      "training loss: -0.6014120578765869\n",
      "training loss: -0.6468156576156616\n",
      "training loss: -0.6491817831993103\n",
      "training loss: -0.6589147448539734\n",
      "training loss: -0.6605200171470642\n",
      "training loss: -0.6607894897460938\n",
      "training loss: -0.7156144380569458\n",
      "training loss: -0.7236294746398926\n",
      "training loss: -0.7553623914718628\n",
      "training loss: -0.7665367722511292\n",
      "training loss: -0.7399844527244568\n",
      "training loss: -0.7198753356933594\n",
      "training loss: -0.6325004696846008\n",
      "training loss: -0.6942853927612305\n",
      "training loss: -0.6480106115341187\n",
      "training loss: -0.5897189378738403\n",
      "training loss: -0.6158910393714905\n",
      "training loss: -0.6191281676292419\n",
      "training loss: -0.6241807341575623\n",
      "training loss: -0.6656203866004944\n",
      "training loss: -0.6865628957748413\n",
      "training loss: -0.6700340509414673\n",
      "training loss: -0.679585337638855\n",
      "training loss: -0.637614369392395\n",
      "training loss: -0.6275637149810791\n",
      "training loss: -0.5997614860534668\n",
      "training loss: -0.6274875402450562\n",
      "training loss: -0.6712795495986938\n",
      "training loss: -0.6711272597312927\n",
      "training loss: -0.679362952709198\n",
      "training loss: -0.6562961339950562\n",
      "training loss: -0.6103211641311646\n",
      "training loss: -0.5477747321128845\n",
      "training loss: -0.6100683808326721\n",
      "training loss: -0.6431086659431458\n",
      "training loss: -0.6396458148956299\n",
      "training loss: -0.6524545550346375\n",
      "training loss: -0.6885020136833191\n",
      "training loss: -0.6975215077400208\n",
      "training loss: -0.6908056139945984\n",
      "training loss: -0.6679197549819946\n",
      "training loss: -0.6410767436027527\n",
      "training loss: -0.6215059161186218\n",
      "training loss: -0.6015511751174927\n",
      "training loss: -0.6211339831352234\n",
      "training loss: -0.6178226470947266\n",
      "training loss: -0.6053014993667603\n",
      "training loss: -0.6342533826828003\n",
      "training loss: -0.6507627964019775\n",
      "training loss: -0.6734440326690674\n",
      "training loss: -0.6930656433105469\n",
      "training loss: -0.675186038017273\n",
      "training loss: -0.6634095907211304\n",
      "training loss: -0.6807260513305664\n",
      "training loss: -0.6756722927093506\n",
      "training loss: -0.6476780772209167\n",
      "training loss: -0.5808807611465454\n",
      "training loss: -0.5475607514381409\n",
      "training loss: -0.5067334771156311\n",
      "training loss: -0.5925607085227966\n",
      "training loss: -0.6048371195793152\n",
      "training loss: -0.6379256844520569\n",
      "training loss: -0.6687172651290894\n",
      "training loss: -0.6824074983596802\n",
      "training loss: -0.6858232021331787\n",
      "training loss: -0.6991734504699707\n",
      "training loss: -0.7076496481895447\n",
      "training loss: -0.729374885559082\n",
      "training loss: -0.7448796629905701\n",
      "training loss: -0.7519890069961548\n",
      "training loss: -0.7032843232154846\n",
      "training loss: -0.7183964252471924\n",
      "training loss: -0.7408447861671448\n",
      "Final learning rate: 5e-05\n",
      "training loss: -0.7426619529724121\n",
      "Final learning rate: 5e-05\n",
      "training loss: -0.6642547249794006\n",
      "Final learning rate: 5e-05\n",
      "The ending score for metric val_cum_r2 is: -1.1653e+05\n",
      "The ending score for metric val_cum_pearson is: 8.6213e-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Actually learning (?)\n",
    "HIDDEN_SIZE = 2048\n",
    "train_lookback = 16\n",
    "eval_lookback = 4\n",
    "feature = ['book', 'cashflow', 'sales', 'earnings_ttm', 'earnings', 'market_cap']\n",
    "# feature = ['cashflow', 'market_cap', 'market_share']\n",
    "# feature = ['avg_price', 'mean_volume', 'market_share', 'market_cap', 'return_0']\n",
    "# feature = ['avg_price']\n",
    "\n",
    "preprocess = CombinedScaler()\n",
    "lr = 5e-4 # use 0 if warmup scheduler\n",
    "n_epoch = 10\n",
    "\n",
    "\n",
    "constructor = lambda o: optim.lr_scheduler.CyclicLR(o, base_lr=lr / 10, max_lr=lr, step_size_up=n_epoch // 2,\n",
    "                                                    cycle_momentum=False)\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)\n",
    "\n",
    "model = NN_wrapper(preprocess=preprocess, lr=lr, lr_scheduler_constructor=constructor, n_epoch=n_epoch,\n",
    "                   train_lookback=train_lookback, per_eval_lookback=eval_lookback, hidden_size=HIDDEN_SIZE, n_asset=54,\n",
    "                   network='Transformer', feature_name=feature, load_model_path=None, criterion='pearson',\n",
    "                   embed_asset=True, embed_offset=True, embed_season=False, is_eval=False, var_weight=0,\n",
    "                   is_cuda=IS_CUDA)\n",
    "ds_cv = ds.sel(day=slice(200, ds.dims['day'] - 2))[feature + ['return']]\n",
    "performance_cv, cum_y_df = cross_validation(model, feature, ds=ds_cv, train_lookback=train_lookback,\n",
    "                                            per_eval_lookback=eval_lookback)\n",
    "\n",
    "plt.figure(0)\n",
    "plot_performance(performance_cv, metrics_selected=['val_cum_pearson', 'val_cum_r2'])\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b4818a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:24:44.400509Z",
     "iopub.status.busy": "2023-03-15T19:24:44.400166Z",
     "iopub.status.idle": "2023-03-15T19:24:45.387428Z",
     "shell.execute_reply": "2023-03-15T19:24:45.386223Z"
    },
    "papermill": {
     "duration": 1.034122,
     "end_time": "2023-03-15T19:24:45.389937",
     "exception": false,
     "start_time": "2023-03-15T19:24:44.355815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-15\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/model/dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005809a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:24:45.479631Z",
     "iopub.status.busy": "2023-03-15T19:24:45.479023Z",
     "iopub.status.idle": "2023-03-15T19:24:46.471132Z",
     "shell.execute_reply": "2023-03-15T19:24:46.469815Z"
    },
    "papermill": {
     "duration": 1.0394,
     "end_time": "2023-03-15T19:24:46.473704",
     "exception": false,
     "start_time": "2023-03-15T19:24:45.434304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/kaggle/working/model/dump/2023-03-10': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /kaggle/working/model/dump/2023-03-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83fc14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:24:46.581345Z",
     "iopub.status.busy": "2023-03-15T19:24:46.580970Z",
     "iopub.status.idle": "2023-03-15T19:24:47.590555Z",
     "shell.execute_reply": "2023-03-15T19:24:47.589354Z"
    },
    "papermill": {
     "duration": 1.067608,
     "end_time": "2023-03-15T19:24:47.592844",
     "exception": false,
     "start_time": "2023-03-15T19:24:46.525236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 15 19:24:47 UTC 2023\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f9d7236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T19:24:47.683748Z",
     "iopub.status.busy": "2023-03-15T19:24:47.682630Z",
     "iopub.status.idle": "2023-03-15T19:29:04.884672Z",
     "shell.execute_reply": "2023-03-15T19:29:04.883632Z"
    },
    "papermill": {
     "duration": 257.249678,
     "end_time": "2023-03-15T19:29:04.886832",
     "exception": false,
     "start_time": "2023-03-15T19:24:47.637154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding feature dimension is 68\n",
      "Environment is initialized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9482e6c2c443e098558c46b2e446e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: -0.22408004105091095\n",
      "training loss: -0.20688092708587646\n",
      "training loss: -0.21102730929851532\n",
      "training loss: -0.21210788190364838\n",
      "training loss: -0.19244790077209473\n",
      "training loss: -0.18687377870082855\n",
      "training loss: -0.1875665932893753\n",
      "training loss: -0.19684401154518127\n",
      "training loss: -0.21644748747348785\n",
      "training loss: -0.22576501965522766\n",
      "training loss: -0.2373756766319275\n",
      "training loss: -0.2432093471288681\n",
      "training loss: -0.24149298667907715\n",
      "training loss: -0.26965659856796265\n",
      "training loss: -0.31386256217956543\n",
      "training loss: -0.32540565729141235\n",
      "training loss: -0.35092541575431824\n",
      "training loss: -0.34193238615989685\n",
      "training loss: -0.3244868218898773\n",
      "training loss: -0.3430067002773285\n",
      "training loss: -0.3829655647277832\n",
      "training loss: -0.42139652371406555\n",
      "training loss: -0.4067866802215576\n",
      "training loss: -0.3996012508869171\n",
      "training loss: -0.4330771565437317\n",
      "training loss: -0.4437102973461151\n",
      "training loss: -0.4676865041255951\n",
      "training loss: -0.3535626232624054\n",
      "training loss: -0.3614828586578369\n",
      "training loss: -0.38379591703414917\n",
      "training loss: -0.4113931357860565\n",
      "training loss: -0.3995809555053711\n",
      "training loss: -0.4353402256965637\n",
      "training loss: -0.44572731852531433\n",
      "training loss: -0.40438520908355713\n",
      "training loss: -0.4065938889980316\n",
      "training loss: -0.409343957901001\n",
      "training loss: -0.4066457450389862\n",
      "training loss: -0.3835066258907318\n",
      "training loss: -0.39012497663497925\n",
      "training loss: -0.38906532526016235\n",
      "training loss: -0.40779635310173035\n",
      "training loss: -0.40117427706718445\n",
      "training loss: -0.4134202301502228\n",
      "training loss: -0.44607603549957275\n",
      "training loss: -0.47181302309036255\n",
      "training loss: -0.46943584084510803\n",
      "training loss: -0.48482534289360046\n",
      "training loss: -0.539455235004425\n",
      "training loss: -0.5511206984519958\n",
      "training loss: -0.5798751711845398\n",
      "training loss: -0.5970271229743958\n",
      "training loss: -0.491566002368927\n",
      "training loss: -0.4832787811756134\n",
      "training loss: -0.4586118161678314\n",
      "training loss: -0.414526104927063\n",
      "training loss: -0.37993744015693665\n",
      "training loss: -0.30740299820899963\n",
      "training loss: -0.282011479139328\n",
      "training loss: -0.3162502348423004\n",
      "training loss: -0.3088889718055725\n",
      "training loss: -0.3287902772426605\n",
      "training loss: -0.38389620184898376\n",
      "training loss: -0.42407509684562683\n",
      "training loss: -0.44193920493125916\n",
      "training loss: -0.47368475794792175\n",
      "training loss: -0.4978545606136322\n",
      "training loss: -0.5132327079772949\n",
      "training loss: -0.5287656784057617\n",
      "training loss: -0.5504933595657349\n",
      "training loss: -0.45898547768592834\n",
      "training loss: -0.533886194229126\n",
      "training loss: -0.5262595415115356\n",
      "training loss: -0.5068919062614441\n",
      "training loss: -0.511393129825592\n",
      "training loss: -0.5001188516616821\n",
      "training loss: -0.4641396701335907\n",
      "training loss: -0.44418200850486755\n",
      "training loss: -0.3845427334308624\n",
      "training loss: -0.42090752720832825\n",
      "training loss: -0.40629956126213074\n",
      "training loss: -0.3753330409526825\n",
      "training loss: -0.4012956917285919\n",
      "training loss: -0.41434887051582336\n",
      "training loss: -0.4055057764053345\n",
      "training loss: -0.4024416506290436\n",
      "training loss: -0.4240412414073944\n",
      "training loss: -0.41124022006988525\n",
      "training loss: -0.46590155363082886\n",
      "training loss: -0.4261772334575653\n",
      "training loss: -0.47736161947250366\n",
      "training loss: -0.41594743728637695\n",
      "training loss: -0.4811640679836273\n",
      "training loss: -0.5109254717826843\n",
      "training loss: -0.450920045375824\n",
      "training loss: -0.431481271982193\n",
      "training loss: -0.43075546622276306\n",
      "training loss: -0.42599090933799744\n",
      "training loss: -0.4021286964416504\n",
      "training loss: -0.392975777387619\n",
      "training loss: -0.42616814374923706\n",
      "training loss: -0.48202258348464966\n",
      "training loss: -0.5113888382911682\n",
      "training loss: -0.5294546484947205\n",
      "training loss: -0.5591593384742737\n",
      "training loss: -0.5693284869194031\n",
      "training loss: -0.5583382844924927\n",
      "training loss: -0.5040769577026367\n",
      "training loss: -0.5210384726524353\n",
      "training loss: -0.5383679866790771\n",
      "training loss: -0.527693510055542\n",
      "training loss: -0.46917974948883057\n",
      "training loss: -0.4243960380554199\n",
      "training loss: -0.42276763916015625\n",
      "training loss: -0.4604213535785675\n",
      "training loss: -0.45541247725486755\n",
      "training loss: -0.44057682156562805\n",
      "training loss: -0.4307764172554016\n",
      "training loss: -0.4704226553440094\n",
      "training loss: -0.3936845064163208\n",
      "training loss: -0.43288588523864746\n",
      "training loss: -0.45020273327827454\n",
      "training loss: -0.44188088178634644\n",
      "training loss: -0.4366152882575989\n",
      "training loss: -0.426974892616272\n",
      "training loss: -0.45162275433540344\n",
      "training loss: -0.4600469470024109\n",
      "training loss: -0.4649597108364105\n",
      "training loss: -0.45530831813812256\n",
      "training loss: -0.4634969234466553\n",
      "training loss: -0.4504414200782776\n",
      "training loss: -0.44313740730285645\n",
      "training loss: -0.4341355264186859\n",
      "training loss: -0.45162519812583923\n",
      "training loss: -0.46890130639076233\n",
      "training loss: -0.47090843319892883\n",
      "training loss: -0.47190365195274353\n",
      "training loss: -0.44382286071777344\n",
      "training loss: -0.4138866662979126\n",
      "training loss: -0.43926969170570374\n",
      "training loss: -0.45569396018981934\n",
      "training loss: -0.447358101606369\n",
      "training loss: -0.39977332949638367\n",
      "training loss: -0.39629167318344116\n",
      "training loss: -0.4270084500312805\n",
      "training loss: -0.41288647055625916\n",
      "training loss: -0.4127323031425476\n",
      "training loss: -0.38133060932159424\n",
      "training loss: -0.37366923689842224\n",
      "training loss: -0.39799201488494873\n",
      "training loss: -0.4278554916381836\n",
      "training loss: -0.4589976370334625\n",
      "training loss: -0.4532243609428406\n",
      "training loss: -0.4514364004135132\n",
      "training loss: -0.4210328161716461\n",
      "training loss: -0.4167174696922302\n",
      "training loss: -0.41372406482696533\n",
      "training loss: -0.41971299052238464\n",
      "training loss: -0.3532200753688812\n",
      "training loss: -0.35469844937324524\n",
      "training loss: -0.34481585025787354\n",
      "training loss: -0.32579290866851807\n",
      "training loss: -0.34234869480133057\n",
      "training loss: -0.3535793423652649\n",
      "training loss: -0.3785405158996582\n",
      "training loss: -0.39862996339797974\n",
      "training loss: -0.40341055393218994\n",
      "training loss: -0.4011707603931427\n",
      "training loss: -0.41495004296302795\n",
      "training loss: -0.4530722200870514\n",
      "training loss: -0.4813576936721802\n",
      "training loss: -0.5089554786682129\n",
      "training loss: -0.5297884345054626\n",
      "training loss: -0.530544638633728\n",
      "training loss: -0.5279491543769836\n",
      "training loss: -0.5181750059127808\n",
      "training loss: -0.508668065071106\n",
      "training loss: -0.39560994505882263\n",
      "training loss: -0.3892124593257904\n",
      "training loss: -0.39638441801071167\n",
      "training loss: -0.3840937316417694\n",
      "training loss: -0.41162043809890747\n",
      "training loss: -0.4224148988723755\n",
      "training loss: -0.4401897192001343\n",
      "training loss: -0.46251049637794495\n",
      "training loss: -0.4728943407535553\n",
      "training loss: -0.47336968779563904\n",
      "training loss: -0.458552747964859\n",
      "training loss: -0.46679267287254333\n",
      "training loss: -0.5150856375694275\n",
      "training loss: -0.5184776186943054\n",
      "training loss: -0.512894868850708\n",
      "training loss: -0.5144780278205872\n",
      "training loss: -0.4856520891189575\n",
      "training loss: -0.42670372128486633\n",
      "training loss: -0.41256582736968994\n",
      "training loss: -0.4168875515460968\n",
      "training loss: -0.424656480550766\n",
      "training loss: -0.4433858096599579\n",
      "training loss: -0.4931524693965912\n",
      "training loss: -0.434901624917984\n",
      "training loss: -0.48669829964637756\n",
      "training loss: -0.4561278522014618\n",
      "training loss: -0.47646966576576233\n",
      "training loss: -0.4558514654636383\n",
      "training loss: -0.44328773021698\n",
      "training loss: -0.4483669400215149\n",
      "training loss: -0.423870325088501\n",
      "training loss: -0.43941712379455566\n",
      "training loss: -0.4188153147697449\n",
      "training loss: -0.4006710350513458\n",
      "training loss: -0.38877397775650024\n",
      "training loss: -0.40868479013442993\n",
      "training loss: -0.41394850611686707\n",
      "training loss: -0.4118393063545227\n",
      "training loss: -0.4310591220855713\n",
      "training loss: -0.43994686007499695\n",
      "training loss: -0.43845462799072266\n",
      "training loss: -0.47535330057144165\n",
      "training loss: -0.4857858717441559\n",
      "training loss: -0.4547182321548462\n",
      "training loss: -0.40898555517196655\n",
      "training loss: -0.3729044198989868\n",
      "training loss: -0.35482126474380493\n",
      "training loss: -0.3532150387763977\n",
      "training loss: -0.3436218202114105\n",
      "training loss: -0.367552787065506\n",
      "training loss: -0.36442121863365173\n",
      "training loss: -0.3586311638355255\n",
      "training loss: -0.36900267004966736\n",
      "training loss: -0.3951835632324219\n",
      "training loss: -0.44763487577438354\n",
      "training loss: -0.4477067291736603\n",
      "training loss: -0.45976880192756653\n",
      "training loss: -0.4834129214286804\n",
      "training loss: -0.49523648619651794\n",
      "training loss: -0.5086367726325989\n",
      "training loss: -0.5013940930366516\n",
      "training loss: -0.3764593303203583\n",
      "training loss: -0.303484171628952\n",
      "training loss: -0.3134326636791229\n",
      "training loss: -0.32311615347862244\n",
      "training loss: -0.34792160987854004\n",
      "training loss: -0.3516775071620941\n",
      "training loss: -0.37369129061698914\n",
      "training loss: -0.4118214249610901\n",
      "training loss: -0.4327237904071808\n",
      "training loss: -0.4599626064300537\n",
      "training loss: -0.4889846444129944\n",
      "training loss: -0.511857807636261\n",
      "training loss: -0.5404257774353027\n",
      "training loss: -0.545775294303894\n",
      "training loss: -0.5303084850311279\n",
      "training loss: -0.48083731532096863\n",
      "training loss: -0.40511220693588257\n",
      "training loss: -0.4171217083930969\n",
      "training loss: -0.45498716831207275\n",
      "training loss: -0.46331387758255005\n",
      "training loss: -0.4674021601676941\n",
      "training loss: -0.4646824300289154\n",
      "training loss: -0.45721378922462463\n",
      "training loss: -0.4487361013889313\n",
      "training loss: -0.4445876479148865\n",
      "training loss: -0.4577098488807678\n",
      "training loss: -0.45636916160583496\n",
      "training loss: -0.4701951742172241\n",
      "training loss: -0.48619264364242554\n",
      "training loss: -0.4747166931629181\n",
      "training loss: -0.4168502986431122\n",
      "training loss: -0.3540225028991699\n",
      "training loss: -0.36189529299736023\n",
      "training loss: -0.34847819805145264\n",
      "training loss: -0.33722972869873047\n",
      "training loss: -0.28515300154685974\n",
      "training loss: -0.2667240500450134\n",
      "training loss: -0.347709983587265\n",
      "training loss: -0.396799772977829\n",
      "training loss: -0.42818590998649597\n",
      "training loss: -0.45737168192863464\n",
      "training loss: -0.43992379307746887\n",
      "training loss: -0.47329097986221313\n",
      "training loss: -0.48631176352500916\n",
      "training loss: -0.46556568145751953\n",
      "training loss: -0.4331114888191223\n",
      "training loss: -0.38310936093330383\n",
      "training loss: -0.35847464203834534\n",
      "training loss: -0.36044996976852417\n",
      "training loss: -0.3848946988582611\n",
      "training loss: -0.36766090989112854\n",
      "training loss: -0.365407258272171\n",
      "training loss: -0.3669588267803192\n",
      "training loss: -0.3546883165836334\n",
      "training loss: -0.3610651195049286\n",
      "training loss: -0.38684338331222534\n",
      "training loss: -0.4161914885044098\n",
      "training loss: -0.41053399443626404\n",
      "training loss: -0.4163265824317932\n",
      "training loss: -0.40285927057266235\n",
      "training loss: -0.4360319674015045\n",
      "training loss: -0.4388442635536194\n",
      "training loss: -0.401782363653183\n",
      "training loss: -0.3803030252456665\n",
      "training loss: -0.39679187536239624\n",
      "training loss: -0.4145542085170746\n",
      "training loss: -0.4141768217086792\n",
      "training loss: -0.44969257712364197\n",
      "training loss: -0.45402124524116516\n",
      "training loss: -0.4967041015625\n",
      "training loss: -0.4683644771575928\n",
      "training loss: -0.532368540763855\n",
      "training loss: -0.5154186487197876\n",
      "training loss: -0.4712444245815277\n",
      "training loss: -0.45537295937538147\n",
      "training loss: -0.47868332266807556\n",
      "training loss: -0.5064567923545837\n",
      "training loss: -0.511768639087677\n",
      "training loss: -0.5188628435134888\n",
      "training loss: -0.5059985518455505\n",
      "training loss: -0.4219204783439636\n",
      "training loss: -0.45648708939552307\n",
      "training loss: -0.47106853127479553\n",
      "training loss: -0.48268887400627136\n",
      "training loss: -0.4782376289367676\n",
      "training loss: -0.4683120846748352\n",
      "training loss: -0.48444098234176636\n",
      "training loss: -0.42765259742736816\n",
      "training loss: -0.4252799153327942\n",
      "training loss: -0.393515408039093\n",
      "training loss: -0.4145984351634979\n",
      "training loss: -0.43143099546432495\n",
      "training loss: -0.4444679617881775\n",
      "training loss: -0.4266735315322876\n",
      "training loss: -0.4151824414730072\n",
      "training loss: -0.4204857051372528\n",
      "training loss: -0.4231598973274231\n",
      "training loss: -0.3770621716976166\n",
      "training loss: -0.39477449655532837\n",
      "training loss: -0.4577069878578186\n",
      "training loss: -0.4531225562095642\n",
      "training loss: -0.43363961577415466\n",
      "training loss: -0.41540583968162537\n",
      "training loss: -0.3961074948310852\n",
      "training loss: -0.35548028349876404\n",
      "training loss: -0.3635719418525696\n",
      "training loss: -0.348427414894104\n",
      "training loss: -0.31008198857307434\n",
      "training loss: -0.3357659876346588\n",
      "training loss: -0.3622806668281555\n",
      "training loss: -0.3881211280822754\n",
      "training loss: -0.4255179762840271\n",
      "training loss: -0.4387187659740448\n",
      "training loss: -0.4616209864616394\n",
      "training loss: -0.4796678423881531\n",
      "training loss: -0.4890299141407013\n",
      "training loss: -0.4964679479598999\n",
      "training loss: -0.5323985815048218\n",
      "training loss: -0.49261611700057983\n",
      "training loss: -0.5219483971595764\n",
      "training loss: -0.4888896942138672\n",
      "training loss: -0.4744425117969513\n",
      "training loss: -0.48240241408348083\n",
      "training loss: -0.4884142577648163\n",
      "training loss: -0.5005117058753967\n",
      "training loss: -0.5092009902000427\n",
      "training loss: -0.5212032198905945\n",
      "training loss: -0.4755721390247345\n",
      "training loss: -0.46123096346855164\n",
      "training loss: -0.4676181972026825\n",
      "training loss: -0.47762855887413025\n",
      "training loss: -0.47049960494041443\n",
      "training loss: -0.48865175247192383\n",
      "training loss: -0.5043786764144897\n",
      "training loss: -0.5251924991607666\n",
      "training loss: -0.539287805557251\n",
      "training loss: -0.5726658701896667\n",
      "training loss: -0.5888883471488953\n",
      "training loss: -0.5218559503555298\n",
      "training loss: -0.5049209594726562\n",
      "training loss: -0.4908684492111206\n",
      "training loss: -0.4610978364944458\n",
      "training loss: -0.45368677377700806\n",
      "training loss: -0.4655087888240814\n",
      "training loss: -0.48135054111480713\n",
      "training loss: -0.4859580993652344\n",
      "training loss: -0.45544812083244324\n",
      "training loss: -0.45974159240722656\n",
      "training loss: -0.49038681387901306\n",
      "training loss: -0.4950956106185913\n",
      "training loss: -0.48411813378334045\n",
      "training loss: -0.4539389908313751\n",
      "training loss: -0.41940838098526\n",
      "training loss: -0.4438610076904297\n",
      "training loss: -0.4526582360267639\n",
      "training loss: -0.4422808289527893\n",
      "training loss: -0.4607573449611664\n",
      "training loss: -0.45729610323905945\n",
      "training loss: -0.4936787188053131\n",
      "training loss: -0.541118323802948\n",
      "training loss: -0.5644405484199524\n",
      "training loss: -0.5443732142448425\n",
      "training loss: -0.5529958009719849\n",
      "training loss: -0.5511816143989563\n",
      "training loss: -0.5380700826644897\n",
      "training loss: -0.5030750632286072\n",
      "training loss: -0.472949743270874\n",
      "training loss: -0.36433228850364685\n",
      "training loss: -0.36939293146133423\n",
      "training loss: -0.38346806168556213\n",
      "training loss: -0.36226871609687805\n",
      "training loss: -0.3664294183254242\n",
      "training loss: -0.3727165162563324\n",
      "training loss: -0.40752094984054565\n",
      "training loss: -0.42498600482940674\n",
      "training loss: -0.4256700575351715\n",
      "training loss: -0.42879238724708557\n",
      "training loss: -0.4416924715042114\n",
      "training loss: -0.4561443030834198\n",
      "training loss: -0.46352627873420715\n",
      "training loss: -0.4783541262149811\n",
      "training loss: -0.4542367458343506\n",
      "training loss: -0.4421495795249939\n",
      "training loss: -0.4273087680339813\n",
      "training loss: -0.42000752687454224\n",
      "training loss: -0.41799527406692505\n",
      "training loss: -0.39805105328559875\n",
      "training loss: -0.3874659240245819\n",
      "training loss: -0.399877667427063\n",
      "training loss: -0.4255083501338959\n",
      "training loss: -0.42286360263824463\n",
      "training loss: -0.41399484872817993\n",
      "training loss: -0.42736056447029114\n",
      "training loss: -0.46290093660354614\n",
      "training loss: -0.5041896104812622\n",
      "training loss: -0.5606783032417297\n",
      "training loss: -0.5727198719978333\n",
      "training loss: -0.5522987246513367\n",
      "training loss: -0.5306541919708252\n",
      "training loss: -0.5188649296760559\n",
      "training loss: -0.47598105669021606\n",
      "training loss: -0.4835171699523926\n",
      "training loss: -0.49322864413261414\n",
      "training loss: -0.49215415120124817\n",
      "training loss: -0.4745217263698578\n",
      "training loss: -0.48035767674446106\n",
      "training loss: -0.45385271310806274\n",
      "training loss: -0.4564461410045624\n",
      "training loss: -0.4473922550678253\n",
      "training loss: -0.44566377997398376\n",
      "training loss: -0.48428189754486084\n",
      "training loss: -0.473317950963974\n",
      "training loss: -0.44010546803474426\n",
      "training loss: -0.41132500767707825\n",
      "training loss: -0.42609989643096924\n",
      "training loss: -0.41482898592948914\n",
      "training loss: -0.42199796438217163\n",
      "training loss: -0.4188022017478943\n",
      "training loss: -0.40425926446914673\n",
      "training loss: -0.39632076025009155\n",
      "training loss: -0.4091116786003113\n",
      "training loss: -0.4611116051673889\n",
      "training loss: -0.5032804012298584\n",
      "training loss: -0.521091639995575\n",
      "training loss: -0.5047516822814941\n",
      "training loss: -0.4939191937446594\n",
      "training loss: -0.4724388122558594\n",
      "training loss: -0.4552917182445526\n",
      "training loss: -0.45411810278892517\n",
      "training loss: -0.4683477282524109\n",
      "training loss: -0.4890350103378296\n",
      "training loss: -0.504693865776062\n",
      "training loss: -0.49050867557525635\n",
      "training loss: -0.5008500814437866\n",
      "training loss: -0.5009766817092896\n",
      "training loss: -0.5039132237434387\n",
      "training loss: -0.5217753648757935\n",
      "training loss: -0.5198287963867188\n",
      "training loss: -0.5159613490104675\n",
      "training loss: -0.5239500403404236\n",
      "training loss: -0.5366947650909424\n",
      "training loss: -0.5305266976356506\n",
      "training loss: -0.49706190824508667\n",
      "training loss: -0.49294513463974\n",
      "training loss: -0.491560697555542\n",
      "training loss: -0.46941351890563965\n",
      "training loss: -0.44428113102912903\n",
      "training loss: -0.461166650056839\n",
      "training loss: -0.4614185690879822\n",
      "training loss: -0.44280651211738586\n",
      "training loss: -0.420437216758728\n",
      "training loss: -0.42407411336898804\n",
      "training loss: -0.45086801052093506\n",
      "training loss: -0.4655344486236572\n",
      "training loss: -0.48109152913093567\n",
      "training loss: -0.48215505480766296\n",
      "training loss: -0.4724607765674591\n",
      "training loss: -0.44144630432128906\n",
      "training loss: -0.44793784618377686\n",
      "training loss: -0.4444575607776642\n",
      "training loss: -0.4215998947620392\n",
      "training loss: -0.4230232238769531\n",
      "training loss: -0.42786818742752075\n",
      "training loss: -0.4508326053619385\n",
      "training loss: -0.44221949577331543\n",
      "training loss: -0.45345738530158997\n",
      "training loss: -0.4741702973842621\n",
      "training loss: -0.5229097008705139\n",
      "training loss: -0.5537662506103516\n",
      "training loss: -0.5691238045692444\n",
      "training loss: -0.5870984196662903\n",
      "training loss: -0.5799770951271057\n",
      "training loss: -0.5557845830917358\n",
      "training loss: -0.5325143933296204\n",
      "training loss: -0.5319061875343323\n",
      "training loss: -0.5413773059844971\n",
      "training loss: -0.5474960207939148\n",
      "training loss: -0.5599257946014404\n",
      "training loss: -0.5611726641654968\n",
      "training loss: -0.5467925667762756\n",
      "training loss: -0.5183383822441101\n",
      "training loss: -0.476127564907074\n",
      "training loss: -0.475996732711792\n",
      "training loss: -0.4463683068752289\n",
      "training loss: -0.4338546693325043\n",
      "training loss: -0.4659664034843445\n",
      "training loss: -0.43296101689338684\n",
      "training loss: -0.40258386731147766\n",
      "training loss: -0.4291509985923767\n",
      "training loss: -0.4306100904941559\n",
      "training loss: -0.4228558838367462\n",
      "training loss: -0.43319615721702576\n",
      "training loss: -0.4131242334842682\n",
      "training loss: -0.39884597063064575\n",
      "training loss: -0.41161173582077026\n",
      "training loss: -0.4508192241191864\n",
      "training loss: -0.4289742112159729\n",
      "training loss: -0.4339570105075836\n",
      "training loss: -0.4322137236595154\n",
      "training loss: -0.4043666124343872\n",
      "training loss: -0.3996128439903259\n",
      "training loss: -0.40636545419692993\n",
      "training loss: -0.39729630947113037\n",
      "training loss: -0.40176767110824585\n",
      "training loss: -0.4049351215362549\n",
      "training loss: -0.40631356835365295\n",
      "training loss: -0.4021150767803192\n",
      "training loss: -0.4163118302822113\n",
      "training loss: -0.41343069076538086\n",
      "training loss: -0.43328210711479187\n",
      "training loss: -0.4398130476474762\n",
      "training loss: -0.4400225579738617\n",
      "training loss: -0.4593956470489502\n",
      "training loss: -0.44420892000198364\n",
      "training loss: -0.4644997715950012\n",
      "training loss: -0.41429564356803894\n",
      "training loss: -0.43164557218551636\n",
      "training loss: -0.44739627838134766\n",
      "training loss: -0.4541328549385071\n",
      "training loss: -0.4335763156414032\n",
      "training loss: -0.4393993020057678\n",
      "training loss: -0.40801066160202026\n",
      "training loss: -0.4231266975402832\n",
      "training loss: -0.43923184275627136\n",
      "training loss: -0.4590473771095276\n",
      "training loss: -0.42648983001708984\n",
      "training loss: -0.4128798246383667\n",
      "training loss: -0.46344998478889465\n",
      "training loss: -0.4480527341365814\n",
      "training loss: -0.4206385910511017\n",
      "training loss: -0.4423885941505432\n",
      "training loss: -0.4327965974807739\n",
      "training loss: -0.41300973296165466\n",
      "training loss: -0.3897116780281067\n",
      "training loss: -0.3778202533721924\n",
      "training loss: -0.3815900385379791\n",
      "training loss: -0.4059661030769348\n",
      "training loss: -0.43408939242362976\n",
      "training loss: -0.45407748222351074\n",
      "training loss: -0.4415677785873413\n",
      "training loss: -0.4350806474685669\n",
      "training loss: -0.449213445186615\n",
      "training loss: -0.442859023809433\n",
      "training loss: -0.4381364583969116\n",
      "training loss: -0.4452860653400421\n",
      "training loss: -0.4318784773349762\n",
      "training loss: -0.4094424545764923\n",
      "training loss: -0.42562881112098694\n",
      "training loss: -0.45260629057884216\n",
      "training loss: -0.45999443531036377\n",
      "training loss: -0.46438127756118774\n",
      "training loss: -0.4839555025100708\n",
      "training loss: -0.46643897891044617\n",
      "training loss: -0.45150548219680786\n",
      "training loss: -0.4494897127151489\n",
      "training loss: -0.44282108545303345\n",
      "training loss: -0.44248905777931213\n",
      "training loss: -0.4688561260700226\n",
      "training loss: -0.47569170594215393\n",
      "training loss: -0.45953601598739624\n",
      "training loss: -0.4669821560382843\n",
      "training loss: -0.4933284819126129\n",
      "training loss: -0.4875497817993164\n",
      "training loss: -0.4603419601917267\n",
      "training loss: -0.45814383029937744\n",
      "training loss: -0.4728991985321045\n",
      "training loss: -0.45090678334236145\n",
      "training loss: -0.4400629699230194\n",
      "training loss: -0.43508103489875793\n",
      "training loss: -0.44096311926841736\n",
      "training loss: -0.47949501872062683\n",
      "training loss: -0.4931704103946686\n",
      "training loss: -0.48175203800201416\n",
      "training loss: -0.47665202617645264\n",
      "training loss: -0.48077625036239624\n",
      "training loss: -0.4764370918273926\n",
      "training loss: -0.48758193850517273\n",
      "training loss: -0.4669632017612457\n",
      "training loss: -0.4523000717163086\n",
      "training loss: -0.45811235904693604\n",
      "training loss: -0.4897526502609253\n",
      "training loss: -0.4992901086807251\n",
      "training loss: -0.49163323640823364\n",
      "training loss: -0.49690428376197815\n",
      "training loss: -0.4936199486255646\n",
      "training loss: -0.4981108009815216\n",
      "training loss: -0.4923143982887268\n",
      "training loss: -0.5025752186775208\n",
      "training loss: -0.5002965927124023\n",
      "training loss: -0.49431291222572327\n",
      "training loss: -0.4802175462245941\n",
      "training loss: -0.46309784054756165\n",
      "training loss: -0.45118340849876404\n",
      "training loss: -0.41283056139945984\n",
      "training loss: -0.4029260277748108\n",
      "training loss: -0.41809460520744324\n",
      "training loss: -0.45449739694595337\n",
      "training loss: -0.4846820831298828\n",
      "training loss: -0.5110675096511841\n",
      "training loss: -0.5319307446479797\n",
      "training loss: -0.51332688331604\n",
      "training loss: -0.5078067183494568\n",
      "training loss: -0.48154258728027344\n",
      "training loss: -0.4703298807144165\n",
      "training loss: -0.4451114237308502\n",
      "training loss: -0.4330510199069977\n",
      "training loss: -0.4418412446975708\n",
      "training loss: -0.4118998050689697\n",
      "training loss: -0.4130028486251831\n",
      "training loss: -0.44761860370635986\n",
      "training loss: -0.40915942192077637\n",
      "training loss: -0.3931776285171509\n",
      "training loss: -0.4342975616455078\n",
      "training loss: -0.46462345123291016\n",
      "training loss: -0.46894127130508423\n",
      "training loss: -0.4731824994087219\n",
      "training loss: -0.48360705375671387\n",
      "training loss: -0.48447757959365845\n",
      "training loss: -0.511381983757019\n",
      "training loss: -0.5223800539970398\n",
      "training loss: -0.5208653807640076\n",
      "training loss: -0.4639953374862671\n",
      "training loss: -0.4495806097984314\n",
      "training loss: -0.4650290906429291\n",
      "training loss: -0.4562494158744812\n",
      "training loss: -0.43020352721214294\n",
      "training loss: -0.4113377630710602\n",
      "training loss: -0.3977024257183075\n",
      "training loss: -0.39600974321365356\n",
      "training loss: -0.3943291902542114\n",
      "training loss: -0.39708301424980164\n",
      "training loss: -0.4149896204471588\n",
      "training loss: -0.41885387897491455\n",
      "training loss: -0.43296363949775696\n",
      "training loss: -0.471086710691452\n",
      "training loss: -0.4974920153617859\n",
      "training loss: -0.5161983966827393\n",
      "training loss: -0.5262042880058289\n",
      "training loss: -0.5130096673965454\n",
      "training loss: -0.5012674331665039\n",
      "training loss: -0.4560464024543762\n",
      "training loss: -0.4059511125087738\n",
      "training loss: -0.3662852644920349\n",
      "training loss: -0.38117045164108276\n",
      "training loss: -0.4018632769584656\n",
      "training loss: -0.405277281999588\n",
      "training loss: -0.4660530686378479\n",
      "training loss: -0.48728716373443604\n",
      "training loss: -0.5144143104553223\n",
      "training loss: -0.5205124020576477\n",
      "training loss: -0.5032554864883423\n",
      "training loss: -0.47542083263397217\n",
      "training loss: -0.484004944562912\n",
      "training loss: -0.4704693853855133\n",
      "training loss: -0.4382616877555847\n",
      "training loss: -0.44814035296440125\n",
      "training loss: -0.43953636288642883\n",
      "training loss: -0.4441089928150177\n",
      "training loss: -0.4528824985027313\n",
      "training loss: -0.4520236551761627\n",
      "training loss: -0.4545527994632721\n",
      "training loss: -0.48930811882019043\n",
      "Data Feeding is finished.\n",
      "The ending score for metric test_cum_r2 is: -2.8243e+05\n",
      "The ending score for metric test_cum_pearson is: 5.6544e-02\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAG2CAYAAABvWcJYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABie0lEQVR4nO3deVxV1f7/8ddhdoJUBBxQUNM0hxRK0WvaIKiVWt00Lcoyv9cmp2wwNb32S23QrCzNsrLbLb2llqUpVmqm5AjmlFpqmEKKAzgFCPv3x5YDBw6IcA5wju/n43EenLP22pu1tsj+8Flrr20xDMNAREREROzyqOgGiIiIiFRmCpZEREREiqFgSURERKQYCpZEREREiqFgSURERKQYCpZEREREiqFgSURERKQYCpZEREREiqFgSURERKQYCpZEREREiuFywdI777xDeHg4fn5+REREsHbt2iLrJicnM3DgQJo3b46HhwcjRoywW2/hwoW0bNkSX19fWrZsyeLFi53UehEREXE1LhUsLViwgBEjRjB27FgSEhLo0qULPXv2JCkpyW79jIwM6tSpw9ixY2nbtq3dOvHx8fTv35/Y2Fi2bdtGbGws/fr1Y8OGDc7sioiIiLgIiys9SLdDhw60b9+eWbNmWctatGhB3759mTJlSrH7duvWjeuuu44ZM2bYlPfv35/09HS+/fZba1mPHj2oWbMmn332mUPbLyIiIq7Hq6IbUFKZmZls2bKF5557zqY8Ojqa9evXl/q48fHxjBw50qYsJiamUFCVX0ZGBhkZGdbPOTk5nDhxgtq1a2OxWErdFhERESk/hmFw+vRp6tWrh4dH0YNtLhMspaamkp2dTXBwsE15cHAwKSkppT5uSkrKZR9zypQp/Pvf/y719xQREZHK49ChQzRo0KDI7S4TLOUqmLkxDKPM2ZzLPeaYMWMYNWqU9XNaWhoNGzbk0KFD+Pv7l6ktlcZ/7oY/N8Cd78I1t1V0a8QV/DgN1r0OLXpD33cqujUiIpeUnp5OaGgoNWrUKLaeywRLgYGBeHp6Fsr4HD16tFBm6HKEhIRc9jF9fX3x9fUtVO7v7+8+wZL3BfC1wFW1wF36JM7VKho2z4C/foYaNUBD0iLiIi6VdHGZu+F8fHyIiIhg5cqVNuUrV66kU6dOpT5uVFRUoWPGxcWV6Zhu4cLFOVlehYNCEbsaRIJXFTh7DP7aWdGtERFxGJfJLAGMGjWK2NhYIiMjiYqKYs6cOSQlJTF06FDAHB47fPgwH3/8sXWfxMREAM6cOcOxY8dITEzEx8eHli1bAjB8+HBuvPFGXn75Zfr06cNXX33Fd999x08//VTu/atULvxtfvXyq9h2iOvw8oUmN8GeZbBzMYS0qugWiYg4hEsFS/379+f48eNMmjSJ5ORkWrVqxbJly2jUqBFgLkJZcM2ldu3aWd9v2bKFTz/9lEaNGnHw4EEAOnXqxPz58xk3bhzjx4+nSZMmLFiwgA4dOpRbv8rV0qfgj/XwyPfgU7XoesosSWm0utsMlnZ8ATeP01CciLgFl1pnqbJKT08nICCAtLS0yj1nKfsCvFjbfD/wf9AspnAdwzAvcK80gXOp8NjPENSifNsprivzLLzaFLLOweDvIPT6im6ROFB2djZZWVkV3QyREvP29sbT07PI7SW9frtUZknK6OSBvPfedrJKa16BzR/C4DhllqR0fKpB856wYyF8+SgM/Qm8NZTr6gzDICUlhVOnTlV0U0Qu21VXXUVISEiZ7pxXsHQlObor7/2FjMLbV71kfo0bpzlLUno3j4Nfl8LxfXBoAzTuWtEtkjLKDZSCgoKoWrWqFt8Vl2AYBufOnePo0aMA1K1bt9THUrB0JTn6a977C+eLrnf8d8i5mGpXsCSXq1Zjc4h311fwxzrIOg/V60DtpuAXUNGtk8uUnZ1tDZRq165d0c0RuSxVqlQBzCWBgoKCih2SK46CpSvJ+ZN577P+Lrre8X157z19nNcecV/12pvB0pqX8xVazAVO+8yEKjUrrGlyeXLnKFWtWswNISKVWO7PblZWVqmDJZdZZ0kc4EK+AKm4zFL+esosSWmE3mD72TcAMODXb+CLhyHjdIU0S0pPQ2/iqhzxs6tg6UqSPwgqLrOUy+IJnko+SimEdsx7H9YFxiTBQ9+amcrff4Blz1Rc20RELpOCpSvJJTNLBaJvZZWktDw84L6FZtB023SzrFEn6Hdxwdjtn8Pp0j8AW0SkPClYupLkvwMuy06w5FFgLFfLBkhZXH0rDF4BdZrllTXvCfUjzRsIfl1acW2TK0K3bt0YMWKEw443aNAg+vbt67DjVSaLFi2ie/fu1KlTB39/f6KiolixYkVFN6vSULB0JbEZhrMXLBUYcrO3FpNIWeUuhrp/lf3txa2Tm/W3eaPCX7vg9VawcIj9n2URKRHDMLhw4QI//vgj3bt3Z9myZWzZsoWbbrqJO+64g4SEhIpuYqWgYOlKkj+zdMHOnKWCF6lqgc5tj1yZmtxsft23ErbMg7TDedtS98GszvDeLeaK8/n9uhRmRsKrV8OsKEg7BNv/ByvGwtlUWPmC+SgfEcws0Jo1a3jjjTewWCxYLBYOHjzIrl276NWrF9WrVyc4OJjY2FhSU1Ot+33xxRe0bt2aKlWqULt2bW699VbOnj3LxIkTmTdvHl999ZX1eKtXr75kO/7880/uvfdeatWqRbVq1YiMjGTDhg3WNhbMVI0YMYJu3bpZP3fr1o0nn3ySESNGULNmTYKDg5kzZw5nz57loYceokaNGjRp0oRvv/22ROdl9erVWCwWVqxYQWRkJL6+vqxdu5YZM2bwzDPPcP3113P11VczefJkrr76ar7++usSHdfdafbulaS4zJJhQHambVm1Os5vk1x56kdA45vMzNLXw8yywObQ9Rlz1e/cn8MDq6Hpreb7+LdhxfP2j7d5rjkHKiMd1r0B4TdCyz7Q6p/muk5GTuEhZikTwzA4n5VdId+7irdnie5ueuONN9i7dy+tWrVi0qRJgLlmVNeuXRkyZAjTp0/n/PnzPPvss/Tr148ffviB5ORkBgwYwCuvvMKdd97J6dOnWbt2LYZhMHr0aHbv3k16ejoffvghALVq1Sq2DWfOnKFr167Ur1+fJUuWEBISwtatW8nJybmsPs+bN49nnnmGjRs3smDBAh599FG+/PJL7rzzTp5//nlef/11YmNjSUpKKvESD8888wyvvfYajRs35qqrriq0PScnh9OnT1+yj1cKBUtXkvx3wBXMLOVkAwUyS9WDnN4kuQJZLPDPD8yV4vevhvTDkLoHFg62rbdlnhks7V2RFyhdd58ZaCUnmsN5u7+Bje+agVKuAz+ar6WjAQM8faHdfXDjM+Bf+hV8Jc/5rGxavlAx81l2TYqhqs+lL10BAQH4+PhQtWpVQkJCAHjhhRdo3749kydPttb74IMPCA0NZe/evZw5c4YLFy5w1113WR/Q3rp1a2vdKlWqkJGRYT3epXz66accO3aMTZs2WYOOpk2blrivudq2bcu4ceMAGDNmDFOnTiUwMJAhQ4ZY+zVr1ix++eUXOnbsWNyhrCZNmkT37t2L3D5t2jTOnj1Lv379Lru97kjB0pXEJrN0znZbjp2HYyqzJM5StRb0fcd8/9t3MP9+8w7N0I7QY7I5DLd7CexaAnFjzXqRg+G2aWaw1eYes6x+pJmhSt0L/xhlTiD/Yx388jkc3WnWyc6AzR/Atvlw76fQ5Kby769UClu2bGHVqlVUr1690Lbff/+d6OhobrnlFlq3bk1MTAzR0dH885//pGbN0i2impiYSLt27cqcnWnTpo31vaenJ7Vr17YJ4oKDgwGsj/UoicjIyCK3ffbZZ0ycOJGvvvqKoCD90QwKlq4sNnfDFcgsFRyCA2WWpHw0vRWe3Ax/boJmPc0H70Y+bA6v/S/WrFOjLkS/aAZK+flUhcErIeUXaPQPc8mC0Bug8wg4ewwsHnB0N3w3EQ5vhv/0heBW0LgbXDcQgq8t3766iSrenuyaFFNh37u0cnJyuOOOO3j55ZcLbatbty6enp6sXLmS9evXExcXx1tvvcXYsWPZsGED4eHhl9/Wi4/aKIqHhwdGgbmiuSum5+ft7W3z2WKx2JTlDktezvBetWrV7JYvWLCAwYMH8/nnn3PrrbeW+HjuThO8ryQXihmGy7aXWVKwJOUkoAFce6cZKAH0fNkMfnJFPQ4+9n+5U+Uqc56SR75fZxaLGexXC4TwLvDQMgjtYG77awfEz4TZXWDrx8XffSd2WSwWqvp4VcjrclZj9vHxITs7b25V+/bt2blzJ2FhYTRt2tTmlRs8WCwWOnfuzL///W8SEhLw8fFh8eLFdo93KW3atCExMZETJ07Y3V6nTh2Sk5NtyhITE0t8fEf77LPPGDRoEJ9++im33XZbhbWjMlKwdCUpbp0lu5klDcNJBfH0Nhew7PgYXP+I+SoLL1+49zPoNAxuGmdms4xsWPIkzO1u+9xEcRthYWFs2LCBgwcPkpqayuOPP86JEycYMGAAGzduZP/+/cTFxfHwww+TnZ3Nhg0bmDx5Mps3byYpKYlFixZx7NgxWrRoYT3eL7/8wp49e0hNTbWbBcpvwIABhISE0LdvX9atW8f+/ftZuHAh8fHxANx8881s3ryZjz/+mH379jFhwgR27Njh9PNiz2effcYDDzzAtGnT6NixIykpKaSkpJCWllYh7alsFCxdSYrNLNkJlpRZkopUrTb0mGLOU/IufjijxMeLfhG6Pg0D/wddnwXvaubw37fPlv34UumMHj0aT09PWrZsSZ06dcjMzGTdunVkZ2cTExNDq1atGD58OAEBAXh4eODv78+PP/5Ir169aNasGePGjWPatGn07NkTgCFDhtC8eXMiIyOpU6cO69atK/b7+/j4EBcXR1BQEL169aJ169ZMnTrV+jDXmJgYxo8fb71l//Tp0zzwwANOPy/2vPvuu1y4cIHHH3+cunXrWl/Dhw+vkPZUNhaj4ICpXLb09HQCAgJIS0vD39+/optjX042TMo3ybD21eY8kVypv8HMCNt9nv5day2Je/tzC7x/C2DAv36Eum0rukWVzt9//82BAwcIDw/Hz0+PQBLXU9zPcEmv38osXSkKLUJZIEa2l1mqovU1xM01iDDnSgF8NgBOHrTdbhjww/+DudGQUmB45Oiv8Ec8XOaaOSLiehQsXSnyz1cCc6G+/OwFSx768ZArQM9XILCZud7TR7ebazQBpB+Br56AH1+FQxvgw57m8gPJ28yyd7vAhz3ggxgzcJIrzuTJk6levbrdV+7QXXkbOnRokW0aOnRohbTJHWgYzgFcYhgu/QhMb5H3uWYYDN+W9/nQJphb4DbRiZrYJ1eI9GT4qBec2G9+bnAD/Lmx5Pt7+pjrQEUMgqBrnNLEiqJhuKKdOHGiyDvdqlSpQv369cu5ReZaS+np6Xa3+fv7X5HrJjliGE7rLF0pCg7DlSSzJHKl8K8LDy2HVS/B1nl5gVKdFuYyBg07wvo3YfVU8PCGRlHQ4g7zrrplT8Pe5bBhlvmqGWbuG3QtHNtt/qFSMxzunA31rquoHooT1KpVq9I9DiQoKOiKDIicTcHSlaLgIpQFE4r2VvAWuZLUCIbeb8K1fc1HrLSLhZBWedtvfBoiHjLvzMu/5tOA+bDrK/PxLWmH8uY95Z//dGw3vHezeTdezTCz7ukU6PIU3Dja+X0TkTJRsOTODCNvxeOCwVChzNLF7R5eUPc6uHmc05snUik1udl82WPv7lCLxQywmsXA9i/gyFZz9fB67aF2E/CtAT/NgANrCj8M+IcXocH10Liro3shIg6kYMldrXvTfAL7Q8ugTvPCwVFRw3D12sMjK8unjSLuxLsKtI81XwU1uRk+HwQ7zZWg6TQMUvfB3m9h22cKlkQqOQVL7mrlePPrT6+bcyVKGix52j6DSEQc5K73oPltZqapeQ9I+tkMln5dZi4/oLtPRSotBUvuKP98pNxhg4JzlIoahlOwJOIcnt7Q5p68z/UjwNMXMtLg5AFzyE5EKiX9KeOOzh7Le39VI/NriTNLPs5rl4jk8fSGkNbm+yMJFdsWESmWgiV3dPz3vPe5wc8lg6Us2/oi4nz125tfD2+t2Ha4qW7dujFixAiHHW/QoEH07dvXYccT16FgyR2l/Znvw8Xht5IGSx4amRUpN/XamV+VWRI3ZxgGFy5cqOhmlJqCJXeUk+8HMjcoyv2aGwwVWmfpgu12EXG+ehczS8nbzIddi8MMGjSINWvW8MYbb2CxWLBYLBw8eJBdu3bRq1cvqlevTnBwMLGxsaSmplr3++KLL2jdujVVqlShdu3a3HrrrZw9e5aJEycyb948vvrqK+vxVq9efcl2/Pnnn9x7773UqlWLatWqERkZyYYNG6xtLJipGjFiBN26dbN+7tatG08++SQjRoygZs2aBAcHM2fOHM6ePctDDz1EjRo1aNKkCd9++22Jzsvq1auxWCwsXbqUtm3b4ufnR4cOHdi+fbtNvfXr13PjjTdSpUoVQkNDGTZsGGfPnrVu/+STT4iMjKRGjRqEhIQwcOBAjh49Wuj7rFixgsjISHx9fVm7di3btm3jpptuokaNGvj7+xMREcHmzXkPdV+4cCHXXnstvr6+hIWFMW3aNJt2hYWFMXnyZB5++GFq1KhBw4YNmTNnTon6XhYKltxR/qyRUSCzZA2WCmSWFCyJlL/Aq8G7GmSdNZcScBWGAZlnK+ZVwid0vfHGG0RFRTFkyBCSk5NJTk7G29ubrl27ct1117F582aWL1/OX3/9Rb9+/QBITk5mwIABPPzww+zevZvVq1dz1113YRgGo0ePpl+/fvTo0cN6vE6dOhXbhjNnztC1a1eOHDnCkiVL2LZtG8888ww5l/nw5Xnz5hEYGMjGjRt58sknefTRR7nnnnvo1KkTW7duJSYmhtjYWM6dO1fiYz799NO89tprbNq0iaCgIHr37k1WljnCsH37dmJiYrjrrrv45ZdfWLBgAT/99BNPPPGEdf/MzExefPFFtm3bxpdffsmBAwcYNGhQoe/zzDPPMGXKFHbv3k2bNm247777aNCgAZs2bWLLli0899xzeHubNxZt2bKFfv36ce+997J9+3YmTpzI+PHj+eijj2yOOW3aNCIjI0lISOCxxx7j0Ucf5ddfnft8Rl0Z3ZFNsHTxfe5frUUFS0aB7SLifB6e5jpoR7ZC6l7Xea5c1jmYXK9ivvfzR2xXUC9CQEAAPj4+VK1alZCQEABeeOEF2rdvz+TJk631PvjgA0JDQ9m7dy9nzpzhwoUL3HXXXTRqZN4c07p1a2vdKlWqkJGRYT3epXz66accO3aMTZs2WR+L0rRp0xJ3NVfbtm0ZN85cKHjMmDFMnTqVwMBAhgwZYu3XrFmz+OWXX+jYsWOJjjlhwgS6d+8OmMFYgwYNWLx4Mf369ePVV19l4MCB1vleV199NW+++SZdu3Zl1qxZ+Pn58fDDD1uP1bhxY958801uuOEGzpw5Q/Xq1a3bJk2aZP0+AElJSTz99NNcc8011mPnmj59Orfccgvjx5tL3zRr1oxdu3bx6quv2gRivXr14rHHHgPg2Wef5fXXX2f16tXWYzqDMkvuqGAglL/Mw9N+HWtmST8SIuWq9sWL54nfi68nZbZlyxZWrVpF9erVra/cC+zvv/9O27ZtueWWW2jdujX33HMP7733HidPniz190tMTKRdu3Zlfn5cmzZtrO89PT2pXbu2TRAXHBwMYDMMdilRUVHW97Vq1aJ58+bs3r0bMM/TRx99ZHOeYmJiyMnJ4cCBAwAkJCTQp08fGjVqRI0aNaxDh0lJSTbfJzIy0ubzqFGjeOSRR7j11luZOnUqv/+e93O/e/duOnfubFO/c+fO7Nu3j+zsvGHq/OfDYrEQEhJyWX0vDaUR3FK+NLV1ztLFMg9v2/JcBTNPIlI+coOl479VbDsuh3dVM8NTUd+7lHJycrjjjjt4+eWXC22rW7cunp6erFy5kvXr1xMXF8dbb73F2LFj2bBhA+Hh4Zf9/apUqVLsdg8PD4wCw4q5Q2H55Q5T5bJYLDZllouPtbrc4b2C8h/nX//6F8OGDStUp2HDhpw9e5bo6Giio6P55JNPqFOnDklJScTExJCZaftQ9mrVbLOAEydOZODAgSxdupRvv/2WCRMmMH/+fO68804Mw7C2IVfB8wP2z0dZ+34pujK6o1LNWVKwJFIhchejPO5CmSWLpURDYRXNx8fHJiPRvn17Fi5cSFhYGF5e9n/XWSwWOnfuTOfOnXnhhRdo1KgRixcvZtSoUYWOdylt2rTh/fff58SJE3azS3Xq1GHHjh02ZYmJiYWCAWf4+eefadiwIQAnT55k79691ixb+/bt2blzZ5FDhtu3byc1NZWpU6cSGhoKYDNJ+1KaNWtGs2bNGDlyJAMGDODDDz/kzjvvpGXLlvz00082ddevX0+zZs3w9PQsTTcdRmMu7sgmELrMCd6Wiv2BFLni1LqYsThxoGLb4YbCwsLYsGEDBw8eJDU1lccff5wTJ04wYMAANm7cyP79+4mLi+Phhx8mOzubDRs2MHnyZDZv3kxSUhKLFi3i2LFjtGjRwnq8X375hT179pCammo3C5TfgAEDCAkJoW/fvqxbt479+/ezcOFC4uPjAbj55pvZvHkzH3/8Mfv27WPChAmFgidnmTRpEt9//z07duxg0KBBBAYGWu/Me/bZZ4mPj+fxxx8nMTGRffv2sWTJEp588knAzC75+Pjw1ltvsX//fpYsWcKLL754ye95/vx5nnjiCVavXs0ff/zBunXr2LRpk/X8PvXUU3z//fe8+OKL7N27l3nz5jFz5kxGjx7ttPNQUgqW3JFhbxiu4Jylgo8/ybbdLiLlI8D8y5wzf8GFzOLrymUZPXo0np6etGzZkjp16pCZmcm6devIzs4mJiaGVq1aMXz4cAICAvDw8MDf358ff/yRXr160axZM8aNG8e0adPo2bMnAEOGDKF58+ZERkZSp04d1q1bV+z39/HxIS4ujqCgIHr16kXr1q2ZOnWqNUsSExPD+PHjeeaZZ7j++us5ffo0DzzwgNPPC8DUqVMZPnw4ERERJCcns2TJEnx8zEWJ27Rpw5o1a9i3bx9dunShXbt2jB8/nrp16wJmRuyjjz7i888/p2XLlkydOpXXXnvtkt/T09OT48eP88ADD9CsWTP69etHz549+fe//w2YGa3//e9/zJ8/n1atWvHCCy8wadIku3fZlTvDxbz99ttGWFiY4evra7Rv39748ccfi62/evVqo3379oavr68RHh5uzJo1q1Cd119/3WjWrJnh5+dnNGjQwBgxYoRx/vz5ErcpLS3NAIy0tLTL7o9T/PyuYUzwN1/r3jLLdn1tfn6jXd62nJy8fZY/b5bFja+YNotcqbKzDWNSHfP/3+6l5v/Fo3sM43CCYSRvr+jWGefPnzd27dp1Wb8TpfJatWqVARgnT56s6KaUm+J+hkt6/XapCSoLFixgxIgRvPPOO3Tu3Jl3332Xnj17smvXLuvYa34HDhygV69eDBkyhE8++YR169bx2GOPUadOHe6++24A/vvf//Lcc8/xwQcf0KlTJ/bu3WuNYl9//fXy7J7j2Fs6oOAwHJjZpdzJdFpnSaRieHhAQH04sR/mDzDL4meaX72rwrAEqFGyW9VFxDlcahhu+vTpDB48mEceeYQWLVowY8YMQkNDmTVrlt36s2fPpmHDhsyYMYMWLVrwyCOP8PDDD9ukC+Pj4+ncuTMDBw4kLCyM6OhoBgwYcFmT1SqdksxZKlhPE7xFKk5AA/vlWefgpxnl2hQpucmTJ9vcXp//lTt0V96GDh1aZJuGDh1aIW1yBy5zZczMzLSu9plfdHQ069evt7tPfHw80dHRNmUxMTHMnTuXrKwsvL29+cc//sEnn3zCxo0bueGGG9i/fz/Lli3jwQcfLLItGRkZZGRkWD+np6eXoWdOUGxmycN+PU3wFqk4ufOWACIegj/Wmct8HN0JG+fA9YPN1b7BzEBtmANt74V611VIc8U0dOhQ6+rfBV1q2QBnmTRpUpETov39/QkKCrJ7O74Uz2WCpdTUVLKzs62Lb+UKDg4mJSXF7j4pKSl261+4cIHU1FTq1q3Lvffey7Fjx/jHP/5hfdDfo48+Wigoy2/KlCnWCWmVU/4J3iXNLOUOwylYEil3EYPgt+/A4gE3j4dqtc3yT/4Jv62EbZ/BLS+YZV89CX/8BBtmwahfwb9uhTX7SlerVq0yLzjpaEFBQQQFBVV0M9yOSw3DAXYXrCpYdqn6+ctXr17NSy+9xDvvvMPWrVtZtGgR33zzTbG3QY4ZM4a0tDTr69ChQ6XtjnOUeM6SnXoKlkTKX+gNMGo3DEvMC5TAzB4B7PzS/MPnr11moJRrx8LybKXIFctlMkuBgYF4enoWyiIdPXq0UPYoV0hIiN36Xl5e1K5t/kIaP348sbGxPPLII4D5HKCzZ8/yf//3f4wdOxYPO4//8PX1xdfX1xHdco5i5yx526+nCd4iFcvDs/AfK81iwNPXfBTKXzvhwBrb7ds/h05PUB6cvUKyiLM44mfXZa6MPj4+REREsHLlSu68805r+cqVK+nTp4/dfaKiovj6669tyuLi4oiMjLSukHru3LlCAZGnpyeGYbjuuG6xK3h72q+nCd4ilY9vDWh6K+xZCtv/B6kXH4nSaRj8/A4kJ0Lqvrz5TE7g4+ODh4cHR44coU6dOvj4+BSbzRepLAzDIDMzk2PHjuHh4WFdR6o0XOrKOGrUKGJjY4mMjCQqKoo5c+aQlJRkneE/ZswYDh8+zMcffwyYk+9mzpzJqFGjGDJkCPHx8cydO5fPPvvMesw77riD6dOn065dOzp06MBvv/3G+PHj6d27d4Uvr15qRhnmLGmCt0jlct0AM1ha90ZeWcu+cOxX2BcHOxZBt2ed9u09PDwIDw8nOTmZI0cq6HlwImVQtWpVGjZsaHekqKRcKljq378/x48fZ9KkSSQnJ9OqVSuWLVtGo0aNAEhOTrZ54nF4eDjLli1j5MiRvP3229SrV48333zTusYSwLhx47BYLIwbN47Dhw9Tp04d7rjjDl566aVy75/D2GTENMFbxKVdczuEdoBDG8zPoR2hfnto2ccMllZPNgOnXq9CtcDLO/aZo+a+oR3Bq+i/un18fGjYsCEXLly4rGejiVQ0T09PvLy8ypwNdalgCeCxxx7jscces7vto48+KlTWtWtXtm7dWuTxvLy8mDBhAhMmTHBUEyve5SxKWbCegiWRysVigQHzYfUUOHcCuj1nljXrYf5/zrkAOxdBzUZw68SSHzc7Cz66HVL3QNPucP8Xl2iG+aT7Yh/ymnEa0v6EoBYlb4eIC3C5u+GkBEo1Z0kTvEUqraq1zMzRP+fmzU+qFgj9P4GGnczPCZ+Yz5YzjLz/91l/m0sSnDlqPqjXMCDrvDl098XDZqAE5vIEx/aWvZ3zB8I7HeH3H8p+LJFKRFdGt1TMg3QtHoDFrKNgScS1Ne9pTgCf3hLOHoU9y8zX76tgcBx8MxL2r8qrH9oRTiXBaTtzj3Z9BV2fLn1bkjbAgR/N96smQ5ObS38skUpGmSV3ZHfpgItfLR4XAybs3w2nCd4irsXTG9rdb77//EH4ZYEZOM3pahsoARz62QyU/Oubc6F6vwW3X3wG5m/fla0dB3/Me//nZjj9V9mOJ1KJKI3gjoqbs5QbLBnZRSwdoGBJxOW0fwB+mm5b9nea+bXnq3DtnXAuFf5YDz7VoWVv8L74OI6Tf5hf/9wE509BlatK14aTB/N9MGDfCrNdIm5AmSV3ZG/OkjVzVERmyVCwJOKyaoVD89vM93Wvs90WMQiq1zEnXV8/GNr2zwuUwJwYXqeF+Ttg81yz7PwpWDEW4t825ziVRG7QVaux+fXA2lJ2RqTyUWbJHdm7y61gZil/GWjOkoiru/t98yG7dZrDDy+a6zJ1fLzYJQGs/jESFv8ffD8JNr4Hp5Pztu38Eh5eYfsQbntyM0st+5pZrqO7StkRkcpHmSV3VOzjTjwVLIm4I5+qENLKnMN00zjo95+8h+9eSut/QmBz833+QAngz42w/xJ3t13IhPTD5vvmvcyvx/aYyxOIuAEFS+6ouKUDNMFbxP15+Vycl+RXsvoennDHGxDWBTx9zKG0gZ9DB/PpCGz5yPyafQF+eh2WjrYdnks7ZP4+8a4K9SPMeVE5WbB3BeiZcuIGlEZwR8U97sRiyRcs5aunCd4iV7ZGUTDoG9uygAawYTbs/hpWvmCun5Sy3dxm8YBer5jvc4fgrmpkDtc17GjeXbfgPoj+f9DpyXLrhogzKLPklop53InFwwyY8peBHnciIoUFtzTXZgJzDlRuoASw5UNzRXHIC5ZqhplfOw3Lqxf/jrJL4vIULLkju0sHXGKdJevdcEo2ikg+N47Oex/WBZ7cCiFtIDsTts6Dw1vh5AFze26w1Lgr3HvxgeWnj8Dur8zHoHz5OPzvAUj9rVy7IFJWujK6o1LNWdIEbxGxo+mtcPsMqFrbnAcFEPEgLH0KvptoWzc3WAK4phe0i4WE/8Dng2zrHd8PQ9fmZblFKjllltzJ2VR490bYOCevrMRLB+Ru1zCciORjsUDkQ3mBEkDre+zXDWxq+znyIfu/U/7aDkcSHNdGESdTsORO1rwCydsKFF5uZknBkohcgl8A9J1duDy4te3n+hHw6Hp45Ae48WlzvabW/cxtP77m/HaKOIjGXNxJxunCZdbMkp0VvPNPBFewJCKX47oB5h10b7TNK6seVLhe0DXm1wYR5tcqtWD757BnKRz/3VymwDAuveilSAXST6c7yQ148ivpnCVN8BaRy1UzzFwlHKDJzSWbg1SnmTkPCiDhE/hfLLwSDicOmItbHlxnPjpFd9BJJaIrozvJDXhsyvS4ExFxou6TIPhaCOtc8n3a3gu/rbR9+G/8TPDwhg2zzM/X3A79P9EkcKkUdGV0Jzl2gqVi11mysyilRclGEbkMnl7Q7r7L2ye8a+GyzR/aTgP49RvYvQRa9ilb+0QcQFdGd1LsMNwl1lnK0TCciJST6nUg6FrbMiPbXLupZjh0ubi20/eTIPNs+bdPpAAFS+7EsDPGr3WWRKQyuul582uVWtDh0bzy9g9A5+FQrQ4c/81cxFIP5JUKpiujO7GXWSrp0gGGng0nIuWoxe0waClUqWk+g+6vHXBiv7mQpZ+/uQL4vDvMZ8wtGgLhN5r7tb4HfGtUbNvliqNgyZ3Ym7NUkgneOTl575VZEpHyEvaPvPcPfm27hEDo9dDvY/jsXti52HwB/DgNBsdBQP3yb69csTQM507s3g1XgsxS/v00wVtEKoLFUnitpWbRcOds8KkBnj7g6Qvpf8Ly5+DvNNi2wFxmYPsX8OeWimm3XBGURnAnpc4s5dtPmSURqUza9IOWfc3fW6l7YPY/zLvkDvwIf5+yrRv1hLmMQepec+mBBpEV0WJxQ7oyupPi5izl2FnB2xos5dtPwZKIVDZePubX4Gvh2jthx8LCgRKYazXlWj8T7noXwrtBtdrl0EhxZ7oyupMSZZYs+dZZshcsaYK3iFRiN4+DX5fChb+hx8tmABXUEvYuh2VPm+s+1Qwzn5P5xcPmH4ft7ocGN0DmGahRF+pcA2mH4M/NEHi1+ZiWkNbmZHMROxQsuZNi5yxd/OrhmS+zVCDrBMosiUjlVqsxDPjMfK5c5OC8eU7t7jPvsLN4gpcfLB0FWz82/yjc+rH5Ko6HF4R1gZa9oWl384/K/WsgbixUrQ1XR8N1A831oRz5HLvDW2HzB1C3rW1/pFLRldGd2F2UsiRzlnLXMLEosyQilV+Tm81XQX4Bee97v2m+1r0JG98zlxvwrwdHd8O5VKgeDPUjIDkRzp0wh/X2rzJfBZ0/aa759PM7ZiDmUx06DjUXz7T3OJas83DmKGSdg6samYtt+gUUrpu6z1weIfMMJPwHDq6Fuz8ws2P2/LUTzvwFjW/SY2DKmYIld2L3wZMluBsud8E3T2+nNk9EpNx1Hma+8jOMwsHG8d/NieO7lsCRreZz6qoHmRPFwzpD4qfmpPKsc+YQ4A//D3Z/AzePNwO3U3/A0V2Qec7MamWk2x6/eggEXWOuSH7dQIh8GL6baAZKuRn9XV/B1fPNYcOCNr0PS58y3/d6DW4Y4pDTIyWjYMmdlHbpgNzMkoeCJRG5AtjLytRuAv8Yab6ys8wAJn+9ln3gQqZ5R95v38Oql8ys1H/vNn+nFnyCgqevmanPOmd+PpNivsCcK5Vx2nz+HRZ4dL0552rlC/DV45AUDz1fBZ+qZv3U3+DbZ/OO/d1EuPYuTVwvRwqW3EmJlw4oMME7++LwXVGpXxGRK0lRWXYvH3MieEhrM/uzdjps+Qiycp9fZwEMCO0ID3xlHicj3fxD9K8d5rDbgTWw/XMzMAJo1gPqNDeHCDe9D6eSIOET2LEIOj1pzmNa/pw5zaLJLeYQYvI2+P7f5jCjlAtdHd2J3aUDLlJmSUTEcaoFQo/J0O05M8ip1Rha3AHnjkPVwLyJ2rl32DXsaL7a9Df/sN29xPwdHPW4ud23Btz3BXwzCv74ycxIrXnZfIGZqYqZbGanPu4LW+dB6A32h+zE4RQsuZNST/DWQ3RFRErFzx+6jMr7XD2o+PpePnDPh+bcpYwzUCM4b1ud5vDQUjiSCBvnwKGNcHyfue321805T0HXQLcxsHqyOWSX8F/zocThXRzeNcmjq6M7KThmDnbmLFnsTPDOHYZTZklEpFz4VDNf9tS7Dvq+Y86R+v178066Rp3ytt/4tDlfas8ySFoPH/eB6BehcTdzzSmLxfy9vvw5s070i9Dq7nLolPtSsOROSpxZyp2zlLvOUu4wnH4cREQqDS8faN6zcLmHB/zzQ9gXZ96lt/dbWPG8ua35bXBVKPy+ypyMDrBkuPn7/to7tTxMKenq6E7sTfDW0gEiIu7H289cQPPq7vCfuyDlF3MZgj1LC9fNPA0LB8O6Geaimr7VzT+Oa9Q1A67DW6BWE3N4sE7zcu+KK1Cw5E7sLh1QMLPkqQneIiLuwrsKPPyt+f5IIqx/y5ws7uFlTkK/4f9g7TRzDlTKdvNlz9Gd8Gl/eHwDePmWW/NdhYIld2J3GK4kmSUtHSAi4vLqXQf/nFu4POYluO4+cy0nD09zjae0w5B+GJrcBKEd4POH4OQBM9i6cXS5N72y09XRndhbwftyHneizJKIiHsKbmm+inLLC/DVY+bK5H4BWiG8AJd7Yt8777xDeHg4fn5+REREsHbt2mLrr1mzhoiICPz8/GjcuDGzZ88uVOfUqVM8/vjj1K1bFz8/P1q0aMGyZcuc1QXnsbvOkuYsiYjIJVw3EDo+Bhiw7Gnz0S5i5VLB0oIFCxgxYgRjx44lISGBLl260LNnT5KSkuzWP3DgAL169aJLly4kJCTw/PPPM2zYMBYuXGitk5mZSffu3Tl48CBffPEFe/bs4b333qN+/frl1S3HuezHneTeDad1lkRErmgWi7noZduBgAHr3qjoFlUqLnV1nD59OoMHD+aRRx4BYMaMGaxYsYJZs2YxZcqUQvVnz55Nw4YNmTFjBgAtWrRg8+bNvPbaa9x9t7nmxAcffMCJEydYv3493t5mZqVRo0bl0yFHK9HjTuyts6TMkojIFc9iMecrbfvUfP5d2mEIcMHEgRO4TGYpMzOTLVu2EB0dbVMeHR3N+vXr7e4THx9fqH5MTAybN28mK8sMEJYsWUJUVBSPP/44wcHBtGrVismTJ5Odbe82fFNGRgbp6ek2r0rBXmbJuu1iFsnes+E0Z0lERMB8oHCjzoAB2z6r6NZUGi4TLKWmppKdnU1wcLBNeXBwMCkpKXb3SUlJsVv/woULpKamArB//36++OILsrOzWbZsGePGjWPatGm89NJLRbZlypQpBAQEWF+hoaFl7J0TlWSCtzJLIiKS67r7zK87F1dsOyoRlwmWcllysyIXGYZRqOxS9fOX5+TkEBQUxJw5c4iIiODee+9l7NixzJo1q8hjjhkzhrS0NOvr0KFDpe2O81nnJV3MOmnOkoiIFKfJzebXo7vM59eJ6wRLgYGBeHp6FsoiHT16tFD2KFdISIjd+l5eXtSuXRuAunXr0qxZMzw985aAb9GiBSkpKWRmZto9rq+vL/7+/javSkuZJRERuRz+daF6iHmNmFIf3r8Vzp90zLET/guz/wF7ljvmeOXEZYIlHx8fIiIiWLlypU35ypUr6dSpk919oqKiCtWPi4sjMjLSOpm7c+fO/Pbbb+TkW6No79691K1bFx8fHwf3oiIUuBvOQyt4i4jIJdRvn/f+z02w6f2yBUxHEmDfd+ZaTinb4bP+sDeucL0Nc+DjvrD4UfhuYt6iyRXMpcZdRo0aRWxsLJGRkURFRTFnzhySkpIYOnQoYA6PHT58mI8//hiAoUOHMnPmTEaNGsWQIUOIj49n7ty5fPZZ3qS1Rx99lLfeeovhw4fz5JNPsm/fPiZPnsywYcMqpI8OV6LMklbwFhGRfK65DfbkW2/wh/9nvupcY36uURdqNYZdX0Kft+0/8BfgzFH4ZiT8+k3hbf+Lhfs+h/AbIe1POH8Kvn3atk7VQOj0hCN6VCYudXXs378/x48fZ9KkSSQnJ9OqVSuWLVtmvdU/OTnZZs2l8PBwli1bxsiRI3n77bepV68eb775pnXZAIDQ0FDi4uIYOXIkbdq0oX79+gwfPpxnn3223PvnFCV53IkySyIikl/re2DtdDjxu235sV/zvu5fZb5fMgwGLYWss3D2OGx6D65/xHzI79cjCj/cd+DnZqZq3wr47z1Q9zo49LP9dsSNhepB0KafI3t32VwqWAJ47LHHeOyxx+xu++ijjwqVde3ala1btxZ7zKioKH7+uYh/KFendZZERORyefnC/60yRx7mDzCHzjqPMB/S+3ca/LUD/lgP50/A2aPw9vW2+x/eArGLzUDJ4gH9/wtph8w/4K/ubmaTFtwHv31nGyhZPM2H+dZqbGakts6DpaMh9AaoGVaeZ8CGywVLcrlyM0slWWdJPw4iInKRX4D5dXCc+exRDzvTnBc+Ats/L1x+9pg5kRugeS+4ppftdm8/uPdT2PoxnE4x6/z+vbnOU+DVZp3bpkPSz5C6B95oC4/GF/98OydymQneUkolGYazzllSZklEROywFygBNLmlcFn9SNvPbQfY39fL13xg7y3joUEEdH0GWuVNk8HTC9rH5n3e/r/La7MDKZXg7oqb4J279pLmLImISGm07AMH1kCD682H8Sb/Yg6ZJW+DuHHQ9FZzsnhpXT8E/k6HkFbQorfj2n2ZFCy5vdzMUr5FKT0vLomQGyRpzpKIiJSGT1W4c3be54YdzK/1roNBdu6Au1zefnDz2LIfp4w0DOfuCq3S7Z0XLOUGSVrBW0REpEgKltxdbrCUnS8gyg2KCgZLyiyJiIgUomDJ3VnveMsNljyLHobTnCUREZFCFCy5PTsPy83NIGVffPZdbtCkFbxFREQKUbDk7gpmljy98wVLuZmlfPOZRERExIaCJXdnneB98W44D6+8oMgaLF3MMGnOkoiISCEKltxewWG4fHOWcoOkc8fNr1Vrl2/TREREXICCJXdXaIJ3vjlLuWVnjppfqweVb9tERERcgIIld2cdhsv3/Lf8E7xzcsyHIAJUDy7/9omIiFRyCpbcnZFjBkS5GSYPL9tFKc+fzMswVatTMW0UERGpxBQsuT0j71EnYM5Zyr8o5ZkU833V2prgLSIiYoeCJXeRO9xmrzw3cwQFMkuZcOYv872G4EREROxSsOQuShws5VtnKScLzhwz32sITkRExC4FS+7OyLGTWcq3ztL5k+b7KjXLv20iIiIuQMGS2ygis4SRtyAlFFhnKQv+PmW+r3KVE9smIiLiuhQsuYsih+Fy8lbqtniCxZJvBe9MOH/KfO93lbNbKCIi4pIULLmNEsxZyr0LLv+ilH+nme+VWRIREbFLwZLbKyZYys7MG4ZTZklERMQuBUvuorhhuPwP0QXbpQNyh+GUWRIREbFLwZLbKMkwnOfFr/kWpVRmSUREpFgKltxFsZmlgsNwFzNLmWfg6C7zvV+Ac9snIiLiohQsub18maXcuUq5wVJ+GoYTERGxS8GS27iMYTh7z4CrUss5zRIREXFxCpbcRUked1LwbrhcHl7KLImIiBRBwZLbuIw5Sx4FgiWvKs5rloiIiItTsOT2SpBZ8vIt3yaJiIi4EAVL7qJEd8N52n7NVS3Qee0SERFxcQqW3EZxc5YKLEpZUJ+3ndMkERERN1DE1VNcTlGZJXvDcADhN8KxPfDkVvCt7vTmiYiIuCoFS+7OyDFX6gbbid2xX5lBlJedNZdERETESsGS2yhmztLy58z3+ecqeXiAhwIlERGRS9GcJXdR3ATv08nm+4ITu0VEROSSFCy5jaLmLOWTdd75zRAREXEzCpauJOdPVXQLREREXI7LBUvvvPMO4eHh+Pn5ERERwdq1a4utv2bNGiIiIvDz86Nx48bMnj27yLrz58/HYrHQt29fB7e6HBR5N1w+f59yejNERETcjUsFSwsWLGDEiBGMHTuWhIQEunTpQs+ePUlKSrJb/8CBA/Tq1YsuXbqQkJDA888/z7Bhw1i4cGGhun/88QejR4+mS5cuzu5GxVFmSURE5LJZDKMkKYnKoUOHDrRv355Zs2ZZy1q0aEHfvn2ZMmVKofrPPvssS5YsYffu3dayoUOHsm3bNuLj461l2dnZdO3alYceeoi1a9dy6tQpvvzyyxK3Kz09nYCAANLS0vD39y9d58rq7HF4tfGl601Mc35bREREXEBJr98uk1nKzMxky5YtREdH25RHR0ezfv16u/vEx8cXqh8TE8PmzZvJysqylk2aNIk6deowePDgErUlIyOD9PR0m5eIiIi4J5cJllJTU8nOziY4ONimPDg4mJSUFLv7pKSk2K1/4cIFUlNTAVi3bh1z587lvffeK3FbpkyZQkBAgPUVGhp6mb1xBpdJEIqIiLgUlwmWclksFpvPhmEUKrtU/dzy06dPc//99/Pee+8RGFjyh8mOGTOGtLQ06+vQoUOX0QMnKcloaufhzm+HiIiIm3GZFbwDAwPx9PQslEU6evRooexRrpCQELv1vby8qF27Njt37uTgwYPccccd1u05OTkAeHl5sWfPHpo0aVLouL6+vvj6+pa1S+Wr/YNw678ruhUiIiIux2UySz4+PkRERLBy5Uqb8pUrV9KpUye7+0RFRRWqHxcXR2RkJN7e3lxzzTVs376dxMRE66t3797cdNNNJCYmVpLhtZKyl1nKl1ULaQ3FZOBERETEPpfJLAGMGjWK2NhYIiMjiYqKYs6cOSQlJTF06FDAHB47fPgwH3/8MWDe+TZz5kxGjRrFkCFDiI+PZ+7cuXz22WcA+Pn50apVK5vvcdVVVwEUKq/07A3DeXiaD8sF8K1Rvu0RERFxEy4VLPXv35/jx48zadIkkpOTadWqFcuWLaNRo0YAJCcn26y5FB4ezrJlyxg5ciRvv/029erV48033+Tuu++uqC44kZ1gyeIJXAyWfKqVa2tERETchUuts1RZVYp1lk6nwLTmtmXeVSHrnPl+4P+gWUz5t0tERKSScrt1luQS7A7D5UsceniWX1tERETciIIlt2FvGC7fP6+HS424ioiIVBoKltxFURO8re8VLImIiJSGgiV3ZjMMp2BJRESkNBQsuY2i7oa7SMGSiIhIqShYcheXHIbTBG8REZHSULDkNjRnSURExBkULLkzDcOJiIiUmYIld6G74URERJxCwZLb0KKUIiIizqBgyV3YyyxpGE5ERKTMFCy5K4sHeGgFbxERkbIqVbA0bNgw3nzzzULlM2fOZMSIEWVtk5RKwcySxTZAsmgYTkREpDRKFSwtXLiQzp07Fyrv1KkTX3zxRZkbJaVQcBjO4gFY8j4rsyQiIlIqpQqWjh8/TkBAQKFyf39/UlNTy9wocQCLBZtskyZ4i4iIlEqpgqWmTZuyfPnyQuXffvstjRs3LnOjxBEsttkmZZZERERKpVRX0FGjRvHEE09w7Ngxbr75ZgC+//57pk2bxowZMxzZPikpu8Nw+ShYEhERKZVSXUEffvhhMjIyeOmll3jxxRcBCAsLY9asWTzwwAMObaCUlL1gSZklERGRsir1FfTRRx/l0Ucf5dixY1SpUoXq1as7sl1SVpaCw3CasyQiIlIaZU431KlTxxHtkLK61DCcxYKIiIhcvhIHS+3bt+f777+nZs2atGvXDksxF9+tW7c6pHFyOeyss2TvESgiIiJyWUocLPXp0wdfX18A+vbt66z2SGkVyizZKRMREZHLVuJgacKECQBkZ2fTrVs32rRpQ82aNZ3WMCmjgsNwIiIiUiqXfUX19PQkJiaGU6dOOaE5UnoahhMREXGGUqUfWrduzf79+x3dFimLS03wFhERkVIp1RX1pZdeYvTo0XzzzTckJyeTnp5u85KKoGBJRETEGUq1dECPHj0A6N27t81dcYZhYLFYyM7OdkzrpPQsFo3CiYiIOECpgqVVq1Y5uh1SVoXufNOcJREREUcoVbAUHh5OaGhoobWWDMPg0KFDDmmYXK6Cw3BahFJERMQRSjWxJTw8nGPHjhUqP3HiBOHh4WVulJSCvQneWmdJRESkzEoVLOXOTSrozJkz+Pn5lblR4gAFH6QrIiIipXJZw3CjRo0CwGKxMH78eKpWrWrdlp2dzYYNG7juuusc2kApKTtzlpRZEhERKbPLCpYSEhIAM7O0fft2fHx8rNt8fHxo27Yto0ePdmwLpWTsPe5EREREyuyygqXcu+Aeeugh3njjDfz9/Z3SKCkNe+ssKbMkIiJSVqWas/Thhx/i7+/Pb7/9xooVKzh//jxgZpykkqhRT8NwIiIiDlCqYOnEiRPccsstNGvWjF69epGcnAzAI488wlNPPeXQBkoJ5Q+MmtwMd86quLaIiIi4kVIFSyNGjMDb25ukpCSbSd79+/dn+fLlDmucXI6LwVJAKMQuhpphaBhORESk7Eq1KGVcXBwrVqygQYMGNuVXX301f/zxh0MaJpfJGhflm9mtYTgREZEyK1Vm6ezZszYZpVypqan4+vqWuVHFeeeddwgPD8fPz4+IiAjWrl1bbP01a9YQERGBn58fjRs3Zvbs2Tbb33vvPbp06ULNmjWpWbMmt956Kxs3bnRmF5xLd8GJiIg4VKmCpRtvvJGPP/7Y+tlisZCTk8Orr77KTTfd5LDGFbRgwQJGjBjB2LFjSUhIoEuXLvTs2ZOkpCS79Q8cOECvXr3o0qULCQkJPP/88wwbNoyFCxda66xevZoBAwawatUq4uPjadiwIdHR0Rw+fNhp/XAOe1kkZZZERETKymKU4ha2Xbt20a1bNyIiIvjhhx/o3bs3O3fu5MSJE6xbt44mTZo4o6106NCB9u3bM2tW3uTlFi1a0LdvX6ZMmVKo/rPPPsuSJUvYvXu3tWzo0KFs27aN+Ph4u98jOzubmjVrMnPmTB544IEStSs9PZ2AgADS0tIqbjmFQ5tg7q1wVSMY8YtZFjce1r8J/g1g1M6KaZeIiEglVdLrd6kySy1btmTbtm3ccMMNdO/enbNnz3LXXXeRkJDgtEApMzOTLVu2EB0dbVMeHR3N+vXr7e4THx9fqH5MTAybN28mKyvL7j7nzp0jKyuLWrVqFdmWjIwM0tPTbV4V72LMm/8xNDeNhb6zYMj3FdMkERERN1CqCd4ANWvW5LbbbuP6668nJycHgE2bNgHQu3dvx7Qun9TUVLKzswkODrYpDw4OJiUlxe4+KSkpdutfuHCB1NRU6tatW2if5557jvr163PrrbcW2ZYpU6bw73//uxS9KGfefnDdwIpuhYiIiEsrVbC0fPlyHnjgAY4fP15oIUqLxUJ2drZDGmdPwQf4FvVQ3+Lq2ysHeOWVV/jss89YvXp1sQ8EHjNmjPU5eWCm8UJDQ0vUfqex/jtohreIiIgjlWoY7oknnuCee+7hyJEj5OTk2LycFSgFBgbi6elZKIt09OjRQtmjXCEhIXbre3l5Ubt2bZvy1157jcmTJxMXF0ebNm2KbYuvry/+/v42r4pnZxhOREREyqxUwdLRo0cZNWpUkUGKM/j4+BAREcHKlSttyleuXEmnTp3s7hMVFVWoflxcHJGRkXh7e1vLXn31VV588UWWL19OZGSk4xtfrhQsiYiIOFKpgqV//vOfrF692sFNubRRo0bx/vvv88EHH7B7925GjhxJUlISQ4cOBczhsfx3sA0dOpQ//viDUaNGsXv3bj744APmzp3L6NGjrXVeeeUVxo0bxwcffEBYWBgpKSmkpKRw5syZcu9fmWgBShEREaco1ZylmTNncs8997B27Vpat25tk6UBGDZsmEMaV1D//v05fvw4kyZNIjk5mVatWrFs2TIaNWoEQHJyss2aS+Hh4SxbtoyRI0fy9ttvU69ePd58803uvvtua5133nmHzMxM/vnPf9p8rwkTJjBx4kSn9MM5NAwnIiLiDKVaZ+n9999n6NChVKlShdq1a9tMlrZYLOzfv9+hjazsKsU6SwfXwUe9oPbV8OTmimmDiIiICynp9btUmaVx48YxadIknnvuOTw8SjWSJ86izJKIiIhDlSrSyczMpH///gqUKhXNWRIREXGGUkU7Dz74IAsWLHB0W6QstM6SiIiIU5RqGC47O5tXXnmFFStW0KZNm0ITvKdPn+6Qxsnl0ARvERERZyhVsLR9+3batWsHwI4dO2y2FbeatpQHnX8RERFHKlWwtGrVKke3Q8pK6yyJiIg4hWZouw0Nw4mIiDiDgiV3oQneIiIiTqFgyd0osyQiIuJQCpbchuYsiYiIOIOCJXehYTgRERGnULDkNnIneFdsK0RERNyNgiW3o2hJRETEkRQsuQtNWRIREXEKBUtuQ+ssiYiIOIOCJXehCd4iIiJOoWDJ3SizJCIi4lAKltyGJi2JiIg4g4Ild6FhOBEREadQsORuNAwnIiLiUAqW3IaG4URERJxBwZK70DCciIiIUyhYchtaZ0lERMQZFCy5HQVLIiIijqRgyV0YmrMkIiLiDAqW3IaG4URERJxBwZK70ARvERERp1Cw5G6UWRIREXEoBUtuQ3OWREREnEHBkrvQMJyIiIhTKFhyG5rgLSIi4gwKltyOgiURERFHUrDkLrTOkoiIiFMoWHIbGoYTERFxBgVL7kKZJREREadQsORulFkSERFxKAVLIiIiIsVQsOQutM6SiIiIU7hcsPTOO+8QHh6On58fERERrF27ttj6a9asISIiAj8/Pxo3bszs2bML1Vm4cCEtW7bE19eXli1bsnjxYmc134k0wVtERMQZXCpYWrBgASNGjGDs2LEkJCTQpUsXevbsSVJSkt36Bw4coFevXnTp0oWEhASef/55hg0bxsKFC6114uPj6d+/P7GxsWzbto3Y2Fj69evHhg0byqtbDqZgSURExJEshuE6t1F16NCB9u3bM2vWLGtZixYt6Nu3L1OmTClU/9lnn2XJkiXs3r3bWjZ06FC2bdtGfHw8AP379yc9PZ1vv/3WWqdHjx7UrFmTzz77rETtSk9PJyAggLS0NPz9/UvbvbLZtgAW/x80vgke+LJi2iAiIuJCSnr9dpnMUmZmJlu2bCE6OtqmPDo6mvXr19vdJz4+vlD9mJgYNm/eTFZWVrF1ijpm5aVhOBEREWfwqugGlFRqairZ2dkEBwfblAcHB5OSkmJ3n5SUFLv1L1y4QGpqKnXr1i2yTlHHBMjIyCAjI8P6OT09/XK743ia4C0iIuIULpNZymUpkDkxDKNQ2aXqFyy/3GNOmTKFgIAA6ys0NLTE7RcRERHX4jLBUmBgIJ6enoUyPkePHi2UGcoVEhJit76Xlxe1a9cutk5RxwQYM2YMaWlp1tehQ4dK0yUH0zCciIiIM7hMsOTj40NERAQrV660KV+5ciWdOnWyu09UVFSh+nFxcURGRuLt7V1snaKOCeDr64u/v7/Nq8JpGE5ERMQpXGbOEsCoUaOIjY0lMjKSqKgo5syZQ1JSEkOHDgXMjM/hw4f5+OOPAfPOt5kzZzJq1CiGDBlCfHw8c+fOtbnLbfjw4dx44428/PLL9OnTh6+++orvvvuOn376qUL6WGbKLImIiDiUSwVL/fv35/jx40yaNInk5GRatWrFsmXLaNSoEQDJyck2ay6Fh4ezbNkyRo4cydtvv029evV48803ufvuu611OnXqxPz58xk3bhzjx4+nSZMmLFiwgA4dOpR7/8rGZVaAEBERcSkutc5SZVUp1lna+h9Y8gRcHQP3/a9i2iAiIuJC3G6dJbkUTfAWERFxBgVLbkfBkoiIiCMpWHIXGk0VERFxCgVLbkPDcCIiIs6gYMldaJ0lERERp1Cw5G6UWRIREXEoBUtuQ3OWREREnEHBkrvQBG8RERGnULDkNjTBW0RExBkULLkdBUsiIiKOpGDJXWgYTkRExCkULLkbDcOJiIg4lIIld6F1lkRERJxCwZK7UWZJRETEoRQsuQ3NWRIREXEGBUvuQsNwIiIiTqFgyW1onSURERFnULDkdhQsiYiIOJKCJXehdZZEREScQsGS29AwnIiIiDMoWHI7CpZEREQcScGSu9AwnIiIiFMoWHIbGoYTERFxBgVL7kLrLImIiDiFgiV3o8ySiIiIQylYchuasyQiIuIMCpbchYbhREREnELBktvQBG8RERFnULDkdhQsiYiIOJKCJXehdZZEREScQsGS28gdhqvYVoiIiLgbBUvuQhO8RUREnELBkrvRBG8RERGH8qroBkgZGAZs+QhqN0XrLImIiDiHMkuubPMH8M0ImHd7vlhJmSURERFHUrDkyn58Nd8HrbMkIiLiDAqWXNnpZDuFCpZEREQcScGSS8sXGGmdJREREadQsOTKbIbcNAwnIiLiDC4TLJ08eZLY2FgCAgIICAggNjaWU6dOFbuPYRhMnDiRevXqUaVKFbp168bOnTut20+cOMGTTz5J8+bNqVq1Kg0bNmTYsGGkpaU5uTcOYsn3z6d1lkRERJzCZYKlgQMHkpiYyPLly1m+fDmJiYnExsYWu88rr7zC9OnTmTlzJps2bSIkJITu3btz+vRpAI4cOcKRI0d47bXX2L59Ox999BHLly9n8ODB5dGlsrPY+edTZklERMShXGKdpd27d7N8+XJ+/vlnOnToAMB7771HVFQUe/bsoXnz5oX2MQyDGTNmMHbsWO666y4A5s2bR3BwMJ9++in/+te/aNWqFQsXLrTu06RJE1566SXuv/9+Lly4gJdXZT89dobhRERExKFcIrMUHx9PQECANVAC6NixIwEBAaxfv97uPgcOHCAlJYXo6Ghrma+vL127di1yH4C0tDT8/f2LDZQyMjJIT0+3eVUIDcOJiIg4nUsESykpKQQFBRUqDwoKIiUlpch9AIKDg23Kg4ODi9zn+PHjvPjii/zrX/8qtj1Tpkyxzp0KCAggNDS0JN1wvPxDbjkXCpeJiIhImVVosDRx4kQsFkuxr82bNwNgsRMEGIZhtzy/gtuL2ic9PZ3bbruNli1bMmHChGKPOWbMGNLS0qyvQ4cOXaqrzpE/s5QbLCmzJCIi4lAVOinniSee4N577y22TlhYGL/88gt//fVXoW3Hjh0rlDnKFRISApgZprp161rLjx49Wmif06dP06NHD6pXr87ixYvx9vYutk2+vr74+voWW6dc2AzD5VRcO0RERNxYhQZLgYGBBAYGXrJeVFQUaWlpbNy4kRtuuAGADRs2kJaWRqdOnezuEx4eTkhICCtXrqRdu3YAZGZmsmbNGl5++WVrvfT0dGJiYvD19WXJkiX4+fk5oGflRcNwIiIizuYSc5ZatGhBjx49GDJkCD///DM///wzQ4YM4fbbb7e5E+6aa65h8eLFgDn8NmLECCZPnszixYvZsWMHgwYNomrVqgwcOBAwM0rR0dGcPXuWuXPnkp6eTkpKCikpKWRnZ1dIXy9L/rhIw3AiIiJOUdnvjbf673//y7Bhw6x3t/Xu3ZuZM2fa1NmzZ4/NgpLPPPMM58+f57HHHuPkyZN06NCBuLg4atSoAcCWLVvYsGEDAE2bNrU51oEDBwgLC3NijxzA7pwlERERcSSLYeihYmWVnp5OQECAddmBcvNyOJw/Yb5vdz8kfAIdH4MeU8qvDSIiIi6qpNdvlxiGkyLkn9SdkztsqGE4ERERR1Kw5NLyJQU1wVtERMQpFCy5MsNOsCQiIiIOpWDJldkMwylYEhERcQYFS67MJrN0cc6ShuFEREQcSsGSK7ObWVKwJCIi4kgKllyZhuFEREScTsGSS9PdcCIiIs6mYMmVaZ0lERERp1Ow5MrsDcMpsyQiIuJQCpZcmdZZEhERcToFS67KMLA7Z0nDcCIiIg6lYMlVFXz+sdZZEhERcQoFSy6rYLCkzJKIiIgzKFhyVfknd4PmLImIiDiJgiVXVVSwpGE4ERERh1Kw5KqKmrOkYTgRERGHUrDkqpRZEhERKRcKllyV5iyJiIiUCwVLLkt3w4mIiJQHBUuuqlBmSessiYiIOIOCJVdVVLAkIiIiDqVgyVUVuhsu6+IbZZZEREQcScGSqyoULOluOBEREWdQsOSyCgRLucNyCpZEREQcSsGSqyo4ZymXRf+kIiIijqQrq6tSsCQiIlIudGV1VQXnLOVSsCQiIuJQurK6KmWWREREyoWurK5KwZKIiEi50JXVZWkYTkREpDzoyuqqlFkSEREpF7qyuioFSyIiIuVCV1ZXpbvhREREyoWurK5KwZKIiEi50JXVVWkYTkREpFzoyuqylFkSEREpD7qyuipllkRERMqFy1xZT548SWxsLAEBAQQEBBAbG8upU6eK3ccwDCZOnEi9evWoUqUK3bp1Y+fOnUXW7dmzJxaLhS+//NLxHXA0BUsiIiLlwmWurAMHDiQxMZHly5ezfPlyEhMTiY2NLXafV155henTpzNz5kw2bdpESEgI3bt35/Tp04XqzpgxA4vF4qzmO54meIuIiJQLr4puQEns3r2b5cuX8/PPP9OhQwcA3nvvPaKiotizZw/NmzcvtI9hGMyYMYOxY8dy1113ATBv3jyCg4P59NNP+de//mWtu23bNqZPn86mTZuoW7du+XSqrJRZEhERKRcucWWNj48nICDAGigBdOzYkYCAANavX293nwMHDpCSkkJ0dLS1zNfXl65du9rsc+7cOQYMGMDMmTMJCQkpUXsyMjJIT0+3eZW7IoMlF8qOiYiIuACXCJZSUlIICgoqVB4UFERKSkqR+wAEBwfblAcHB9vsM3LkSDp16kSfPn1K3J4pU6ZY504FBAQQGhpa4n0dR8NwIiIi5aFCr6wTJ07EYrEU+9q8eTOA3flEhmFccp5Rwe3591myZAk//PADM2bMuKx2jxkzhrS0NOvr0KFDl7W/Q2gYTkREpFxU6JylJ554gnvvvbfYOmFhYfzyyy/89ddfhbYdO3asUOYoV+6QWkpKis08pKNHj1r3+eGHH/j999+56qqrbPa9++676dKlC6tXr7Z7bF9fX3x9fYttt9NpgreIiEi5qNBgKTAwkMDAwEvWi4qKIi0tjY0bN3LDDTcAsGHDBtLS0ujUqZPdfcLDwwkJCWHlypW0a9cOgMzMTNasWcPLL78MwHPPPccjjzxis1/r1q15/fXXueOOO8rSNedTsCQiIlIuXOJuuBYtWtCjRw+GDBnCu+++C8D//d//cfvtt9vcCXfNNdcwZcoU7rzzTiwWCyNGjGDy5MlcffXVXH311UyePJmqVasycOBAwMw+2ZvU3bBhQ8LDw8unc6WlYTgREZFy4RLBEsB///tfhg0bZr27rXfv3sycOdOmzp49e0hLS7N+fuaZZzh//jyPPfYYJ0+epEOHDsTFxVGjRo1ybbtzKLMkIiJSHiyGUdR4jpRUeno6AQEBpKWl4e/vXz7f9I/18GHPwuUDP4dm0YXLRURExEZJr99KQ7gqDcOJiIiUC11ZXVVRCUEP/ZOKiIg4kq6srkqZJRERkXKhK6urUrAkIiJSLnRldVm6G05ERKQ8uMzSAXJRxmnY8C74FjFrX8GSiIiIQylYcjXLx0DCf4rermBJRETEoXRldTV7VxS/XcGSiIiIQ+nK6moy0ovfrmBJRETEoXRldTUX/i5+u8VSPu0QERG5QihYcjfKLImIiDiUrqyurmBwpGBJRETEoXRldXUe3rafFSyJiIg4lK6srs5TwZKIiIgz6crqSnLsPOLEo8BSWQqWREREHEpXVleSkVa4TJklERERp9KV1ZWcP1m4TJklERERp9LjThzAMMyH2qanX2LByLI6eggyCjxA19fDtuzMWfBxcjtERETcQO51O/c6XhSLcakackl//vknoaGhFd0MERERKYVDhw7RoEGDIrcrWHKAnJwcjhw5Qo0aNbA4cAXt9PR0QkNDOXToEP7+/g47rqu40vsPOgegcwA6B1d6/0HnAJxzDgzD4PTp09SrVw8Pj6KnsWgYzgE8PDyKjUjLyt/f/4r9zwHqP+gcgM4B6Bxc6f0HnQNw/DkICAi4ZB3NBhYREREphoIlERERkWIoWKrEfH19mTBhAr6+vhXdlApxpfcfdA5A5wB0Dq70/oPOAVTsOdAEbxEREZFiKLMkIiIiUgwFSyIiIiLFULAkIiIiUgwFSyIiIiLFULDkZD/++CN33HEH9erVw2Kx8OWXX9psNwyDiRMnUq9ePapUqUK3bt3YuXOnTZ2MjAyefPJJAgMDqVatGr179+bPP/+0qXPy5EliY2MJCAggICCA2NhYTp065eTeXdql+r9o0SJiYmIIDAzEYrGQmJhY6Biu3H8o/hxkZWXx7LPP0rp1a6pVq0a9evV44IEHOHLkiM0x3PkcAEycOJFrrrmGatWqUbNmTW699VY2bNhgU8eVz8Gl+p/fv/71LywWCzNmzLApd+X+w6XPwaBBg7BYLDavjh072tRx93MAsHv3bnr37k1AQAA1atSgY8eOJCUlWbe78jm4VP8L/vvnvl599VVrnYrqv4IlJzt79ixt27Zl5syZdre/8sorTJ8+nZkzZ7Jp0yZCQkLo3r07p0+fttYZMWIEixcvZv78+fz000+cOXOG22+/nezsbGudgQMHkpiYyPLly1m+fDmJiYnExsY6vX+Xcqn+nz17ls6dOzN16tQij+HK/Yfiz8G5c+fYunUr48ePZ+vWrSxatIi9e/fSu3dvm3rufA4AmjVrxsyZM9m+fTs//fQTYWFhREdHc+zYMWsdVz4Hl+p/ri+//JINGzZQr169Qttcuf9QsnPQo0cPkpOTra9ly5bZbHf3c/D777/zj3/8g2uuuYbVq1ezbds2xo8fj5+fn7WOK5+DS/U//799cnIyH3zwARaLhbvvvttap8L6b0i5AYzFixdbP+fk5BghISHG1KlTrWV///23ERAQYMyePdswDMM4deqU4e3tbcyfP99a5/Dhw4aHh4exfPlywzAMY9euXQZg/Pzzz9Y68fHxBmD8+uuvTu5VyRXsf34HDhwwACMhIcGm3J36bxjFn4NcGzduNADjjz/+MAzjyjwHaWlpBmB89913hmG41zkoqv9//vmnUb9+fWPHjh1Go0aNjNdff926zZ36bxj2z8GDDz5o9OnTp8h9roRz0L9/f+P+++8vch93Ogcl+T3Qp08f4+abb7Z+rsj+K7NUgQ4cOEBKSgrR0dHWMl9fX7p27cr69esB2LJlC1lZWTZ16tWrR6tWrax14uPjCQgIoEOHDtY6HTt2JCAgwFrHVV2J/U9LS8NisXDVVVcBV945yMzMZM6cOQQEBNC2bVvA/c9BTk4OsbGxPP3001x77bWFtrt7/3OtXr2aoKAgmjVrxpAhQzh69Kh1m7ufg5ycHJYuXUqzZs2IiYkhKCiIDh062AxVufs5yO+vv/5i6dKlDB482FpWkf1XsFSBUlJSAAgODrYpDw4Otm5LSUnBx8eHmjVrFlsnKCio0PGDgoKsdVzVldb/v//+m+eee46BAwdaHxR5pZyDb775hurVq+Pn58frr7/OypUrCQwMBNz/HLz88st4eXkxbNgwu9vdvf8APXv25L///S8//PAD06ZNY9OmTdx8881kZGQA7n8Ojh49ypkzZ5g6dSo9evQgLi6OO++8k7vuuos1a9YA7n8O8ps3bx41atTgrrvuspZVZP+9Sr2nOIzFYrH5bBhGobKCCtaxV78kx3FV7tj/rKws7r33XnJycnjnnXcuWd/dzsFNN91EYmIiqampvPfee/Tr148NGzbY/cWXyx3OwZYtW3jjjTfYunXrZbfTHfqfq3///tb3rVq1IjIykkaNGrF06VKbC2ZB7nIOcnJyAOjTpw8jR44E4LrrrmP9+vXMnj2brl27Frmvu5yD/D744APuu+8+m/laRSmP/iuzVIFCQkIACkW7R48etWabQkJCyMzM5OTJk8XW+euvvwod/9ixY4WyVq7mSul/VlYW/fr148CBA6xcudKaVYIr5xxUq1aNpk2b0rFjR+bOnYuXlxdz584F3PscrF27lqNHj9KwYUO8vLzw8vLijz/+4KmnniIsLAxw7/4XpW7dujRq1Ih9+/YB7n8OAgMD8fLyomXLljblLVq0sN4N5+7nINfatWvZs2cPjzzyiE15RfZfwVIFCg8PJyQkhJUrV1rLMjMzWbNmDZ06dQIgIiICb29vmzrJycns2LHDWicqKoq0tDQ2btxorbNhwwbS0tKsdVzVldD/3EBp3759fPfdd9SuXdtm+5VwDuwxDMM6BOPO5yA2NpZffvmFxMRE66tevXo8/fTTrFixAnDv/hfl+PHjHDp0iLp16wLufw58fHy4/vrr2bNnj0353r17adSoEeD+5yDX3LlziYiIsM5ZzFWh/S/11HApkdOnTxsJCQlGQkKCARjTp083EhISrHc6TZ061QgICDAWLVpkbN++3RgwYIBRt25dIz093XqMoUOHGg0aNDC+++47Y+vWrcbNN99stG3b1rhw4YK1To8ePYw2bdoY8fHxRnx8vNG6dWvj9ttvL/f+FnSp/h8/ftxISEgwli5dagDG/PnzjYSEBCM5Odl6DFfuv2EUfw6ysrKM3r17Gw0aNDASExON5ORk6ysjI8N6DHc+B2fOnDHGjBljxMfHGwcPHjS2bNliDB482PD19TV27NhhPYYrn4NL/T8oqODdcIbh2v03jOLPwenTp42nnnrKWL9+vXHgwAFj1apVRlRUlFG/fn23+V1oGJf+OVi0aJHh7e1tzJkzx9i3b5/x1ltvGZ6ensbatWutx3Dlc1CS/wdpaWlG1apVjVmzZtk9RkX1X8GSk61atcoACr0efPBBwzDM5QMmTJhghISEGL6+vsaNN95obN++3eYY58+fN5544gmjVq1aRpUqVYzbb7/dSEpKsqlz/Phx47777jNq1Khh1KhRw7jvvvuMkydPllMvi3ap/n/44Yd2t0+YMMF6DFfuv2EUfw5yl0yw91q1apX1GO58Ds6fP2/ceeedRr169QwfHx+jbt26Ru/evY2NGzfaHMOVz8Gl/h8UZC9YcuX+G0bx5+DcuXNGdHS0UadOHcPb29to2LCh8eCDDxbqnzufg1xz5841mjZtavj5+Rlt27Y1vvzyS5tjuPI5KEn/3333XaNKlSrGqVOn7B6jovpvMQzDKH1eSkRERMS9ac6SiIiISDEULImIiIgUQ8GSiIiISDEULImIiIgUQ8GSiIiISDEULImIiIgUQ8GSiIiISDEULIlIhenWrRsjRowo0zEOHjyIxWIhMTHRIW0qrXPnznH33Xfj7++PxWLh1KlTduvNmTOH0NBQPDw8mDFjRomObbFY+PLLL4vcXlnOgYi78qroBojIlWvRokV4e3tXdDMcYt68eaxdu5b169cTGBhIQEBAoTrp6ek88cQTTJ8+nbvvvttuHRGpfBQsiUiFqVWrVkU3wWF+//13WrRoQatWrYqsk5SURFZWFrfddpv1AbEiUvlpGE5EKkzBYbiwsDAmT57Mww8/TI0aNWjYsCFz5syx2Wfjxo20a9cOPz8/IiMjSUhIKHTcXbt20atXL6pXr05wcDCxsbGkpqYCsHr1anx8fFi7dq21/rRp0wgMDCQ5ObnIti5cuJBrr70WX19fwsLCmDZtmk0/pk2bxo8//ojFYqFbt26F9v/oo49o3bo1AI0bN8ZisXDw4EEAZs2aRZMmTfDx8aF58+b85z//Kfa8leQciIgDlenJciIiZdC1a1dj+PDh1s+NGjUyatWqZbz99tvGvn37jClTphgeHh7G7t27DcMwjDNnzhh16tQx+vfvb+zYscP4+uuvjcaNGxuAkZCQYBiGYRw5csQIDAw0xowZY+zevdvYunWr0b17d+Omm26yfp+nn37aaNSokXHq1CkjMTHR8PX1NRYtWlRkOzdv3mx4eHgYkyZNMvbs2WN8+OGHRpUqVYwPP/zQMAzzwZ1DhgwxoqKijOTkZOP48eOFjnHu3Dnju+++MwBj48aNRnJysnHhwgXrk+bffvttY8+ePca0adMMT09P44cffrDuCxiLFy8u8TkQEcdSsCQiFcZesHT//fdbP+fk5BhBQUHGrFmzDMMwn0heq1Yt4+zZs9Y6s2bNsgkUxo8fb0RHR9t8n0OHDhmAsWfPHsMwDCMjI8No166d0a9fP+Paa681HnnkkWLbOXDgQKN79+42ZU8//bTRsmVL6+fhw4cbXbt2LfY4CQkJBmAcOHDAWtapUydjyJAhNvXuueceo1evXtbP+YOlkpwDEXEsDcOJSKXSpk0b63uLxUJISAhHjx4FYPfu3bRt25aqVata60RFRdnsv2XLFlatWkX16tWtr2uuuQYw5xUB+Pj48Mknn7Bw4ULOnz9/ybvSdu/eTefOnW3KOnfuzL59+8jOzi51X4s79u7du4usf6lzICKOpQneIlKpFLw7zmKxkJOTA4BhGJfcPycnhzvuuIOXX3650Lb8k6rXr18PwIkTJzhx4gTVqlUr8piGYWCxWAqVOYq9Yxcsc8b3FZGSUWZJRFxGy5Yt2bZtG+fPn7eW/fzzzzZ12rdvz86dOwkLC6Np06Y2r9yA6Pfff2fkyJG89957dOzYkQceeMAakBX1fX/66SebsvXr19OsWTM8PT3L1KcWLVrYPXaLFi2KbMulzoGIOJaCJRFxGQMHDsTDw4PBgweza9culi1bxmuvvWZT5/HHH+fEiRMMGDCAjRs3sn//fuLi4nj44YfJzs4mOzub2NhYoqOjeeihh/jwww/ZsWOHzd1tBT311FN8//33vPjii+zdu5d58+Yxc+ZMRo8eXeY+Pf3003z00UfMnj2bffv2MX36dBYtWlTksUtyDkTEsRQsiYjLqF69Ol9//TW7du2iXbt2jB07ttBwW7169Vi3bh3Z2dnExMTQqlUrhg8fTkBAAB4eHrz00kscPHjQuiRBSEgI77//PuPGjStyBez27dvzv//9j/nz59OqVSteeOEFJk2axKBBg8rcp759+/LGG2/w6quvcu211/Luu+/y4Ycf2l1+oKTnQEQcy2JoAFxERESkSMosiYiIiBRDwZKIiIhIMRQsiYiIiBRDwZKIiIhIMRQsiYiIiBRDwZKIiIhIMRQsiYiIiBRDwZKIiIhIMRQsiYiIiBRDwZKIiIhIMRQsiYiIiBRDwZKIiIhIMf4/WEEtgE2fjR8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Evaluation\n",
    "from qids_package import qids\n",
    "'model/dump/' \n",
    "load_model_path = '/kaggle/working/model/dump/'+ str(date.today()) + '/'+ str(date.today()) + '_Transformer'\n",
    "preprocess = CombinedScaler()\n",
    "lr = 5e-4\n",
    "n_epoch = 2\n",
    "# constructor = lambda o: optim.lr_scheduler.CyclicLR(o, base_lr=lr/10, max_lr=lr, step_size_up=n_epoch // 2,\n",
    "#                                                     cycle_momentum=False)\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)\n",
    "\n",
    "model = NN_wrapper(preprocess=preprocess, lr=lr, lr_scheduler_constructor=None, n_epoch=n_epoch,\n",
    "                   train_lookback=train_lookback, per_eval_lookback=eval_lookback, hidden_size=HIDDEN_SIZE, n_asset=54,\n",
    "                   network='Transformer', feature_name=feature, load_model_path=load_model_path,\n",
    "                   embed_asset=True, embed_offset=True, criterion='pearson', l2_weight=1e-5,\n",
    "                   is_eval=True,\n",
    "                   is_cuda=IS_CUDA)\n",
    "performance_eval, y_pred = evaluation_for_submission(model, given_ds=ds, lookback_window=train_lookback,\n",
    "                                                     per_eval_lookback=eval_lookback, qids=qids)\n",
    "plt.figure(0)\n",
    "plot_performance(performance_eval, metrics_selected=['test_cum_pearson', 'test_cum_r2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a571937",
   "metadata": {
    "papermill": {
     "duration": 0.078571,
     "end_time": "2023-03-15T19:29:05.045623",
     "exception": false,
     "start_time": "2023-03-15T19:29:04.967052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1033.447633,
   "end_time": "2023-03-15T19:29:08.125282",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-15T19:11:54.677649",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05aef2834a0f4edca21dbda7a4cbfb84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0c965f259f934e2fb99a4f4ee35442dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14277c824ebf42abab9cfec87ba9f103": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "194f431566e74789bc6113a3735874f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ebeda2551a0a43dba116eea67736e70f",
       "placeholder": "",
       "style": "IPY_MODEL_f239e2e6182d4cd1a160b9348c4de7e3",
       "value": " 699/700 [04:11&lt;00:00,  2.59it/s]"
      }
     },
     "453a30739cf84bffb5b78e0d5832a2fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0c965f259f934e2fb99a4f4ee35442dd",
       "max": 700.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_81759f23a6634e7f832778228bd73d5e",
       "value": 699.0
      }
     },
     "6b308a03170f416096e88cb9e0b30b8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f51b0c79fa3e415fb9438a2415014705",
       "placeholder": "",
       "style": "IPY_MODEL_968c79c1556e44d48e4a134d1ccc8c29",
       "value": "Day 1700, train r2=-274293.7185, test cum r2=-282430.2547, test cum pearson 0.0565: 100%"
      }
     },
     "727e0a3fd68949ec9486f8361dc787f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c34177166ce8466e868c077a776a4400",
       "placeholder": "",
       "style": "IPY_MODEL_14277c824ebf42abab9cfec87ba9f103",
       "value": "Validation on day 998, train_r2=-182201.8939, val_r2=-221356.6731, val_cum_r2=-116526.9451, val_cum_pearson=0.0862: 100%"
      }
     },
     "7a9482e6c2c443e098558c46b2e446e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6b308a03170f416096e88cb9e0b30b8b",
        "IPY_MODEL_453a30739cf84bffb5b78e0d5832a2fb",
        "IPY_MODEL_194f431566e74789bc6113a3735874f7"
       ],
       "layout": "IPY_MODEL_d5e92db93e5843759eb302058935dc46"
      }
     },
     "81759f23a6634e7f832778228bd73d5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "968c79c1556e44d48e4a134d1ccc8c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "acc9d73102f947428045cda58c5990df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3f4398a3a764c6692e9e42ca4e05548": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_727e0a3fd68949ec9486f8361dc787f2",
        "IPY_MODEL_ed1dbb74710a43299c4ca40f981fde16",
        "IPY_MODEL_fd28b7d2009a4b1ebbed9f956e048d1a"
       ],
       "layout": "IPY_MODEL_eb4af1b89b2a47a5875aaf9e70639f70"
      }
     },
     "c34177166ce8466e868c077a776a4400": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c637c5a0693847fd86c04a01998574a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5e92db93e5843759eb302058935dc46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb4af1b89b2a47a5875aaf9e70639f70": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebeda2551a0a43dba116eea67736e70f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec4e40d4c7e44e5faf48bd817d81e370": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ed1dbb74710a43299c4ca40f981fde16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c637c5a0693847fd86c04a01998574a3",
       "max": 779.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ec4e40d4c7e44e5faf48bd817d81e370",
       "value": 779.0
      }
     },
     "f239e2e6182d4cd1a160b9348c4de7e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f51b0c79fa3e415fb9438a2415014705": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd28b7d2009a4b1ebbed9f956e048d1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_acc9d73102f947428045cda58c5990df",
       "placeholder": "",
       "style": "IPY_MODEL_05aef2834a0f4edca21dbda7a4cbfb84",
       "value": " 779/779 [11:58&lt;00:00,  1.08s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
