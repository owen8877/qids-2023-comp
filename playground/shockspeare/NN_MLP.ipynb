{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class FCNet(nn.Module): # type1 - fully connected layers\n",
    "    def __init__(self, D_in, D_out, input_shape):\n",
    "        # D_in - dimension of input channel, should be 3 for point cloud coordinates\n",
    "        # D_out - number of classes label\n",
    "        # k - number of rays\n",
    "        # N - number of points on one ray\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape[0]*input_shape[1]*input_shape[2], 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, D_out)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "#         self.final_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.dropout(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "#         x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    def method_of_pred(self, x):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class oneDVerConvNet(nn.Module): # type3 - 1D Vertical Convolution\n",
    "    def __init__(self, D_in, D_out, input_shape, b_size):\n",
    "        # input_shape: without batch dimension\n",
    "        # b_size: batch size\n",
    "        super(oneDVerConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(D_in, 16, (3,1), stride = (1,1), padding = (1,0)),\n",
    "            # nn.BatchNorm2d(16), # no batch\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size = (3,1), stride = (1,1), padding= (1,0)),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size = (3,1), stride = (1,1), padding= (1,0)),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 1024, kernel_size = (3,1), stride = (1,1), padding= (1,0)),\n",
    "            # nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        n_size = self._get_conv_output(input_shape, b_size)\n",
    "        # print(n_size)\n",
    " #       self.fc1 = nn.Linear(n_size, 256)\n",
    " #        self.fc1 = nn.Linear(1024, 256)\n",
    " #        self.fc2 = nn.Linear(256, 64)\n",
    " #        self.fc3 = nn.Linear(64,D_out)\n",
    "        self.transform1 = nn.Conv2d(n_size, 256, kernel_size = (1,1), stride = (1,1), padding=0)\n",
    "        self.transform2 = nn.Conv2d(256, 64, kernel_size = (1,1), stride = (1,1), padding=0)\n",
    "        self.transform3 = nn.Conv2d(64, D_out, kernel_size = (1,1), stride = (1,1), padding=0)\n",
    "        # self.bn1 = nn.BatchNorm1d(256)\n",
    "        # self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def _get_conv_output(self, shape, b_size):\n",
    "        input = Variable(torch.rand(b_size, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(b_size, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # print(x.shape) # shape = (b_size, feature=1024, H=1, W = 54)\n",
    "        x = torch.max(x,3)[0]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # x = torch.max(x,3)[0] # questionable maybe do along [1]\n",
    "        # print(x.shape) # shape = (b_size, feature=1024, H=1) after max\n",
    "        #print(x.data.size())\n",
    "        # x = x.view(-1,self.num_flat_features(x))\n",
    "#        print(x.data.size())\n",
    "#         x = F.relu(self.bn1(self.fc1(x)))\n",
    "#         x = F.relu(self.bn2(self.drop_out(self.fc2(x))))\n",
    "        x = F.relu(self.transform1(x))\n",
    "        x = F.relu(self.drop_out(self.transform2(x)))\n",
    "        x = self.transform3(x)\n",
    "##        return x, new_features, contribution\n",
    "        return x.squeeze()\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class train_NN():\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        net,\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.net = net\n",
    "        self.n_jobs = n_jobs\n",
    "        self.positive = positive\n",
    "\n",
    "    def fit(self, X, y, nepoch=10):\n",
    "        for epoch in range(nepoch):\n",
    "            running_loss = 0\n",
    "            self.net.train()\n",
    "            np.random.seed()\n",
    "            self.optimizer.zero_grad()\n",
    "            for (i, data) in enumerate(X):\n",
    "                data, label = X, y[i]\n",
    "                label = label.view(-1)\n",
    "                num_examples = label.size()[0] # need this because it's not always the batch size\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(smatrix)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pipeline import Dataset, backtest\n",
    "from matplotlib import pyplot as plt\n",
    "from datatools import data_quantization, check_dataframe, extract_market_data\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pandas import Series, DataFrame, MultiIndex\n",
    "from visualization.metric import Performance\n",
    "from sklearn.metrics import r2_score\n",
    "from visualization.metric import plot_performance\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "dataset = Dataset.load('../../data/parsed')\n",
    "# dataset = load_mini_dataset('../../data/parsed_mini/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% DataLoading\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices matched\n",
      "Features matched\n",
      "DataFame is all good for the tests\n"
     ]
    }
   ],
   "source": [
    "quantized_fundamental, _ = data_quantization(dataset.fundamental)\n",
    "df = pd.concat([quantized_fundamental, dataset.fundamental, dataset.ref_return], axis=1).dropna()\n",
    "quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                            'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE']\n",
    "original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf']\n",
    "\n",
    "check_dataframe(df, expect_index=['day','asset'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Data Preprocessing\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "df['return_known'] = df['return'].shift(2*54)\n",
    "df=df.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf','return_known']\n",
    "quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                            'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE', 'return_known']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training always loop over all available data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42b007856de8495d892de83a3655be40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 0.3705, cum_train_r2: -0.58%\n",
      "Fold [200/995], Loss: 0.0517, cum_train_r2: -1.03%\n",
      "Fold [300/995], Loss: 0.0010, cum_train_r2: -0.76%\n",
      "Fold [400/995], Loss: 0.0006, cum_train_r2: -0.88%\n",
      "Fold [500/995], Loss: 0.0008, cum_train_r2: -3.21%\n",
      "Fold [600/995], Loss: 0.0014, cum_train_r2: -0.98%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c_/y_0x4rsd6tb4chd_y83jnpzr0000gn/T/ipykernel_2610/3159425957.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlookback_window\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0mdays_train_valid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdays_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlookback_window\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# last ends at valid-day-2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m             \u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train_true\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays_train_valid\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfeature_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays_train_valid\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mreturn_column\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0mX_np\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mswaplevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mascending\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1065\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_scalar_access\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1066\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_takeable\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1067\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_tuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1068\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1069\u001B[0m             \u001B[0;31m# we by definition only have the 0th axis\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m_getitem_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m   1245\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0msuppress\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mIndexingError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1246\u001B[0m             \u001B[0mtup\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_expand_ellipsis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1247\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_lowerdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1248\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1249\u001B[0m         \u001B[0;31m# no multi-index, so validate all of the indexers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m_getitem_lowerdim\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m    939\u001B[0m         \u001B[0;31m# we may have a nested tuples indexer here\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    940\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_nested_tuple_indexer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 941\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_nested_tuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    942\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    943\u001B[0m         \u001B[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m_getitem_nested_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1000\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtup\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1001\u001B[0;31m             \u001B[0mcheck_deprecated_indexers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1002\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1003\u001B[0m         \u001B[0;31m# we have too many indexers for our dim, but have at least 1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36mcheck_deprecated_indexers\u001B[0;34m(key)\u001B[0m\n\u001B[1;32m   2669\u001B[0m     if (\n\u001B[1;32m   2670\u001B[0m         \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2671\u001B[0;31m         \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2672\u001B[0m         \u001B[0;32mand\u001B[0m \u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2673\u001B[0m     ):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "n_splits = 995\n",
    "# n_epoch = 10\n",
    "learning_rate = 0.1\n",
    "lookback_window = 16\n",
    "# Define loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "# Define network\n",
    " # net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "# Define the optimizier\n",
    "# optimizer = optim.LBFGS(net.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.5)\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "# label_train_list = []\n",
    "# test_list2 = []\n",
    "# test_score_old = 0\n",
    "\n",
    "days = df.index.get_level_values('day').unique()\n",
    "tscv = TimeSeriesSplit(n_splits=min(n_splits, len(days)))\n",
    "pbar = tqdm(tscv.split(days), total=tscv.n_splits)\n",
    "\n",
    "cum_y_val_true = Series(dtype=float)\n",
    "cum_y_val_prediction = Series(dtype=float)\n",
    "performance = Performance()\n",
    "# feature_columns = quantile_feature\n",
    "feature_columns = original_feature\n",
    "return_column = ['return']\n",
    "\n",
    "\n",
    "# for epoch in range(n_epoch):\n",
    "for fold, (train, val) in enumerate(pbar):\n",
    "    # X, _ = data_quantization(df[original_feature])\n",
    "    running_loss = 0\n",
    "    net.train()\n",
    "    np.random.seed()\n",
    "    days_train = days[train]\n",
    "\n",
    "    if len(days_train) < 2:\n",
    "        print('Skipping this fold since we cannot truncate the last day.')\n",
    "        continue\n",
    "    if (lookback_window is not None) and (len(days_train) > lookback_window):\n",
    "        cum_y_train_true = Series(dtype=float)\n",
    "        cum_y_train_prediction = Series(dtype=float)\n",
    "        for i in range(lookback_window, len(days_train)):\n",
    "            days_train_valid = days_train[i-lookback_window:i] # last ends at valid-day-2\n",
    "            X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid[-1]),:][return_column]\n",
    "\n",
    "            X_np = X_train.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "            # shape (asset, days, feature) -> (ft, days, asset)\n",
    "            X_np_tensor = X_np.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "            X_np_tensor = X_np_tensor[np.newaxis,:]  # add batch dimension\n",
    "            X_torch = torch.from_numpy(X_np_tensor)\n",
    "            labels = torch.tensor(y_train_true['return'].values).to(torch.float)\n",
    "            #\n",
    "            # def closure():\n",
    "            #     optimizer.zero_grad()\n",
    "            #     outputs=net(X_torch)\n",
    "            #     loss = criterion(outputs, labels)\n",
    "            #     loss.backward()\n",
    "            #     return loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X_torch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # optimizer.step(closure) # need closure for LBFGS\n",
    "            optimizer.step()\n",
    "            cum_y_train_prediction = pd.concat([cum_y_train_prediction, Series(outputs.detach().numpy())])\n",
    "            cum_y_train_true = pd.concat([cum_y_train_true, y_train_true.squeeze()], ignore_index=True)\n",
    "\n",
    "        performance[fold,'train_r2'] = r2_score(cum_y_train_prediction, cum_y_train_true)\n",
    "        if fold % 100 == 0:\n",
    "            print('Fold [{}/{}], Loss: {:.4f}, cum_train_r2: {:.2f}%'.format(fold, len(pbar), loss.item(), r2_score(cum_y_train_prediction, cum_y_train_true)))\n",
    "        # Validation:\n",
    "        days_val = days[int(val)-lookback_window:int(val)]\n",
    "\n",
    "        X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val[-1],), :][return_column]\n",
    "        net.eval()\n",
    "        net.train(False)\n",
    "        with torch.no_grad():\n",
    "            X_np_val = X_val.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "            # shape (asset, days, feature) -> (ft, days, asset)\n",
    "            X_np_val_tensor = X_np_val.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "            X_np_val_tensor = X_np_val_tensor[np.newaxis,:]  # add batch dimension\n",
    "            X_torch_val = torch.from_numpy(X_np_val_tensor)\n",
    "            labels = torch.tensor(y_val_true['return'].values).to(torch.float)\n",
    "\n",
    "            outputs_val = net(X_torch_val)\n",
    "            cum_y_val_prediction = pd.concat([cum_y_val_prediction, Series(outputs_val.numpy())])\n",
    "            cum_y_val_true = pd.concat([cum_y_val_true, y_val_true.squeeze()], ignore_index=True)\n",
    "\n",
    "        val_r2 = r2_score(y_val_true.squeeze(), Series(outputs_val.numpy())) # Do I need index for series to corr or R2?\n",
    "        performance[fold, 'val_r2'] = val_r2\n",
    "        val_pearson = y_val_true.squeeze().corr(Series(outputs_val.numpy()))\n",
    "        performance[fold, 'val_pearson'] = val_pearson\n",
    "        val_cum_r2 = r2_score(cum_y_val_true, cum_y_val_prediction)\n",
    "        performance[fold, 'val_cum_r2'] = val_cum_r2\n",
    "        val_cum_pearson = cum_y_val_true.corr(cum_y_val_prediction)\n",
    "        performance[fold, 'val_cum_pearson'] = val_cum_pearson\n",
    "\n",
    "        pbar.set_description(f'Fold {fold}, val_cum_r2={val_cum_r2:.4f}, val_cum_pearson={val_cum_pearson:.4f}')\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    # if fold > 20:\n",
    "    #     break\n",
    "    # X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid,), :][return_column]\n",
    "    # X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val,), :][return_column]\n",
    "\n",
    "    #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training only use the current data (one data one training each fold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dcac9298fb14c2aba108bdd91d0cd54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 738.3954, cum_train_r2: -0.64%\n",
      "Fold [200/995], Loss: 196.6356, cum_train_r2: -0.49%\n",
      "Fold [300/995], Loss: 46.9000, cum_train_r2: -0.71%\n",
      "Fold [400/995], Loss: 67.9767, cum_train_r2: -0.55%\n",
      "Fold [500/995], Loss: 47.4468, cum_train_r2: -1.03%\n",
      "Fold [600/995], Loss: 26.7176, cum_train_r2: -1.31%\n",
      "Fold [700/995], Loss: 52.4082, cum_train_r2: -1.12%\n",
      "Fold [800/995], Loss: 184.5518, cum_train_r2: -0.21%\n",
      "Fold [900/995], Loss: 40.3373, cum_train_r2: -0.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1873f645f47445ff8e0d69c9ade7a812"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 738.3954, cum_train_r2: -0.64%\n",
      "Fold [200/995], Loss: 196.6356, cum_train_r2: -0.49%\n",
      "Fold [300/995], Loss: 46.9000, cum_train_r2: -0.71%\n",
      "Fold [400/995], Loss: 67.9767, cum_train_r2: -0.55%\n",
      "Fold [500/995], Loss: 47.4468, cum_train_r2: -1.03%\n",
      "Fold [600/995], Loss: 26.7176, cum_train_r2: -1.31%\n",
      "Fold [700/995], Loss: 52.4082, cum_train_r2: -1.12%\n",
      "Fold [800/995], Loss: 184.5518, cum_train_r2: -0.21%\n",
      "Fold [900/995], Loss: 40.3373, cum_train_r2: -0.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d2679851609405e8a79753326262133"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 738.3954, cum_train_r2: -0.64%\n",
      "Fold [200/995], Loss: 196.6356, cum_train_r2: -0.49%\n",
      "Fold [300/995], Loss: 46.9000, cum_train_r2: -0.71%\n",
      "Fold [400/995], Loss: 67.9767, cum_train_r2: -0.55%\n",
      "Fold [500/995], Loss: 47.4468, cum_train_r2: -1.03%\n",
      "Fold [600/995], Loss: 26.7176, cum_train_r2: -1.31%\n",
      "Fold [700/995], Loss: 52.4082, cum_train_r2: -1.12%\n",
      "Fold [800/995], Loss: 184.5518, cum_train_r2: -0.21%\n",
      "Fold [900/995], Loss: 40.3373, cum_train_r2: -0.26%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "n_splits = 995\n",
    "n_epoch = 3\n",
    "learning_rate = 0.03\n",
    "lookback_window = 16\n",
    "# Define loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "# Define feature\n",
    "original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf','return_known']\n",
    "feature_columns = original_feature\n",
    "return_column = ['return']\n",
    "num_feature = len(feature_columns)\n",
    "\n",
    "# Define network\n",
    " # net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "net = oneDVerConvNet(D_in=num_feature, D_out=1, input_shape=(num_feature,lookback_window,54), b_size=1)\n",
    "# Define the optimizier\n",
    "optimizer = optim.LBFGS(net.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.5)\n",
    "\n",
    "loss_list = []\n",
    "# label_train_list = []\n",
    "# test_list2 = []\n",
    "# test_score_old = 0\n",
    "days = df.index.get_level_values('day').unique()\n",
    "tscv = TimeSeriesSplit(n_splits=min(n_splits, len(days)))\n",
    "\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    pbar = tqdm(tscv.split(days), total=tscv.n_splits)\n",
    "\n",
    "    cum_y_val_true = Series(dtype=float)\n",
    "    cum_y_val_prediction = Series(dtype=float)\n",
    "    performance = Performance()\n",
    "    # feature_columns = quantile_feature\n",
    "\n",
    "    net.train()\n",
    "    # np.random.seed(44)\n",
    "    # for epoch in range(n_epoch):\n",
    "    for fold, (train, val) in enumerate(pbar):\n",
    "        # X, _ = data_quantization(df[original_feature])\n",
    "        running_loss = 0\n",
    "        days_train = days[train]\n",
    "\n",
    "        if len(days_train) < 2:\n",
    "            print('Skipping this fold since we cannot truncate the last day.')\n",
    "            continue\n",
    "        if (lookback_window is not None) and (len(days_train) > lookback_window):\n",
    "            cum_y_train_true = Series(dtype=float)\n",
    "            cum_y_train_prediction = Series(dtype=float)\n",
    "\n",
    "            days_train_valid = days_train[-lookback_window-1:-1] # last ends at valid-day-2\n",
    "            X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid[-1]),:][return_column]\n",
    "\n",
    "            X_np = X_train.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "                # shape (asset, days, feature) -> (ft, days, asset)\n",
    "            X_np_tensor = X_np.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "            X_np_tensor = X_np_tensor[np.newaxis,:]  # add batch dimension\n",
    "            X_torch = torch.from_numpy(X_np_tensor)\n",
    "            labels = torch.tensor(y_train_true['return'].values).to(torch.float)\n",
    "                #\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs=net(X_torch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X_torch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step(closure) # need closure for LBFGS\n",
    "            # optimizer.step()\n",
    "\n",
    "            scheduler.step() ### !!!!!\n",
    "            cum_y_train_prediction = pd.concat([cum_y_train_prediction, Series(outputs.detach().numpy())])\n",
    "            cum_y_train_true = pd.concat([cum_y_train_true, y_train_true.squeeze()], ignore_index=True)\n",
    "\n",
    "            performance[fold,'train_r2'] = r2_score(cum_y_train_prediction, cum_y_train_true)\n",
    "            if fold % 100 == 0:\n",
    "                print('Fold [{}/{}], Loss: {:.4f}, cum_train_r2: {:.2f}%'.format(fold, len(pbar), loss.item(), r2_score(cum_y_train_prediction, cum_y_train_true)))\n",
    "            # Validation:\n",
    "            days_val = days[int(val)-lookback_window:int(val)]\n",
    "\n",
    "            X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val[-1],), :][return_column]\n",
    "            net.eval()\n",
    "            net.train(False)\n",
    "            with torch.no_grad():\n",
    "                X_np_val = X_val.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "                # shape (asset, days, feature) -> (ft, days, asset)\n",
    "                X_np_val_tensor = X_np_val.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "                X_np_val_tensor = X_np_val_tensor[np.newaxis,:]  # add batch dimension\n",
    "                X_torch_val = torch.from_numpy(X_np_val_tensor)\n",
    "                labels = torch.tensor(y_val_true['return'].values).to(torch.float)\n",
    "\n",
    "                outputs_val = net(X_torch_val)\n",
    "                cum_y_val_prediction = pd.concat([cum_y_val_prediction, Series(outputs_val.numpy())])\n",
    "                cum_y_val_true = pd.concat([cum_y_val_true, y_val_true.squeeze()], ignore_index=True)\n",
    "\n",
    "            val_r2 = r2_score(y_val_true.squeeze(), Series(outputs_val.numpy())) # Do I need index for series to corr or R2?\n",
    "            performance[fold, 'val_r2'] = val_r2\n",
    "            val_pearson = y_val_true.squeeze().corr(Series(outputs_val.numpy()))\n",
    "            performance[fold, 'val_pearson'] = val_pearson\n",
    "            val_cum_r2 = r2_score(cum_y_val_true, cum_y_val_prediction)\n",
    "            performance[fold, 'val_cum_r2'] = val_cum_r2\n",
    "            val_cum_pearson = cum_y_val_true.corr(cum_y_val_prediction)\n",
    "            performance[fold, 'val_cum_pearson'] = val_cum_pearson\n",
    "\n",
    "            pbar.set_description(f'Fold {fold}, val_cum_r2={val_cum_r2:.4f}, val_cum_pearson={val_cum_pearson:.4f}')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # if fold > 20:\n",
    "        #     break\n",
    "        # X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid,), :][return_column]\n",
    "        # X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val,), :][return_column]\n",
    "\n",
    "        #\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline case: use only old return:\n",
    "- validation R_2 can achieve almost -0 using ADAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices matched\n",
      "Features matched\n",
      "DataFame is all good for the tests\n"
     ]
    }
   ],
   "source": [
    "quantized_fundamental, _ = data_quantization(dataset.fundamental)\n",
    "df = pd.concat([quantized_fundamental, dataset.fundamental, dataset.ref_return], axis=1).dropna()\n",
    "quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                            'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE']\n",
    "original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf']\n",
    "\n",
    "check_dataframe(df, expect_index=['day','asset'], shutup=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "df['return_known'] = df['return'].shift(2*54)\n",
    "df=df.dropna()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "645363c1e5ba4d339eac484a51e518e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 0.0032, cum_train_r2: -1274534.66%\n",
      "Fold [200/995], Loss: 0.0017, cum_train_r2: -2280200.11%\n",
      "Fold [300/995], Loss: 0.0009, cum_train_r2: -615003.87%\n",
      "Fold [400/995], Loss: 0.0004, cum_train_r2: -586695.02%\n",
      "Fold [500/995], Loss: 0.0004, cum_train_r2: -803927.17%\n",
      "Fold [600/995], Loss: 0.0006, cum_train_r2: -1908007.20%\n",
      "Fold [700/995], Loss: 0.0007, cum_train_r2: -946195.67%\n",
      "Fold [800/995], Loss: 0.0012, cum_train_r2: -1575971.34%\n",
      "Fold [900/995], Loss: 0.0004, cum_train_r2: -562351.15%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7a49bcdb9374da7af5f86f94dc6ef56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 0.0032, cum_train_r2: -1274622.31%\n",
      "Fold [200/995], Loss: 0.0017, cum_train_r2: -2280191.03%\n",
      "Fold [300/995], Loss: 0.0009, cum_train_r2: -615003.87%\n",
      "Fold [400/995], Loss: 0.0004, cum_train_r2: -586692.69%\n",
      "Fold [500/995], Loss: 0.0004, cum_train_r2: -803927.17%\n",
      "Fold [600/995], Loss: 0.0006, cum_train_r2: -1908033.95%\n",
      "Fold [700/995], Loss: 0.0007, cum_train_r2: -946192.65%\n",
      "Fold [800/995], Loss: 0.0012, cum_train_r2: -1575956.92%\n",
      "Fold [900/995], Loss: 0.0004, cum_train_r2: -562350.45%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21e64f44dc8d42b2a25255c43fb4a0ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 0.0032, cum_train_r2: -1274622.31%\n",
      "Fold [200/995], Loss: 0.0017, cum_train_r2: -2280191.03%\n",
      "Fold [300/995], Loss: 0.0009, cum_train_r2: -615003.87%\n",
      "Fold [400/995], Loss: 0.0004, cum_train_r2: -586692.69%\n",
      "Fold [500/995], Loss: 0.0004, cum_train_r2: -803927.17%\n",
      "Fold [600/995], Loss: 0.0006, cum_train_r2: -1908033.95%\n",
      "Fold [700/995], Loss: 0.0007, cum_train_r2: -946192.65%\n",
      "Fold [800/995], Loss: 0.0012, cum_train_r2: -1575956.92%\n",
      "Fold [900/995], Loss: 0.0004, cum_train_r2: -562350.45%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameter\n",
    "n_splits = 995\n",
    "n_epoch = 3\n",
    "learning_rate = 0.05\n",
    "lookback_window = 16\n",
    "# Define loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "# Define feature:\n",
    "# feature_columns = quantile_feature\n",
    "# feature_columns = original_featxure\n",
    "feature_columns = ['return_known']\n",
    "return_column = ['return']\n",
    "num_feature = len(feature_columns)\n",
    "\n",
    "# Define network\n",
    " # net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "net = oneDVerConvNet(D_in=num_feature, D_out=1, input_shape=(num_feature,lookback_window,54), b_size=1)\n",
    "# Define the optimizier\n",
    "optimizer = optim.LBFGS(net.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.5)\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "# label_train_list = []\n",
    "# test_list2 = []\n",
    "# test_score_old = 0\n",
    "\n",
    "days = df.index.get_level_values('day').unique()\n",
    "tscv = TimeSeriesSplit(n_splits=min(n_splits, len(days)))\n",
    "\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    pbar = tqdm(tscv.split(days), total=tscv.n_splits)\n",
    "    cum_y_val_true = Series(dtype=float)\n",
    "    cum_y_val_prediction = Series(dtype=float)\n",
    "    performance = Performance()\n",
    "    net.train()\n",
    "    np.random.seed(44)\n",
    "    # for epoch in range(n_epoch):\n",
    "    for fold, (train, val) in enumerate(pbar):\n",
    "        # X, _ = data_quantization(df[original_feature])\n",
    "        running_loss = 0\n",
    "        days_train = days[train]\n",
    "\n",
    "        if len(days_train) < 2:\n",
    "            print('Skipping this fold since we cannot truncate the last day.')\n",
    "            continue\n",
    "        if (lookback_window is not None) and (len(days_train) > lookback_window):\n",
    "            cum_y_train_true = Series(dtype=float)\n",
    "            cum_y_train_prediction = Series(dtype=float)\n",
    "\n",
    "            days_train_valid = days_train[-lookback_window-1:-1] # last ends at valid-day-2\n",
    "            X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid[-1]),:][return_column]\n",
    "\n",
    "            X_np = X_train.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "                # shape (asset, days, feature) -> (ft, days, asset)\n",
    "            X_np_tensor = X_np.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "            X_np_tensor = X_np_tensor[np.newaxis,:]  # add batch dimension\n",
    "            X_torch = torch.from_numpy(X_np_tensor)\n",
    "            labels = torch.tensor(y_train_true['return'].values).to(torch.float)\n",
    "                #\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs=net(X_torch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X_torch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step(closure) # need closure for LBFGS\n",
    "            # optimizer.step()\n",
    "\n",
    "            scheduler.step() ### !!!!!\n",
    "            cum_y_train_prediction = pd.concat([cum_y_train_prediction, Series(outputs.detach().numpy())])\n",
    "            cum_y_train_true = pd.concat([cum_y_train_true, y_train_true.squeeze()], ignore_index=True)\n",
    "\n",
    "            performance[fold,'train_r2'] = r2_score(cum_y_train_prediction, cum_y_train_true)\n",
    "            if fold % 100 == 0:\n",
    "                print('Fold [{}/{}], Loss: {:.4f}, cum_train_r2: {:.2f}%'.format(fold, len(pbar), loss.item(), r2_score(cum_y_train_prediction, cum_y_train_true)))\n",
    "            # Validation:\n",
    "            days_val = days[int(val)-lookback_window:int(val)]\n",
    "\n",
    "            X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val[-1],), :][return_column]\n",
    "            net.eval()\n",
    "            net.train(False)\n",
    "            with torch.no_grad():\n",
    "                X_np_val = X_val.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "                # shape (asset, days, feature) -> (ft, days, asset)\n",
    "                X_np_val_tensor = X_np_val.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "                X_np_val_tensor = X_np_val_tensor[np.newaxis,:]  # add batch dimension\n",
    "                X_torch_val = torch.from_numpy(X_np_val_tensor)\n",
    "                labels = torch.tensor(y_val_true['return'].values).to(torch.float)\n",
    "\n",
    "                outputs_val = net(X_torch_val)\n",
    "                cum_y_val_prediction = pd.concat([cum_y_val_prediction, Series(outputs_val.numpy())])\n",
    "                cum_y_val_true = pd.concat([cum_y_val_true, y_val_true.squeeze()], ignore_index=True)\n",
    "\n",
    "            val_r2 = r2_score(y_val_true.squeeze(), Series(outputs_val.numpy())) # Do I need index for series to corr or R2?\n",
    "            performance[fold, 'val_r2'] = val_r2\n",
    "            val_pearson = y_val_true.squeeze().corr(Series(outputs_val.numpy()))\n",
    "            performance[fold, 'val_pearson'] = val_pearson\n",
    "            val_cum_r2 = r2_score(cum_y_val_true, cum_y_val_prediction)\n",
    "            performance[fold, 'val_cum_r2'] = val_cum_r2\n",
    "            val_cum_pearson = cum_y_val_true.corr(cum_y_val_prediction)\n",
    "            performance[fold, 'val_cum_pearson'] = val_cum_pearson\n",
    "\n",
    "            pbar.set_description(f'Fold {fold}, val_cum_r2={val_cum_r2:.4f}, val_cum_pearson={val_cum_pearson:.4f}')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # if fold > 20:\n",
    "        #     break\n",
    "        # X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid,), :][return_column]\n",
    "        # X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val,), :][return_column]\n",
    "\n",
    "        #\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "0     False\n1     False\n2     False\n3     False\n4     False\n      ...  \n49     True\n50     True\n51     True\n52     True\n53     True\nLength: 2538, dtype: bool"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cum_y_train_prediction.isna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ending score for metric train_r2 is: -7.6914e+05\n",
      "The ending score for metric val_cum_r2 is: -3.1575e-04\n",
      "The ending score for metric val_cum_pearson is: 1.8511e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "# plot_performance(performance, metrics_selected=['train_r2', 'test_cum_r2', 'test_cum_pearson'])\n",
    "plot_performance(performance, metrics_selected=['train_r2', 'val_cum_pearson','val_cum_r2'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<visualization.metric.Performance at 0x175a70730>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "model_path = '../../model/dump/1DConv'\n",
    "torch.save(net.state_dict(), model_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Save model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "from qids_lib import QIDS\n",
    "from pipeline.backtest import evaluation_for_submission\n",
    "\n",
    "\n",
    "dataset = Dataset.load('../data/parsed')\n",
    "df = pd.concat([dataset.fundamental, extract_market_data(dataset.market)], axis=1).dropna()\n",
    "df['return_known'] = df['return'].shift(2*54)\n",
    "df=df.dropna()\n",
    "f_quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                      'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE', 'return_known']\n",
    "m_quantile_feature = ['avg_price_QUANTILE', 'volatility_QUANTILE', 'mean_volume_QUANTILE', 'return_known']\n",
    "# feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf', 'avg_price', 'volatility',\n",
    "#            'mean_volume']\n",
    "\n",
    "\n",
    "\n",
    "# q_df, _ = data_quantization(dataset.fundamental)\n",
    "# full_df = pd.concat([q_df, dataset.ref_return], axis=1).dropna()\n",
    "# model = linear_model(full_df[f_quantile_feature], full_df['return'])\n",
    "\n",
    "qids = QIDS(path_prefix='../')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Test\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}