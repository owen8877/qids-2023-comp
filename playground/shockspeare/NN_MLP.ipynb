{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class FCNet(nn.Module): # type1 - fully connected layers\n",
    "    def __init__(self, D_in, D_out, input_shape):\n",
    "        # D_in - dimension of input channel, should be 3 for point cloud coordinates\n",
    "        # D_out - number of classes label\n",
    "        # k - number of rays\n",
    "        # N - number of points on one ray\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape[0]*input_shape[1]*input_shape[2], 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, D_out)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "#         self.final_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.dropout(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "#         x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    def method_of_pred(self, x):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "class oneDVerConvNet(nn.Module): # type3 - 1D Vertical Convolution\n",
    "    def __init__(self, D_in, D_out, input_shape, b_size):\n",
    "        # input_shape: without batch dimension\n",
    "        # b_size: batch size\n",
    "        super(oneDVerConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(D_in, 16, (3,1), stride = (1,1), padding = (1,0)),\n",
    "            # nn.BatchNorm2d(16), # no batch\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size = (3,1), stride = (1,1), padding= (1,0)),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size = (3,1), stride = (1,1), padding= (1,0)),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 1024, kernel_size = (3,1), stride = (1,1), padding= (1,0)),\n",
    "            # nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = (2,1), stride = (2,1)))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        n_size = self._get_conv_output(input_shape, b_size)\n",
    "        # print(n_size)\n",
    " #       self.fc1 = nn.Linear(n_size, 256)\n",
    " #        self.fc1 = nn.Linear(1024, 256)\n",
    " #        self.fc2 = nn.Linear(256, 64)\n",
    " #        self.fc3 = nn.Linear(64,D_out)\n",
    "        self.transform1 = nn.Conv2d(n_size, 256, kernel_size = (1,1), stride = (1,1), padding=0)\n",
    "        self.transform2 = nn.Conv2d(256, 64, kernel_size = (1,1), stride = (1,1), padding=0)\n",
    "        self.transform3 = nn.Conv2d(64, D_out, kernel_size = (1,1), stride = (1,1), padding=0)\n",
    "        # self.bn1 = nn.BatchNorm1d(256)\n",
    "        # self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def _get_conv_output(self, shape, b_size):\n",
    "        input = Variable(torch.rand(b_size, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(b_size, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # print(x.shape) # shape = (b_size, feature=1024, H=1, W = 54)\n",
    "        x = torch.max(x,3)[0]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # x = torch.max(x,3)[0] # questionable maybe do along [1]\n",
    "        # print(x.shape) # shape = (b_size, feature=1024, H=1) after max\n",
    "        #print(x.data.size())\n",
    "        # x = x.view(-1,self.num_flat_features(x))\n",
    "#        print(x.data.size())\n",
    "#         x = F.relu(self.bn1(self.fc1(x)))\n",
    "#         x = F.relu(self.bn2(self.drop_out(self.fc2(x))))\n",
    "        x = F.relu(self.transform1(x))\n",
    "        x = F.relu(self.drop_out(self.transform2(x)))\n",
    "        x = self.transform3(x)\n",
    "##        return x, new_features, contribution\n",
    "        return x.squeeze()\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class train_NN():\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        net,\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.net = net\n",
    "        self.n_jobs = n_jobs\n",
    "        self.positive = positive\n",
    "\n",
    "    def fit(self, X, y, nepoch=10):\n",
    "        for epoch in range(nepoch):\n",
    "            running_loss = 0\n",
    "            self.net.train()\n",
    "            np.random.seed()\n",
    "            self.optimizer.zero_grad()\n",
    "            for (i, data) in enumerate(X):\n",
    "                data, label = X, y[i]\n",
    "                label = label.view(-1)\n",
    "                num_examples = label.size()[0] # need this because it's not always the batch size\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(smatrix)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pipeline import Dataset, backtest\n",
    "from matplotlib import pyplot as plt\n",
    "from datatools import data_quantization, check_dataframe, extract_market_data\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pandas import Series, DataFrame, MultiIndex\n",
    "from visualization.metric import Performance\n",
    "from sklearn.metrics import r2_score\n",
    "from visualization.metric import plot_performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset = Dataset.load('../../data/parsed')\n",
    "# dataset = load_mini_dataset('../../data/parsed_mini/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% DataLoading\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices matched\n",
      "Features matched\n",
      "DataFame is all good for the tests\n"
     ]
    }
   ],
   "source": [
    "quantized_fundamental, _ = data_quantization(dataset.fundamental)\n",
    "df = pd.concat([quantized_fundamental, dataset.fundamental, dataset.ref_return], axis=1).dropna()\n",
    "quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                            'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE']\n",
    "original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf']\n",
    "\n",
    "check_dataframe(df, expect_index=['day','asset'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Data Preprocessing\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "df['return_known'] = df['return'].shift(2*54)\n",
    "df=df.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "original_feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf','return_known']\n",
    "quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                            'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE', 'return_known']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training always loop over all available data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42b007856de8495d892de83a3655be40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 0.3705, cum_train_r2: -0.58%\n",
      "Fold [200/995], Loss: 0.0517, cum_train_r2: -1.03%\n",
      "Fold [300/995], Loss: 0.0010, cum_train_r2: -0.76%\n",
      "Fold [400/995], Loss: 0.0006, cum_train_r2: -0.88%\n",
      "Fold [500/995], Loss: 0.0008, cum_train_r2: -3.21%\n",
      "Fold [600/995], Loss: 0.0014, cum_train_r2: -0.98%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c_/y_0x4rsd6tb4chd_y83jnpzr0000gn/T/ipykernel_2610/3159425957.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlookback_window\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0mdays_train_valid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdays_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlookback_window\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# last ends at valid-day-2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m             \u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train_true\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays_train_valid\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfeature_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays_train_valid\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mreturn_column\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0mX_np\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mswaplevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mascending\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1065\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_scalar_access\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1066\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_takeable\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1067\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_tuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1068\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1069\u001B[0m             \u001B[0;31m# we by definition only have the 0th axis\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m_getitem_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m   1245\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0msuppress\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mIndexingError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1246\u001B[0m             \u001B[0mtup\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_expand_ellipsis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1247\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_lowerdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1248\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1249\u001B[0m         \u001B[0;31m# no multi-index, so validate all of the indexers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m_getitem_lowerdim\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m    939\u001B[0m         \u001B[0;31m# we may have a nested tuples indexer here\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    940\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_nested_tuple_indexer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 941\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_nested_tuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    942\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    943\u001B[0m         \u001B[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36m_getitem_nested_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1000\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtup\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1001\u001B[0;31m             \u001B[0mcheck_deprecated_indexers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1002\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1003\u001B[0m         \u001B[0;31m# we have too many indexers for our dim, but have at least 1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/indexing.py\u001B[0m in \u001B[0;36mcheck_deprecated_indexers\u001B[0;34m(key)\u001B[0m\n\u001B[1;32m   2669\u001B[0m     if (\n\u001B[1;32m   2670\u001B[0m         \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2671\u001B[0;31m         \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2672\u001B[0m         \u001B[0;32mand\u001B[0m \u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2673\u001B[0m     ):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "n_splits = 995\n",
    "# n_epoch = 10\n",
    "learning_rate = 0.1\n",
    "lookback_window = 16\n",
    "# Define loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "# Define network\n",
    " # net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "# Define the optimizier\n",
    "# optimizer = optim.LBFGS(net.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.5)\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "# label_train_list = []\n",
    "# test_list2 = []\n",
    "# test_score_old = 0\n",
    "\n",
    "days = df.index.get_level_values('day').unique()\n",
    "tscv = TimeSeriesSplit(n_splits=min(n_splits, len(days)))\n",
    "pbar = tqdm(tscv.split(days), total=tscv.n_splits)\n",
    "\n",
    "cum_y_val_true = Series(dtype=float)\n",
    "cum_y_val_prediction = Series(dtype=float)\n",
    "performance = Performance()\n",
    "# feature_columns = quantile_feature\n",
    "feature_columns = original_feature\n",
    "return_column = ['return']\n",
    "\n",
    "\n",
    "# for epoch in range(n_epoch):\n",
    "for fold, (train, val) in enumerate(pbar):\n",
    "    # X, _ = data_quantization(df[original_feature])\n",
    "    running_loss = 0\n",
    "    net.train()\n",
    "    np.random.seed()\n",
    "    days_train = days[train]\n",
    "\n",
    "    if len(days_train) < 2:\n",
    "        print('Skipping this fold since we cannot truncate the last day.')\n",
    "        continue\n",
    "    if (lookback_window is not None) and (len(days_train) > lookback_window):\n",
    "        cum_y_train_true = Series(dtype=float)\n",
    "        cum_y_train_prediction = Series(dtype=float)\n",
    "        for i in range(lookback_window, len(days_train)):\n",
    "            days_train_valid = days_train[i-lookback_window:i] # last ends at valid-day-2\n",
    "            X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid[-1]),:][return_column]\n",
    "\n",
    "            X_np = X_train.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "            # shape (asset, days, feature) -> (ft, days, asset)\n",
    "            X_np_tensor = X_np.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "            X_np_tensor = X_np_tensor[np.newaxis,:]  # add batch dimension\n",
    "            X_torch = torch.from_numpy(X_np_tensor)\n",
    "            labels = torch.tensor(y_train_true['return'].values).to(torch.float)\n",
    "            #\n",
    "            # def closure():\n",
    "            #     optimizer.zero_grad()\n",
    "            #     outputs=net(X_torch)\n",
    "            #     loss = criterion(outputs, labels)\n",
    "            #     loss.backward()\n",
    "            #     return loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X_torch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # optimizer.step(closure) # need closure for LBFGS\n",
    "            optimizer.step()\n",
    "            cum_y_train_prediction = pd.concat([cum_y_train_prediction, Series(outputs.detach().numpy())])\n",
    "            cum_y_train_true = pd.concat([cum_y_train_true, y_train_true.squeeze()], ignore_index=True)\n",
    "\n",
    "        performance[fold,'train_r2'] = r2_score(cum_y_train_prediction, cum_y_train_true)\n",
    "        if fold % 100 == 0:\n",
    "            print('Fold [{}/{}], Loss: {:.4f}, cum_train_r2: {:.2f}%'.format(fold, len(pbar), loss.item(), r2_score(cum_y_train_prediction, cum_y_train_true)))\n",
    "        # Validation:\n",
    "        days_val = days[int(val)-lookback_window:int(val)]\n",
    "\n",
    "        X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val[-1],), :][return_column]\n",
    "        net.eval()\n",
    "        net.train(False)\n",
    "        with torch.no_grad():\n",
    "            X_np_val = X_val.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "            # shape (asset, days, feature) -> (ft, days, asset)\n",
    "            X_np_val_tensor = X_np_val.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "            X_np_val_tensor = X_np_val_tensor[np.newaxis,:]  # add batch dimension\n",
    "            X_torch_val = torch.from_numpy(X_np_val_tensor)\n",
    "            labels = torch.tensor(y_val_true['return'].values).to(torch.float)\n",
    "\n",
    "            outputs_val = net(X_torch_val)\n",
    "            cum_y_val_prediction = pd.concat([cum_y_val_prediction, Series(outputs_val.numpy())])\n",
    "            cum_y_val_true = pd.concat([cum_y_val_true, y_val_true.squeeze()], ignore_index=True)\n",
    "\n",
    "        val_r2 = r2_score(y_val_true.squeeze(), Series(outputs_val.numpy())) # Do I need index for series to corr or R2?\n",
    "        performance[fold, 'val_r2'] = val_r2\n",
    "        val_pearson = y_val_true.squeeze().corr(Series(outputs_val.numpy()))\n",
    "        performance[fold, 'val_pearson'] = val_pearson\n",
    "        val_cum_r2 = r2_score(cum_y_val_true, cum_y_val_prediction)\n",
    "        performance[fold, 'val_cum_r2'] = val_cum_r2\n",
    "        val_cum_pearson = cum_y_val_true.corr(cum_y_val_prediction)\n",
    "        performance[fold, 'val_cum_pearson'] = val_cum_pearson\n",
    "\n",
    "        pbar.set_description(f'Fold {fold}, val_cum_r2={val_cum_r2:.4f}, val_cum_pearson={val_cum_pearson:.4f}')\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    # if fold > 20:\n",
    "    #     break\n",
    "    # X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid,), :][return_column]\n",
    "    # X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val,), :][return_column]\n",
    "\n",
    "    #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training only use the current data (one data one training each fold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8971953fc6694fbbbf087e9a302b7a49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lewisliu/micromamba/envs/DataScience/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [100/995], Loss: 4562.5898, cum_train_r2: -0.71%\n",
      "Fold [200/995], Loss: 1226.9479, cum_train_r2: -0.79%\n",
      "Fold [300/995], Loss: 868.2052, cum_train_r2: -0.61%\n",
      "Fold [400/995], Loss: 507.5324, cum_train_r2: -1.08%\n",
      "Fold [500/995], Loss: 189.3078, cum_train_r2: -1.28%\n",
      "Fold [600/995], Loss: 190.2297, cum_train_r2: -1.39%\n",
      "Fold [700/995], Loss: 860.8625, cum_train_r2: -0.61%\n",
      "Fold [800/995], Loss: 705.0922, cum_train_r2: -0.88%\n",
      "Fold [900/995], Loss: 378.9142, cum_train_r2: -0.69%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be691991bd6d477d872bdcc4466fba0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 3767.4880, cum_train_r2: -0.82%\n",
      "Fold [200/995], Loss: 1265.0452, cum_train_r2: -0.96%\n",
      "Fold [300/995], Loss: 499.5302, cum_train_r2: -1.29%\n",
      "Fold [400/995], Loss: 669.6657, cum_train_r2: -0.67%\n",
      "Fold [500/995], Loss: 183.0281, cum_train_r2: -1.18%\n",
      "Fold [600/995], Loss: 228.0203, cum_train_r2: -1.73%\n",
      "Fold [700/995], Loss: 860.3463, cum_train_r2: -0.91%\n",
      "Fold [800/995], Loss: 782.5058, cum_train_r2: -0.84%\n",
      "Fold [900/995], Loss: 340.9067, cum_train_r2: -1.14%\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f4046735a1649d3b174b9797fbdfd86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping this fold since we cannot truncate the last day.\n",
      "Fold [100/995], Loss: 4682.9399, cum_train_r2: -0.68%\n",
      "Fold [200/995], Loss: 867.1305, cum_train_r2: -0.75%\n",
      "Fold [300/995], Loss: 539.1282, cum_train_r2: -1.12%\n",
      "Fold [400/995], Loss: 534.5932, cum_train_r2: -0.78%\n",
      "Fold [500/995], Loss: 314.9617, cum_train_r2: -1.14%\n",
      "Fold [600/995], Loss: 241.6684, cum_train_r2: -1.30%\n",
      "Fold [700/995], Loss: 986.4664, cum_train_r2: -0.95%\n",
      "Fold [800/995], Loss: 5503.8125, cum_train_r2: -0.15%\n",
      "Fold [900/995], Loss: 365.3145, cum_train_r2: -0.94%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "n_splits = 995\n",
    "n_epoch = 3\n",
    "learning_rate = 0.03\n",
    "lookback_window = 16\n",
    "# Define loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "# Define network\n",
    " # net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,lookback_window,54), b_size=1)\n",
    "# Define the optimizier\n",
    "optimizer = optim.LBFGS(net.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas = (0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.5)\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "# label_train_list = []\n",
    "# test_list2 = []\n",
    "# test_score_old = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    days = df.index.get_level_values('day').unique()\n",
    "    tscv = TimeSeriesSplit(n_splits=min(n_splits, len(days)))\n",
    "    pbar = tqdm(tscv.split(days), total=tscv.n_splits)\n",
    "\n",
    "    cum_y_val_true = Series(dtype=float)\n",
    "    cum_y_val_prediction = Series(dtype=float)\n",
    "    performance = Performance()\n",
    "    # feature_columns = quantile_feature\n",
    "    feature_columns = original_feature\n",
    "    return_column = ['return']\n",
    "\n",
    "\n",
    "    # for epoch in range(n_epoch):\n",
    "    for fold, (train, val) in enumerate(pbar):\n",
    "        # X, _ = data_quantization(df[original_feature])\n",
    "        running_loss = 0\n",
    "        net.train()\n",
    "        np.random.seed()\n",
    "        days_train = days[train]\n",
    "\n",
    "        if len(days_train) < 2:\n",
    "            print('Skipping this fold since we cannot truncate the last day.')\n",
    "            continue\n",
    "        if (lookback_window is not None) and (len(days_train) > lookback_window):\n",
    "            cum_y_train_true = Series(dtype=float)\n",
    "            cum_y_train_prediction = Series(dtype=float)\n",
    "\n",
    "            days_train_valid = days_train[-lookback_window-1:-1] # last ends at valid-day-2\n",
    "            X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid[-1]),:][return_column]\n",
    "\n",
    "            X_np = X_train.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "                # shape (asset, days, feature) -> (ft, days, asset)\n",
    "            X_np_tensor = X_np.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "            X_np_tensor = X_np_tensor[np.newaxis,:]  # add batch dimension\n",
    "            X_torch = torch.from_numpy(X_np_tensor)\n",
    "            labels = torch.tensor(y_train_true['return'].values).to(torch.float)\n",
    "                #\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs=net(X_torch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X_torch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step(closure) # need closure for LBFGS\n",
    "            # optimizer.step()\n",
    "            cum_y_train_prediction = pd.concat([cum_y_train_prediction, Series(outputs.detach().numpy())])\n",
    "            cum_y_train_true = pd.concat([cum_y_train_true, y_train_true.squeeze()], ignore_index=True)\n",
    "\n",
    "            performance[fold,'train_r2'] = r2_score(cum_y_train_prediction, cum_y_train_true)\n",
    "            if fold % 100 == 0:\n",
    "                print('Fold [{}/{}], Loss: {:.4f}, cum_train_r2: {:.2f}%'.format(fold, len(pbar), loss.item(), r2_score(cum_y_train_prediction, cum_y_train_true)))\n",
    "            # Validation:\n",
    "            days_val = days[int(val)-lookback_window:int(val)]\n",
    "\n",
    "            X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val[-1],), :][return_column]\n",
    "            net.eval()\n",
    "            net.train(False)\n",
    "            with torch.no_grad():\n",
    "                X_np_val = X_val.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)\n",
    "                # shape (asset, days, feature) -> (ft, days, asset)\n",
    "                X_np_val_tensor = X_np_val.reshape(54,lookback_window,-1).transpose([2,1,0])\n",
    "                X_np_val_tensor = X_np_val_tensor[np.newaxis,:]  # add batch dimension\n",
    "                X_torch_val = torch.from_numpy(X_np_val_tensor)\n",
    "                labels = torch.tensor(y_val_true['return'].values).to(torch.float)\n",
    "\n",
    "                outputs_val = net(X_torch_val)\n",
    "                cum_y_val_prediction = pd.concat([cum_y_val_prediction, Series(outputs_val.numpy())])\n",
    "                cum_y_val_true = pd.concat([cum_y_val_true, y_val_true.squeeze()], ignore_index=True)\n",
    "\n",
    "            val_r2 = r2_score(y_val_true.squeeze(), Series(outputs_val.numpy())) # Do I need index for series to corr or R2?\n",
    "            performance[fold, 'val_r2'] = val_r2\n",
    "            val_pearson = y_val_true.squeeze().corr(Series(outputs_val.numpy()))\n",
    "            performance[fold, 'val_pearson'] = val_pearson\n",
    "            val_cum_r2 = r2_score(cum_y_val_true, cum_y_val_prediction)\n",
    "            performance[fold, 'val_cum_r2'] = val_cum_r2\n",
    "            val_cum_pearson = cum_y_val_true.corr(cum_y_val_prediction)\n",
    "            performance[fold, 'val_cum_pearson'] = val_cum_pearson\n",
    "\n",
    "            pbar.set_description(f'Fold {fold}, val_cum_r2={val_cum_r2:.4f}, val_cum_pearson={val_cum_pearson:.4f}')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        scheduler.step() ### !!!!!\n",
    "        # if fold > 20:\n",
    "        #     break\n",
    "        # X_train, y_train_true = df.loc[(days_train_valid,), :][feature_columns], df.loc[(days_train_valid,), :][return_column]\n",
    "        # X_val, y_val_true = df.loc[(days_val,), :][feature_columns], df.loc[(days_val,), :][return_column]\n",
    "\n",
    "        #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "277"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_train_valid[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "X_np = X_train.swaplevel(1,0).sort_index(ascending=True).to_numpy().astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9,  9,  6, ...,  6,  5,  1],\n       [ 9,  8,  6, ...,  3,  3,  3],\n       [10, 10,  6, ...,  3,  3,  3],\n       ...,\n       [ 4,  3,  5, ...,  2,  3,  6],\n       [ 2,  1,  5, ...,  2,  3,  6],\n       [ 7,  5,  5, ...,  3,  3,  6]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# shape (asset, days, feature) -> (ft, days, asset)\n",
    "X_np_tensor = X_np.reshape(54,16,-1).transpose([2,1,0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9,  9, 10, 10, 10, 10, 10, 10, 10,  9],\n       [ 9,  8, 10, 10, 10, 10, 10, 10,  9,  9],\n       [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n       [ 5,  4,  5,  5,  5,  5,  5,  5,  5,  5],\n       [ 6,  3,  3,  4,  3,  3,  3,  3,  3,  3],\n       [ 5,  3,  3,  4,  4,  4,  4,  4,  4,  4],\n       [ 1,  3,  3,  3,  3,  3,  3,  3,  3,  3]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv2d input: (B=1, D, H, W)\n",
    "# X_np_tensor: (ft, days, asset)\n",
    "X_np_tensor[:,:,0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "X_np_tensor = X_np_tensor[np.newaxis,:] # add"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "X_torch = torch.from_numpy(X_np_tensor)\n",
    "net = oneDVerConvNet(D_in=7, D_out=1, input_shape=(7,16,54), b_size=1)\n",
    "result = net(X_torch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% batch dimension\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_torch.dtype\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float32')"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float32')"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np_tensor.dtype\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float64"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.dtype\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "         return\nasset          \n0      0.004256\n1      0.006398\n2      0.020687\n3      0.038906\n4      0.067663\n5      0.031702\n6      0.013807\n7      0.001451\n8      0.025641\n9      0.035014\n10     0.026221\n11     0.003853\n12     0.039415\n13     0.000731\n14    -0.002828\n15     0.016185\n16     0.044301\n17     0.024825\n18    -0.030071\n19     0.021049\n20     0.065162\n21     0.009810\n22     0.022805\n23     0.023443\n24     0.006238\n25     0.014275\n26     0.031767\n27     0.015421\n28    -0.004718\n29     0.024829\n30     0.001149\n31     0.061746\n32     0.034813\n33     0.031757\n34     0.005987\n35     0.038168\n36     0.003685\n37     0.006733\n38     0.046112\n39     0.030467\n40     0.031605\n41     0.052146\n42     0.048666\n43     0.030405\n44     0.021027\n45     0.023225\n46     0.024411\n47     0.021748\n48     0.012484\n49     0.037496\n50     0.015389\n51     0.022050\n52     0.042382\n53     0.023863",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>return</th>\n    </tr>\n    <tr>\n      <th>asset</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004256</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.006398</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.020687</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.038906</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.067663</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.031702</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.013807</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.001451</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.025641</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.035014</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.026221</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.003853</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.039415</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000731</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.002828</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.016185</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.044301</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.024825</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-0.030071</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.021049</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.065162</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.009810</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.022805</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.023443</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.006238</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.014275</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.031767</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.015421</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>-0.004718</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.024829</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.001149</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.061746</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.034813</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.031757</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.005987</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.038168</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.003685</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.006733</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.046112</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.030467</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.031605</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.052146</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.048666</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.030405</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.021027</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.023225</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.024411</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.021748</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.012484</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.037496</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.015389</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.022050</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>0.042382</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>0.023863</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(days_train_valid[-1]),:][return_column]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "cum_y_train_true = Series(dtype=float)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "cum_y_train_true = pd.concat([cum_y_train_true, y_train_true_series])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "y_train_true_series = y_train_true.squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "data": {
      "text/plain": "     0         1\n0  NaN  0.004256\n1  NaN  0.006398\n2  NaN  0.020687\n3  NaN  0.038906\n4  NaN  0.067663\n5  NaN  0.031702\n6  NaN  0.013807\n7  NaN  0.001451\n8  NaN  0.025641\n9  NaN  0.035014\n10 NaN  0.026221\n11 NaN  0.003853\n12 NaN  0.039415\n13 NaN  0.000731\n14 NaN -0.002828\n15 NaN  0.016185\n16 NaN  0.044301\n17 NaN  0.024825\n18 NaN -0.030071\n19 NaN  0.021049\n20 NaN  0.065162\n21 NaN  0.009810\n22 NaN  0.022805\n23 NaN  0.023443\n24 NaN  0.006238\n25 NaN  0.014275\n26 NaN  0.031767\n27 NaN  0.015421\n28 NaN -0.004718\n29 NaN  0.024829\n30 NaN  0.001149\n31 NaN  0.061746\n32 NaN  0.034813\n33 NaN  0.031757\n34 NaN  0.005987\n35 NaN  0.038168\n36 NaN  0.003685\n37 NaN  0.006733\n38 NaN  0.046112\n39 NaN  0.030467\n40 NaN  0.031605\n41 NaN  0.052146\n42 NaN  0.048666\n43 NaN  0.030405\n44 NaN  0.021027\n45 NaN  0.023225\n46 NaN  0.024411\n47 NaN  0.021748\n48 NaN  0.012484\n49 NaN  0.037496\n50 NaN  0.015389\n51 NaN  0.022050\n52 NaN  0.042382\n53 NaN  0.023863",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>0.004256</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>0.006398</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>0.020687</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>0.038906</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>0.067663</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>0.031702</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>0.013807</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>0.001451</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>NaN</td>\n      <td>0.025641</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>NaN</td>\n      <td>0.035014</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>NaN</td>\n      <td>0.026221</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>NaN</td>\n      <td>0.003853</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>NaN</td>\n      <td>0.039415</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>NaN</td>\n      <td>0.000731</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NaN</td>\n      <td>-0.002828</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>NaN</td>\n      <td>0.016185</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>NaN</td>\n      <td>0.044301</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>NaN</td>\n      <td>0.024825</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>NaN</td>\n      <td>-0.030071</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>NaN</td>\n      <td>0.021049</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>NaN</td>\n      <td>0.065162</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>NaN</td>\n      <td>0.009810</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>NaN</td>\n      <td>0.022805</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>NaN</td>\n      <td>0.023443</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>NaN</td>\n      <td>0.006238</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>NaN</td>\n      <td>0.014275</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>NaN</td>\n      <td>0.031767</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>NaN</td>\n      <td>0.015421</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>NaN</td>\n      <td>-0.004718</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>NaN</td>\n      <td>0.024829</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>NaN</td>\n      <td>0.001149</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>NaN</td>\n      <td>0.061746</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>NaN</td>\n      <td>0.034813</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>NaN</td>\n      <td>0.031757</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>NaN</td>\n      <td>0.005987</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>NaN</td>\n      <td>0.038168</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>NaN</td>\n      <td>0.003685</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>NaN</td>\n      <td>0.006733</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>NaN</td>\n      <td>0.046112</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>NaN</td>\n      <td>0.030467</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>NaN</td>\n      <td>0.031605</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>NaN</td>\n      <td>0.052146</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>NaN</td>\n      <td>0.048666</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>NaN</td>\n      <td>0.030405</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>NaN</td>\n      <td>0.021027</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>NaN</td>\n      <td>0.023225</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>NaN</td>\n      <td>0.024411</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>NaN</td>\n      <td>0.021748</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>NaN</td>\n      <td>0.012484</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>NaN</td>\n      <td>0.037496</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>NaN</td>\n      <td>0.015389</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>NaN</td>\n      <td>0.022050</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>NaN</td>\n      <td>0.042382</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>NaN</td>\n      <td>0.023863</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_y_train_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n            51, 52, 53],\n           dtype='int64', name='asset')"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_true.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c_/y_0x4rsd6tb4chd_y83jnpzr0000gn/T/ipykernel_2610/2318841515.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0my_val_true\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mSeries\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs_val\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36mcorr\u001B[0;34m(self, method, min_periods, numeric_only)\u001B[0m\n\u001B[1;32m  10307\u001B[0m         \u001B[0mmat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna_value\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  10308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m> 10309\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"pearson\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m  10310\u001B[0m             \u001B[0mcorrel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlibalgos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnancorr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mminp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmin_periods\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  10311\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"spearman\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/micromamba/envs/DataScience/lib/python3.9/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m__nonzero__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1525\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mfinal\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__nonzero__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mNoReturn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1527\u001B[0;31m         raise ValueError(\n\u001B[0m\u001B[1;32m   1528\u001B[0m             \u001B[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m             \u001B[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "y_val_true.corr(Series(outputs_val.numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "test = net(X_torch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "995"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pbar)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "0     False\n1     False\n2     False\n3     False\n4     False\n      ...  \n49     True\n50     True\n51     True\n52     True\n53     True\nLength: 2538, dtype: bool"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cum_y_train_prediction.isna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ending score for metric train_r2 is: -1.1653e+00\n",
      "The ending score for metric val_cum_r2 is: -4.8179e+05\n",
      "The ending score for metric val_cum_pearson is: 9.0559e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "# plot_performance(performance, metrics_selected=['train_r2', 'test_cum_r2', 'test_cum_pearson'])\n",
    "plot_performance(performance, metrics_selected=['train_r2', 'val_cum_pearson','val_cum_r2'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/lewisliu/Desktop/Study/Graduate_Study_Austin/Career/Practice/qids-2023-comp/playground/shockspeare'"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "model_path = '../../model/dump/1DConv'\n",
    "torch.save(net.state_dict(), model_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Save model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "from qids_lib import QIDS\n",
    "from pipeline.backtest import evaluation_for_submission\n",
    "\n",
    "\n",
    "dataset = Dataset.load('../data/parsed')\n",
    "df = pd.concat([dataset.fundamental, extract_market_data(dataset.market)], axis=1).dropna()\n",
    "df['return_known'] = df['return'].shift(2*54)\n",
    "df=df.dropna()\n",
    "f_quantile_feature = ['turnoverRatio_QUANTILE', 'transactionAmount_QUANTILE', 'pb_QUANTILE', 'ps_QUANTILE',\n",
    "                      'pe_ttm_QUANTILE', 'pe_QUANTILE', 'pcf_QUANTILE', 'return_known']\n",
    "m_quantile_feature = ['avg_price_QUANTILE', 'volatility_QUANTILE', 'mean_volume_QUANTILE', 'return_known']\n",
    "# feature = ['turnoverRatio', 'transactionAmount', 'pb', 'ps', 'pe_ttm', 'pe', 'pcf', 'avg_price', 'volatility',\n",
    "#            'mean_volume']\n",
    "\n",
    "\n",
    "\n",
    "# q_df, _ = data_quantization(dataset.fundamental)\n",
    "# full_df = pd.concat([q_df, dataset.ref_return], axis=1).dropna()\n",
    "# model = linear_model(full_df[f_quantile_feature], full_df['return'])\n",
    "\n",
    "qids = QIDS(path_prefix='../')\n",
    "performance = evaluation_for_submission(linear_model, original_feature, dataset=dataset, df=df, qids=qids,\n",
    "                                        lookback_window=200)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Test\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}